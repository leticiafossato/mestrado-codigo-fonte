{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b346887a",
   "metadata": {},
   "source": [
    "https://www.nltk.org/howto/lm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58193246",
   "metadata": {},
   "source": [
    "Essa é a versão final oficial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba26060",
   "metadata": {},
   "source": [
    "# Código Mestrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110edfc",
   "metadata": {},
   "source": [
    "https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb111096",
   "metadata": {},
   "source": [
    "https://github.com/PacktPublishing/fastText-Quick-Start-Guide/blob/master/chapter5/python%20fastText%20unsupervised%20learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a37394",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5cdd3979",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.022383Z",
     "start_time": "2023-05-01T16:29:06.014383Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.lm import Laplace\n",
    "import os\n",
    "import joblib\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import numpy\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import ngrams,bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "from fasttext import load_model\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm.preprocessing import pad_both_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174bd82a",
   "metadata": {},
   "source": [
    "## Leitura de Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2acaae9",
   "metadata": {},
   "source": [
    "### DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "06b6337f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.148714Z",
     "start_time": "2023-05-01T16:29:06.096181Z"
    }
   },
   "outputs": [],
   "source": [
    "papers = joblib.load('papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "262368c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.179056Z",
     "start_time": "2023-05-01T16:29:06.162912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15fc8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T20:19:19.109050Z",
     "start_time": "2023-02-27T20:19:19.099194Z"
    }
   },
   "source": [
    "papers_treino = dict(itertools.islice(papers.items(),495))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "9ab49749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.195028Z",
     "start_time": "2023-05-01T16:29:06.182032Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_treino = dict(list(papers.items())[0:495])\n",
    "papers_teste = dict(list(papers.items())[495:708])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348d7f7",
   "metadata": {},
   "source": [
    "### Padrão Ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "367471ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.225864Z",
     "start_time": "2023-05-01T16:29:06.203056Z"
    }
   },
   "outputs": [],
   "source": [
    "padrao_ouro = joblib.load('selecao_padrao_ouro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "587562e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.241864Z",
     "start_time": "2023-05-01T16:29:06.228865Z"
    }
   },
   "outputs": [],
   "source": [
    "padrao_ouro_treino = dict(list(padrao_ouro.items())[0:495])\n",
    "padrao_ouro_teste  = dict(list(padrao_ouro.items())[495:708])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ecd17e71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.256995Z",
     "start_time": "2023-05-01T16:29:06.243864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb871dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T18:37:54.795627Z",
     "start_time": "2023-03-31T18:37:54.733198Z"
    }
   },
   "source": [
    "pd.DataFrame.from_dict(padrao_ouro_treino.items()).to_excel('ouro_treino.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c591f50",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e3df1d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.337620Z",
     "start_time": "2023-05-01T16:29:06.318647Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessamento(corpus: str):\n",
    "    \"\"\" Pré-processamento inicial para remover pontuações, padronizar a caixa e pegar o conteúdo de cada paper\"\"\"\n",
    "    regIter = re.finditer('(<PAPER>(.*)<\\/PAPER>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    #lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        #txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(\"\\\\t\",\"\",txt)\n",
    "        #txt = re.sub('\\/','', txt)\n",
    "        txt = re.sub(\"\\-\",\"\",txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        #txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        txt = re.sub('&amp;quot;seed&amp;quot;', '', txt)\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;\\?@\\[\\]^_`{|}\\~\\%\\.]', '', txt)\n",
    "        txt = re.sub('=', ' ', txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        #txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "        txt = re.sub('i\\.e.', '', txt)\n",
    "        txt = re.sub('acc\\.', '', txt)\n",
    "        txt = re.sub('e\\.g\\.', '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c9862046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:06.353691Z",
     "start_time": "2023-05-01T16:29:06.340630Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_processamento_ouro(txt):\n",
    "    #txt = re.sub('\\\\n',' ', txt)\n",
    "    txt = txt.lower()\n",
    "    #txt = re.sub(r'<.*?>','', txt)\n",
    "    txt = re.sub(\"\\\\t\",\"\",txt)\n",
    "    #txt = re.sub('\\/','', txt)\n",
    "    txt = re.sub(\"\\-\",\"\",txt)\n",
    "    txt = txt.strip()\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "    txt = re.sub('&amp;quot;', '', txt)\n",
    "    txt = re.sub('&amp;quot;seed&amp;quot;', '', txt)\n",
    "    txt = re.sub('\\.+', '', txt)\n",
    "    txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;=\\?@\\[\\]^_`{|}\\~\\%\\.]', '', txt)\n",
    "    txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "    txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "    #txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    txt = re.sub('i\\.e.', '', txt)\n",
    "    txt = re.sub('acc\\.', '', txt)\n",
    "    txt = re.sub('e\\.g\\.', '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e2c73fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:09.873254Z",
     "start_time": "2023-05-01T16:29:06.355726Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_treino2 = {k: preprocessamento(v) for k, v in papers_treino.items()}\n",
    "padrao_ouro_treino2 = {k: pre_processamento_ouro(v) for k, v in padrao_ouro_treino.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4f9a445e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:11.074760Z",
     "start_time": "2023-05-01T16:29:09.877256Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_teste2 = {k: preprocessamento(v) for k, v in papers_teste.items()}\n",
    "padrao_ouro_teste2 = {k: pre_processamento_ouro(v) for k, v in padrao_ouro_teste.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a15a0be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:11.089847Z",
     "start_time": "2023-05-01T16:29:11.075758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s sid >a practical partofspeech tagger</s> <abstract> <s sid  ssid >we present an implementation of a partofspeech tagger based on a hidden markov model</s> <s sid  ssid >the methodology enables robust and accurate tagging with few resource requirements</s> <s sid  ssid >only a lexicon and some unlabeled training text are required</s> <s sid  ssid >accuracy exceeds </s> <s sid  ssid >we describe implementation strategies and optimizations which result in highspeed operation</s> <s sid  ssid >three applications for tagging are described phrase recognition word sense disambiguation and grammatical function assignment</s> <s sid  ssid > desiderata many words are ambiguous in their part of speech</s> <s sid  ssid >for example tag can be a noun or a verb</s> <s sid  ssid >however when a word appears in the context of other words the ambiguity is often reduced in a tag is a partofspeech label the tag can only be a noun</s> <s sid  ssid >a tagger is a system that uses context to assign parts of speech to words</s> <s sid  ssid >automatic text tagging is an important first step in discovering the linguistic structure of large text corpora</s> <s sid  ssid >partofspeech information facilitates higherlevel analysis such as recognizing noun phrases and other patterns in text</s> <s sid  ssid >for a tagger to function as a practical component in a language processing system we believe that a tagger must be corpora contain ungrammatical constructions isolated phrases such as titles and nonlinguistic data such as tables</s> <s sid  ssid >corpora are also likely to contain words that are unknown to the tagger</s> <s sid  ssid >it is desirable that a tagger deal gracefully with these situations a tagger is to be used to analyze arbitrarily large corpora it must be efficientperforming in time linear in the number of words tagged</s> <s sid  ssid >any training required should also be fast enabling rapid turnaround with new corpora and new text genres</s> <s sid  ssid >a should attempt to assign the correct partofspeech tag to every word encountered</s> <s sid  ssid >a should be able to take advantage of linguistic insights</s> <s sid  ssid >one should be able to correct errors by supplying appropriate priori hints it should be possible to give different hints for different corpora effort required to retarget a tagger to new corpora new tagsets and new languages should be minimal</s> <s sid  ssid > methodology  background several different approaches have been used for building text taggers</s> <s sid  ssid >greene and rubin used a rulebased approach in the taggit program greene and rubin  which was an aid in tagging the brown corpus francis and kueera </s> <s sid  ssid >taggit disambiguated  of the corpus the rest was done manually over a period of several years</s> <s sid  ssid >more recently koskenniemi also used a rulebased approach implemented with finitestate machines koskenniemi </s> <s sid  ssid >statistical methods have also been used eg derose garside al these provide the capability of resolving ambiguity on the basis of most likely interpretation</s> <s sid  ssid >a form of markov model has been widely used that assumes that a word depends probabilistically on just its partofspeech category which in turn depends solely on the categories of the preceding two words</s> <s sid  ssid >two types of training ie parameter estimation have been used with this model</s> <s sid  ssid >the first makes use of a tagged training corpus</s> <s sid  ssid >derouault and merialdo use a bootstrap method for training derouault and merialdo </s> <s sid  ssid >at first a relatively small amount of text is manually tagged and used to train a partially accurate model</s> <s sid  ssid >the model is then used to tag more text and the tags are manually corrected and then used to retrain the model</s> <s sid  ssid >church uses the tagged brown corpus for training church </s> <s sid  ssid >these models involve probabilities for each word in the lexicon so large tagged corpora are required for reliable estimation</s> <s sid  ssid >the second method of training does not require a tagged training corpus</s> <s sid  ssid >in this situation the baumwelch algorithm also known as the forwardbackward algorithm can be used baum </s> <s sid  ssid >under this regime the model is a markov model as state transitions ie partofspeech categories are assumed to be unobservable</s> <s sid  ssid >jelinek has used this method for training a text tagger jelinek </s> <s sid  ssid >parameter smoothing can be conachieved using the method of interpolawhich weighted estimates are taken from secondand firstorder models and a uniform probability distribution jelinek and mercer </s> <s sid  ssid >kupiec used word equivclasses referred to here as classes on parts of speech to pool data from individual words kupiec b</s> <s sid  ssid >the most common words are still represented individually as sufficient data exist for robust estimation</s> <s sid  ssid > however all other words are represented according to the set of possible categories they can assume</s> <s sid  ssid >in this manner the vocabulary of  words in the brown corpus can be reduced to approximately  distinct ambiguity classes kupiec </s> <s sid  ssid >to further reduce the number of parameters a firstorder model can be employed this assumes that a words category depends only on the immediately preceding words category</s> <s sid  ssid >in kupiec a networks are used to selectively augment the context in a basic firstorder model rather than using uniformly secondorder dependencies</s> <s sid  ssid > our approach we next describe how our choice of techniques satisfies the listed in section </s> <s sid  ssid >the use of an complete flexibility in the choice of training corpora</s> <s sid  ssid >text from any desired domain can be used and a tagger can be tailored for use with a particular text database by training on a portion of that database</s> <s sid  ssid >lexicons containing alternative tag sets can be easily accommodated without any need for relabeling the training corpus affording further flexibility in the use of specialized tags</s> <s sid  ssid >as the resources required are simply a lexicon and a suitably large sample of ordinary text taggers can be built with minimal effort even for other languages such as french eg kupiec </s> <s sid  ssid >the use of ambiguity classes and a firstorder model reduces the number of parameters to be estimated without significant reduction in accuracy discussed in section </s> <s sid  ssid >this also enables a tagger to be reliably trained using only moderate amounts of text</s> <s sid  ssid >we have produced reasonable results training on as few as  sentences</s> <s sid  ssid >fewer parameters also reduce the time required for training</s> <s sid  ssid >relatively few ambiguity classes are sufficient for wide coverage so it is unlikely that adding new words to the lexicon requires retraining as their ambiguity classes are already accommodated</s> <s sid  ssid >vocabulary independence is achieved by predicting categories for words not in the lexicon using both context and suffix information</s> <s sid  ssid >probabilities corresponding to category sequences that never occurred in the training data are assigned small nonzero values ensuring that the model will accept any sequence of tokens while still providing the most likely tagging</s> <s sid  ssid >by using the fact that words are typically associated with only a few partofspeech categories and carefully ordering the computation the algorithms have linear complexity section </s> <s sid  ssid > hidden markov modeling the hidden markov modeling component of our tagger is implemented as an independent module following the specgiven in levinson et with special attention to space and time efficiency issues</s> <s sid  ssid >only firstorder modeling is addressed and will be presumed for the remainder of this discussion</s> <s sid  ssid > formalism brief an a doubly stochastic process that generates sequence of symbols  sls   ltilt where w is some finite set of possible symbols by composing an underlying markov process with a statedependent symbol generator ie a markov process with noise</s> <s sid  ssid >th markov process captures the notion of sequence depen and is described by a set of a matrix c probabilities a    lt a the probability of moving from state i to state of initial probabilities h     lt i lt is the probability of starting in state i</s> <s sid  ssid >the symbol ger erator is a statedependent measure on v described by of symbol probabilities  lt j lt lt lt m   iw i is the probability  symbol given that the markov process is i in partofspeech tagging we will model word order di pendency through an underlying markov process that  erates in terms of lexical tags yet we will only be ab to observe the sets of tags or ambiguity classes that ai possible for individual words</s> <s sid  ssid >the ambiguity class of eac word is the set of its permitted parts of speech only or of which is correct in context</s> <s sid  ssid >given the parameters  markov modeling allows us to compute ti most probable sequence of state transitions and hence a mostly likely sequence of lexical tags corresponding to of ambiguity classes</s> <s sid  ssid >in the following identified with the number of possible tags and w wil the set of all ambiguity classes</s> <s sid  ssid >applying an hmm consists of two tasks estimating ti parameters a a training set ar computing the most likely sequence of underlying sta transitions given new observations</s> <s sid  ssid >maximum likeliho estimates that is estimates that maximize the probabili of the training set can be found through application of z ternating expectation in a procedure known as the baur welch or forwardbackward algorithm baum  proceeds by recursively defining two sets of probabiliti the forward probabilities eft</s> <s sid  ssid >i   bisti lt t lt t      for all the backward prob bilities ii     lt t lt     for all forward probabili the joint probability of the sequence up to tir t s  the event that the markov pr is in state i at time the backwa is the probability of seeing the sequen  st that the markov process is state i at time t it follows that the probability of t entire sequence is n n  </s> </abstract> <section title  desiderata number > <s sid  ssid >many words are ambiguous in their part of speech</s> <s sid  ssid >for example tag can be a noun or a verb</s> <s sid  ssid >however when a word appears in the context of other words the ambiguity is often reduced in a tag is a partofspeech label the word tag can only be a noun</s> <s sid  ssid >a partofspeech tagger is a system that uses context to assign parts of speech to words</s> <s sid  ssid >automatic text tagging is an important first step in discovering the linguistic structure of large text corpora</s> <s sid  ssid >partofspeech information facilitates higherlevel analysis such as recognizing noun phrases and other patterns in text</s> <s sid  ssid >for a tagger to function as a practical component in a language processing system we believe that a tagger must be robust text corpora contain ungrammatical constructions isolated phrases such as titles and nonlinguistic data such as tables</s> <s sid  ssid >corpora are also likely to contain words that are unknown to the tagger</s> <s sid  ssid >it is desirable that a tagger deal gracefully with these situations</s> <s sid  ssid >efficient if a tagger is to be used to analyze arbitrarily large corpora it must be efficientperforming in time linear in the number of words tagged</s> <s sid  ssid >any training required should also be fast enabling rapid turnaround with new corpora and new text genres</s> <s sid  ssid >accurate a tagger should attempt to assign the correct partofspeech tag to every word encountered</s> <s sid  ssid >tunable a tagger should be able to take advantage of linguistic insights</s> <s sid  ssid >one should be able to correct systematic errors by supplying appropriate a priori hints it should be possible to give different hints for different corpora</s> <s sid  ssid >reusable the effort required to retarget a tagger to new corpora new tagsets and new languages should be minimal</s> </section> <section title  methodology number > <s sid  ssid >several different approaches have been used for building text taggers</s> <s sid  ssid >greene and rubin used a rulebased approach in the taggit program greene and rubin  which was an aid in tagging the brown corpus francis and kueera </s> <s sid  ssid >taggit disambiguated  of the corpus the rest was done manually over a period of several years</s> <s sid  ssid >more recently koskenniemi also used a rulebased approach implemented with finitestate machines koskenniemi </s> <s sid  ssid >statistical methods have also been used eg derose  garside et al </s> <s sid  ssid >these provide the capability of resolving ambiguity on the basis of most likely interpretation</s> <s sid  ssid >a form of markov model has been widely used that assumes that a word depends probabilistically on just its partofspeech category which in turn depends solely on the categories of the preceding two words</s> <s sid  ssid >two types of training ie parameter estimation have been used with this model</s> <s sid  ssid >the first makes use of a tagged training corpus</s> <s sid  ssid >derouault and merialdo use a bootstrap method for training derouault and merialdo </s> <s sid  ssid >at first a relatively small amount of text is manually tagged and used to train a partially accurate model</s> <s sid  ssid >the model is then used to tag more text and the tags are manually corrected and then used to retrain the model</s> <s sid  ssid >church uses the tagged brown corpus for training church </s> <s sid  ssid >these models involve probabilities for each word in the lexicon so large tagged corpora are required for reliable estimation</s> <s sid  ssid >the second method of training does not require a tagged training corpus</s> <s sid  ssid >in this situation the baumwelch algorithm also known as the forwardbackward algorithm can be used baum </s> <s sid  ssid >under this regime the model is called a hidden markov model hmm as state transitions ie partofspeech categories are assumed to be unobservable</s> <s sid  ssid >jelinek has used this method for training a text tagger jelinek </s> <s sid  ssid >parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from secondand firstorder models and a uniform probability distribution jelinek and mercer </s> <s sid  ssid >kupiec used word equivalence classes referred to here as ambiguity classes based on parts of speech to pool data from individual words kupiec b</s> <s sid  ssid >the most common words are still represented individually as sufficient data exist for robust estimation</s> <s sid  ssid >however all other words are represented according to the set of possible categories they can assume</s> <s sid  ssid >in this manner the vocabulary of  words in the brown corpus can be reduced to approximately  distinct ambiguity classes kupiec </s> <s sid  ssid >to further reduce the number of parameters a firstorder model can be employed this assumes that a words category depends only on the immediately preceding words category</s> <s sid  ssid >in kupiec a networks are used to selectively augment the context in a basic firstorder model rather than using uniformly secondorder dependencies</s> <s sid  ssid >we next describe how our choice of techniques satisfies the criteria listed in section </s> <s sid  ssid >the use of an hmm permits complete flexibility in the choice of training corpora</s> <s sid  ssid >text from any desired domain can be used and a tagger can be tailored for use with a particular text database by training on a portion of that database</s> <s sid  ssid >lexicons containing alternative tag sets can be easily accommodated without any need for relabeling the training corpus affording further flexibility in the use of specialized tags</s> <s sid  ssid >as the resources required are simply a lexicon and a suitably large sample of ordinary text taggers can be built with minimal effort even for other languages such as french eg kupiec </s> <s sid  ssid >the use of ambiguity classes and a firstorder model reduces the number of parameters to be estimated without significant reduction in accuracy discussed in section </s> <s sid  ssid >this also enables a tagger to be reliably trained using only moderate amounts of text</s> <s sid  ssid >we have produced reasonable results training on as few as  sentences</s> <s sid  ssid >fewer parameters also reduce the time required for training</s> <s sid  ssid >relatively few ambiguity classes are sufficient for wide coverage so it is unlikely that adding new words to the lexicon requires retraining as their ambiguity classes are already accommodated</s> <s sid  ssid >vocabulary independence is achieved by predicting categories for words not in the lexicon using both context and suffix information</s> <s sid  ssid >probabilities corresponding to category sequences that never occurred in the training data are assigned small nonzero values ensuring that the model will accept any sequence of tokens while still providing the most likely tagging</s> <s sid  ssid >by using the fact that words are typically associated with only a few partofspeech categories and carefully ordering the computation the algorithms have linear complexity section </s> </section> <section title  hidden markov modeling number > <s sid  ssid >the hidden markov modeling component of our tagger is implemented as an independent module following the specification given in levinson et al  with special attention to space and time efficiency issues</s> <s sid  ssid >only firstorder modeling is addressed and will be presumed for the remainder of this discussion</s> <s sid  ssid >in brief an hmm is a doubly stochastic process that generates sequence of symbols s  sls  st sew  ltilt t where w is some finite set of possible symbols by composing an underlying markov process with a statedependent symbol generator ie a markov process with noise</s> <s sid  ssid >th markov process captures the notion of sequence depen dency and is described by a set of n states a matrix c transition probabilities a   la  lt i j lt n where a is the probability of moving from state i to state j and vector of initial probabilities h     lt i lt n where  is the probability of starting in state i</s> <s sid  ssid >the symbol ger erator is a statedependent measure on v described by matrix of symbol probabilities b kik  lt j lt n an  lt k lt m where m   iw i and kik is the probability  generating symbol sk given that the markov process is i state j in partofspeech tagging we will model word order di pendency through an underlying markov process that  erates in terms of lexical tags yet we will only be ab to observe the sets of tags or ambiguity classes that ai possible for individual words</s> <s sid  ssid >the ambiguity class of eac word is the set of its permitted parts of speech only or of which is correct in context</s> <s sid  ssid >given the parameters  and ii hidden markov modeling allows us to compute ti most probable sequence of state transitions and hence a mostly likely sequence of lexical tags corresponding to sequence of ambiguity classes</s> <s sid  ssid >in the following n can identified with the number of possible tags and w wil the set of all ambiguity classes</s> <s sid  ssid >applying an hmm consists of two tasks estimating ti model parameters a b and h from a training set ar computing the most likely sequence of underlying sta transitions given new observations</s> <s sid  ssid >maximum likeliho estimates that is estimates that maximize the probabili of the training set can be found through application of z ternating expectation in a procedure known as the baur welch or forwardbackward algorithm baum  proceeds by recursively defining two sets of probabiliti the forward probabilities where oti   ribi for all i and the backward prob bilities where otj    for all j</s> <s sid  ssid >the forward probabili oeti is the joint probability of the sequence up to tir t s s  st and the event that the markov pr cess is in state i at time t similarly the backwa probability otj is the probability of seeing the sequen sti st  st given that the markov process is state i at time t it follows that the probability of t entire sequence is for any tin the range  ltt lt t   for an introduction to hidden markov modeling see f biner and juang </s> <s sid  ssid >given an initial choice for the parameters a b and h the expected number of transitions  from state i to state j conditioned on the observation sequence s may be computed as follows rescale</s> <s sid  ssid >one approach premultiplies the a and  probabilities with an accumulating product depending on t levinson et al </s> <s sid  ssid >let eii   aii and define and  </s> <s sid  ssid >  y</s> <s sid  ssid > in summary to find maximum likelihood estimates for a b and h via the baumwelch algorithm one chooses some starting values applies equations  to compute new values and then iterates until convergence</s> <s sid  ssid >it can be shown that this algorithm will converge although possibly to a nonglobal maximum baum </s> <s sid  ssid >once a model has been estimated selecting the most likely underlying sequence of state transitions corresponding to an observation s can be thought of as a maximization over all sequences that might generate s an efficient dynamic programming procedure known as the viterbi algorithm viterbi  arranges for this computation to proceed in time proportional to t suppose v   vt  lt t lt t is a state sequence that generates s then the probability that v generates s is to find the most probable such sequence we start by defining i   rabisi for  lt i lt n and then perform the recursion for  lt t lt t and  lt j lt n the crucial observation is that for each time t and each state i one need only consider the most probable sequence arriving at state i at time t the probability of the most probable sequence is maxiltltnoti while the sequence itself can be reconstructed by defining vt   maxltn oti and vt    ikeqt for t gt t gt </s> <s sid  ssid >the baumwelch algorithm equations  and the viterbi algorithm equation  involve operations on products of numbers constrained to be between  and </s> <s sid  ssid >since these products can easily underflow measures must be taken to now define ampti   ciciti and use a in place of a in equation  to define amp for the next iteration note that eini eeti    for  lt t lt t similarly let oti   oti and define ti   ctti for t gt t gt  where the scaled backward and forward probabilities amp and  can be exchanged for the unscaled probabilities in equations  without affecting the value of the ratios</s> <s sid  ssid >to see this note that ampti   cati and ti where now in terms of the scaled probabilities equation  for example can be seen to be unchanged a slight difficulty occurs in equation  that can be cured by the addition of a new term cti in each product of the upper sum numerical instability in the viterbi algorithm can be ameliorated by operating on a logarithmic scale levinson et al </s> <s sid  ssid >that is one maximizes the log probability of each sequence of state transitions care must be taken with zero probabilities</s> <s sid  ssid >however this can be elegantly handled through the use of ieee negative infinity p </s> <s sid  ssid >as can be seen from equations  the time cost of training is tn</s> <s sid  ssid >similarly as given in equation  the viterbi algorithm is also tn</s> <s sid  ssid >however in partofspeech tagging the problem structure dictates that the matrix of symbol probabilities b is sparsely populated</s> <s sid  ssid >that is  if the ambiguity class corresponding to symbol j includes the partofspeech tag associated with state i</s> <s sid  ssid >in practice the degree of overlap between ambiguity classes is relatively low some tokens are assigned unique tags and hence have only one nonzero symbol probability</s> <s sid  ssid >the sparseness of b leads one to consider restructuring equations  so a check for zero symbol probability can obviate the need for further computation</s> <s sid  ssid >equation  is already conveniently factored so that the dependence on bjsti  is outside the inner sum</s> <s sid  ssid >hence if k is the average number of nonzero entries in each row of b the cost of computing equation  can be reduced to ktn</s> <s sid  ssid >equations  can be similarly reduced by switching the order of iteration</s> <s sid  ssid >for example in equation  rather than for a given t computing oti for each i one at a time one can accumulate terms for all i in parallel</s> <s sid  ssid >the net effect of this rewriting is to place a bsti    check outside the innermost iteration</s> <s sid  ssid >equations  and  submit to a similar approach</s> <s sid  ssid >equation  is already only n</s> <s sid  ssid >hence the overall cost of training can be reduced to ktn which in our experience amounts to an order of magnitude speedup</s> <s sid  ssid > the time complexity of the viterbi algorithm can also be reduced to ktn by noting that b s can be factored out of the maximization of equation </s> <s sid  ssid >adding up the sizes of the probability matrices a b and h it is easy to see that the storage cost for directly representing one model is proportional to nn m </s> <s sid  ssid >running the baumwelch algorithm requires storage for the sequence of observations the a and  probabilities the vector c and copies of the a and b matrices since the originals cannot be overwritten until the end of each iteration</s> <s sid  ssid >hence the grand total of space required for training is proportional to t nt n m  </s> <s sid  ssid >since n and m are fixed by the model the only parameter that can be varied to reduce storage costs is t now adequate training requires processing from tens of thousands to hundreds of thousands of tokens kupiec a</s> <s sid  ssid >the training set can be considered one long sequence it which case t is very large indeed or it can be broken up into a number of smaller sequences at convenient boundaries</s> <s sid  ssid >in firstorder hidden markov modeling the stochastic process effectively restarts at unambiguous tokens such as sentence and paragraph markers hence these tokens are convenient points at which to break the training set</s> <s sid  ssid >if the baumwelch algorithm is run separately from the same starting point on each piece the resulting trained models must be recombined in some way</s> <s sid  ssid >one obvious approach is simply to average</s> <s sid  ssid >however this fails if any two an equivalent approach maintains a mapping from states i to nonzero symbol probabilities and simply avoids in the inner iteration computing products which must be zero kupiec  states are indistinguishable in the sense that they had the same transition probabilities and the same symbol probabilities at start because states are then not matched across trained models</s> <s sid  ssid >it is therefore important that each state have a distinguished role which is relatively easy to achieve in partofspeech tagging</s> <s sid  ssid >our implementation of the baumwelch algorithm breaks up the input into fixedsized pieces of training text</s> <s sid  ssid >the baumwelch algorithm is then run separately on each piece and the results are averaged together</s> <s sid  ssid >running the viterbi algorithm requires storage for the sequence of observations a vector of current maxes a scratch array of the same size and a matrix of ib indices for a total proportional to t  n t and a grand total including the model of t nn  m t </s> <s sid  ssid >again n and m are fixed</s> <s sid  ssid >however t need not be longer than a single sentence since as was observed above the hmm and hence the viterbi algorithm restarts at sentence boundaries</s> <s sid  ssid >an hmm for partofspeech tagging can be tuned in a variety of ways</s> <s sid  ssid >first the choice of tagset and lexicon determines the initial model</s> <s sid  ssid >second empirical and a priori information can influence the choice of starting values for the baumwelch algorithm</s> <s sid  ssid >for example counting instances of ambiguity classes in running text allows one to assign nonuniform starting probabilities in a for a particular tags realization as a particular ambiguity class</s> <s sid  ssid >alternatively one can state a priori that a particular ambiguity class is most likely to be the reflection of some subset of its component tags</s> <s sid  ssid >for example if an ambiguity class consisting of the open class tags is used for unknown words one may encode the fact that most unknown words are nouns or proper nouns by biasing the initial probabilities in b</s> <s sid  ssid >another biasing of starting values can arises from noting that some tags are unlikely to be followed by others</s> <s sid  ssid >for example the lexical item to maps to an ambiguity class containing two tags infinitivemarker and toaspreposition neither of which occurs in any other ambiguity class</s> <s sid  ssid >if nothing more were stated the hmm would have two states which were indistinguishable</s> <s sid  ssid >this can be remedied by setting the initial transition probabilities from infinitivemarker to strongly favor transitions to such states as verbuninflected and adverb</s> <s sid  ssid >our implementation allows for two sorts of biasing of starting values ambiguity classes can be annotated with favored tags and states can be annotated with favored transitions</s> <s sid  ssid >these biases may be specified either as sets or as set complements</s> <s sid  ssid >biases are implemented by replacing the disfavored probabilities with a small constant machine epsilon and redistributing mass to the other possibilities</s> <s sid  ssid >this has the effect of disfavoring the indicated outcomes without disallowing them sufficient converse data can rehabilitate these values</s> </section> <section title  architecture number > <s sid  ssid >in support of this and other work we have developed a system architecture for text access cutting et al </s> <s sid  ssid >this architecture defines five components for such systems corpus which provides text in a generic manner analysis which extracts terms from the text index which stores term occurrence statistics and search which utilizes these statistics to resolve queries</s> <s sid  ssid >the partofspeech tagger described here is implemented as an analysis module</s> <s sid  ssid >figure  illustrates the overall architecture showing the tagger analysis implementation in detail</s> <s sid  ssid >the tagger itself has a modular architecture isolating behind standard protocols those elements which may vary enabling easy substitution of alternate implementations</s> <s sid  ssid >also illustrated here are the data types which flow between tagger components</s> <s sid  ssid >as an analysis implementation the tagger must generate terms from text</s> <s sid  ssid >in this context a term is a word stem annotated with part of speech</s> <s sid  ssid >text enters the analysis subsystem where the first processing module it encounters is the tokenizer whose duty is to convert text a sequence of characters into a sequence of tokens</s> <s sid  ssid >sentence boundaries are also identified by the tokenizer and are passed as reserved tokens</s> <s sid  ssid >the tokenizer subsequently passes tokens to the lexicon</s> <s sid  ssid >here tokens are converted into a set of stems each annotated with a partofspeech tag</s> <s sid  ssid >the set of tags identifies an ambiguity class</s> <s sid  ssid >the identification of these classes is also the responsibility of the lexicon</s> <s sid  ssid >thus the lexicon delivers a set of stems paired with tags and an ambiguity class</s> <s sid  ssid >the training module takes long sequences of ambiguity classes as input</s> <s sid  ssid >it uses the baumwelch algorithm to produce a trained hmm an input to the tagging module</s> <s sid  ssid >training is typically performed on a sample of the corpus at hand with the trained hmm being saved for subsequent use on the corpus at large</s> <s sid  ssid >the tagging module buffers sequences of ambiguity classes between sentence boundaries</s> <s sid  ssid >these sequences are disambiguated by computing the maximal path through the hmm with the viterbi algorithm</s> <s sid  ssid >operating at sentence granularity provides fast throughput without loss of accuracy as sentence boundaries are unambiguous</s> <s sid  ssid >the resulting sequence of tags is used to select the appropriate stems</s> <s sid  ssid >pairs of stems and tags are subsequently emitted</s> <s sid  ssid >the tagger may function as a complete analysis component providing tagged text to search and indexing components or as a subsystem of a more elaborate analysis such as phrase recognition</s> <s sid  ssid >the problem of tokenization has been well addressed by much work in compilation of programming languages</s> <s sid  ssid >the accepted approach is to specify token classes with regular expressions</s> <s sid  ssid >these may be compiled into a single deterministic finite state automaton which partitions character streams into labeled tokens aho et al  lesk </s> <s sid  ssid >in the context of tagging we require at least two token classes sentence boundary and word</s> <s sid  ssid >other classes may include numbers paragraph boundaries and various sorts of punctuation eg braces of various types commas</s> <s sid  ssid >however for simplicity we will henceforth assume only words and sentence boundaries are extracted</s> <s sid  ssid >just as with programming languages with text it is not always possible to unambiguously specify the required token classes with regular expressions</s> <s sid  ssid >however the addition of a simple lookahead mechanism which allows specification of right context ameliorates this aho et al  lesk </s> <s sid  ssid >for example a sentence boundary in english text might be identified by a period followed bywhitespace followed by an uppercase letter</s> <s sid  ssid >however the uppercase letter must not be consumed as it is the first component of the next token</s> <s sid  ssid >a lookahead mechanism allows us to specify in the sentenceboundary regular expression that the final character matched should not be considered a part of the token</s> <s sid  ssid >this method meets our stated goals for the overall system</s> <s sid  ssid >it is efficient requiring that each character be examined only once modulo lookahead</s> <s sid  ssid >it is easily parameterizable providing the expressive power to concisely define accurate and robust token classes</s> <s sid  ssid >the lexicon module is responsible for enumerating parts of speech and their associated stems for each word it is given</s> <s sid  ssid >for the english word does the lexicon might return do verb and doe pluralnoun it is also responsible for identifying ambiguity classes based upon sets of tags</s> <s sid  ssid >we have employed a threestage implementation first we consult a manuallyconstructed lexicon to find stems and parts of speech</s> <s sid  ssid >exhaustive lexicons of this sort are expensive if not impossible to produce</s> <s sid  ssid >fortunately a small set of words accounts for the vast majority of word occurences</s> <s sid  ssid >thus high coverage can be obtained without prohibitive effort</s> <s sid  ssid >words not found in the manually constructed lexicon are generally both open class and regularly inflected</s> <s sid  ssid >as a second stage a languagespecific method can be employed to guess ambiguity classes for unknown words</s> <s sid  ssid >for many languages eg english and french word suffixes provide strong cues to words possible categories</s> <s sid  ssid >probabalistic predictions of a words category can be made by analyzing suffixes in untagged text kupiec  meteer et al </s> <s sid  ssid >as a final stage if a word is not in the manually constructed lexicon and its suffix is not recognized a default ambiguity class is used</s> <s sid  ssid >this class typically contains all the open class categories in the language</s> <s sid  ssid >dictionaries and suffix tables are both efficiently implementable as letter trees or tries knuth  which require that each character of a word be examined only once during a lookup</s> </section> <section title  performance number > <s sid  ssid >in this section we detail how our tagger meets the desiderata that we outlined in section </s> <s sid  ssid >the system is implemented in common lisp steele </s> <s sid  ssid >all timings reported are for a sun sparcstation</s> <s sid  ssid >the english lexicon used contains  tags m    and  ambiguity classes n   </s> <s sid  ssid >training was performed on  words in articles selected randomly from groliers encyclopedia</s> <s sid  ssid >five iterations of training were performed in a total time of  cpu seconds</s> <s sid  ssid >following is a time breakdown by component training average pseconds per token tokenizer lexicon  iteration  iterations total      tagging was performed on  words in a collection of articles by the journalist dave barry</s> <s sid  ssid >this required a total of of  cpu seconds</s> <s sid  ssid >the time breakdown for this was as follows tagging average pseconds per token tokenizer lexicon viterbi total     it can be seen from these figures that training on a new corpus may be accomplished in a matter of minutes and that tens of megabytes of text may then be tagged per hour</s> <s sid  ssid >when using a lexicon and tagset built from the tagged text of the brown corpus francis and kueera  training on one half of the corpus about  words and tagging the other  of word instances were assigned the correct tag</s> <s sid  ssid >eight iterations of training were used</s> <s sid  ssid >this level of accuracy is comparable to the best achieved by other taggers church  merialdo </s> <s sid  ssid >the brown corpus contains fragments and ungrammaticalities thus providing a good demonstration of robustness</s> <s sid  ssid >a tagger should be tunable so that systematic tagging errors and anomalies can be addressed</s> <s sid  ssid >similarly it is important that it be fast and easy to target the tagger to new genres and languages and to experiment with different tagsets reflecting different insights into the linguistic phenomena found in text</s> <s sid  ssid >in section  we describe how the hmm implementation itself supports tuning</s> <s sid  ssid >in addition our implementation supports a number of explicit parameters to facilitate tuning and reuse including specification of lexicon and training corpus</s> <s sid  ssid >there is also support for a flexible tagset</s> <s sid  ssid >for example if we want to collapse distinctions in the lexicon such as those between positive comparative and superlative adjectives we only have to make a small change in the mapping from lexicon to tagset</s> <s sid  ssid >similarly if we wish to make finer grain distinctions than those available in the lexicon such as case marking on pronouns there is a simple way to note such exceptions</s> </section> <section title  applications number > <s sid  ssid >we have used the tagger in a number of applications</s> <s sid  ssid >we describe three applications here phrase recognition word sense disambiguation and grammatical function assignment</s> <s sid  ssid >these projects are part of a research effort to use shallow analysis techniques to extract content from unrestricted text</s> <s sid  ssid >we have constructed a system that recognizes simple phrases when given as input the sequence of tags for a sentence</s> <s sid  ssid >there are recognizers for noun phrases verb groups adverbial phrases and prepositional phrases</s> <s sid  ssid >each of these phrases comprises a contiguous sequence of tags that sat is fies a simple grammar</s> <s sid  ssid >for example a noun phrase can be a unary sequence containing a pronoun tag or an arbitrar ily long sequence of noun and adjective tags possibly pre ceded by a determiner tag and possibly with an embeddec possessive marker</s> <s sid  ssid >the longest possible sequence is founc eg the program committee but not the program conjunctions are not recognized as part of any phrase for example in the fragment the cats and dogs the cats and dogs will be recognized as two noun phrases</s> <s sid  ssid >prepositional phrase attachment is not performed at this stage of processing</s> <s sid  ssid >this approach to phrase recognition in some cases captures only parts of some phrases however our approach minimizes false positives so that we can rely on the recognizers results</s> <s sid  ssid >partofspeech tagging in and of itself is a useful tool in lexical disambiguation for example knowing that dig is being used as a noun rather than as a verb indicates the words appropriate meaning</s> <s sid  ssid >but many words have multiple meanings even while occupying the same part of speech</s> <s sid  ssid >to this end the tagger has been used in the implementation of an experimental noun homograph disambiguation algorithm hearst </s> <s sid  ssid >the algorithm known as catchword performs supervised training over a large text corpus gathering lexical orthographic and simple syntactic evidence for each sense of the ambiguous noun</s> <s sid  ssid >after a period of training catch word classifies new instances of the noun by checking its context against that of previously observed instances and choosing the sense for which the most evidence is found</s> <s sid  ssid >because the sense distinctions made are coarse the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms</s> <s sid  ssid >initial tests resulted in accuracies of around  for nouns with strongly distinct senses</s> <s sid  ssid >this algorithm uses the tagger in two ways i to determine the part of speech of the target word filtering out the nonnoun usages and ii as a step in the phrase recognition analysis of the context surrounding the noun</s> <s sid  ssid >the phrase recognizers also provide input to a system sopa sibun  which recognizes nominal arguments of verbs specifically subject object and predicative arguments</s> <s sid  ssid >sopa does not rely on information such as arity or voice specific to the particular verbs involved</s> <s sid  ssid >the first step in assigning grammatical functions is to partition the tag sequence of each sentence into phrases</s> <s sid  ssid >the phrase types include those mentioned in section  additional types to account for conjunctions complementizers and indicators of sentence boundaries and an unknown type</s> <s sid  ssid >after a sentence has been partitioned each simple noun phrase is examined in the context of the phrase to its left and the phrase to its right</s> <s sid  ssid >on the basis of this local context and a set of rules the noun phrase is marked as a syntactic subject object predicative or is not marked at all</s> <s sid  ssid >a label of predicative is assigned only if it can be determined that the governing verb group is a form of a predicating verb eg a form of be</s> <s sid  ssid >because this cannot always be determined some predicatives are labeled objects</s> <s sid  ssid >if a noun phrase is labeled it is also annotated as to whether the governing verb is the closest verb group to the right or to the left</s> <s sid  ssid >the algorithm has an accuracy of approximately  in assigning grammatical functions</s> </section> <section title acknowledgments number > <s sid  ssid >we would like to thank marti hearst for her contributions to this paper lauri karttunen and annie zaenen for their work on lexicons and kris halvorsen for supporting this project</s> </section>'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_treino2[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b970b8",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c914b",
   "metadata": {},
   "source": [
    "## Pré-Processamento para Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "25b81d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:11.105642Z",
     "start_time": "2023-05-01T16:29:11.091833Z"
    }
   },
   "outputs": [],
   "source": [
    "def separa_frases(txt: str):\n",
    "    \"\"\"Separa as frases dos papers\"\"\"\n",
    "    txt = txt.split('</s>')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ac376f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:11.121643Z",
     "start_time": "2023-05-01T16:29:11.108628Z"
    }
   },
   "outputs": [],
   "source": [
    "def separa_frases_ouro(txt: str):\n",
    "    \"\"\"Separa as frases dos papers\"\"\"\n",
    "    txt = txt.split('\\n')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "07b1a9d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:11.137312Z",
     "start_time": "2023-05-01T16:29:11.123629Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessamento_secundario(txt: str):\n",
    "    \"\"\"Remoção das demais tags após quebrar no </s>\"\"\"\n",
    "    txt = re.sub(r'<.*?>','', txt)\n",
    "    txt = re.sub('\\/','', txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "49eea4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:11.153292Z",
     "start_time": "2023-05-01T16:29:11.139294Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessamento_secundario_ouro(txt: str):\n",
    "    \"\"\"Remoção das demais tags após quebrar no \\n\"\"\"\n",
    "    txt = re.sub(r'<.*?>','', txt)\n",
    "    txt = re.sub('\\\\n',' ', txt)\n",
    "    txt = re.sub('\\/','', txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "d3254c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:11.168688Z",
     "start_time": "2023-05-01T16:29:11.157311Z"
    }
   },
   "outputs": [],
   "source": [
    "def segundo_preprocessamento(dicionario):\n",
    "    \"\"\"input: dicionário\n",
    "        output: frases pré-processadas\"\"\"\n",
    "    frases_papers_treino={}\n",
    "    for i,v in dicionario.items():\n",
    "        x=[]\n",
    "        for j in v:\n",
    "            j=preprocessamento_secundario(j)\n",
    "            if j!='': #remove itens nulos da lista\n",
    "                x.append(j)\n",
    "        frases_papers_treino[i]= x\n",
    "    return frases_papers_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08555b39",
   "metadata": {},
   "source": [
    "frases_papers_treino={}\n",
    "for i,v in papers_treino3.items():\n",
    "    x=[]\n",
    "    for j in v:\n",
    "        j=preprocessamento_secundario(j)\n",
    "        if j!='': #remove itens nulos da lista\n",
    "            x.append(j)\n",
    "    frases_papers_treino[i]= x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "4c2ba8b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:14.318584Z",
     "start_time": "2023-05-01T16:29:11.170693Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_treino3 = {k: separa_frases(v) for k, v in papers_treino2.items()}\n",
    "frases_papers_treino = segundo_preprocessamento(papers_treino3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "9f66fef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:15.603468Z",
     "start_time": "2023-05-01T16:29:14.320063Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_teste3 = {k: separa_frases(v) for k, v in papers_teste2.items()}\n",
    "frases_papers_teste = segundo_preprocessamento(papers_teste3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "db970354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:15.634468Z",
     "start_time": "2023-05-01T16:29:15.606469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers_teste3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf755be",
   "metadata": {},
   "source": [
    "### Input Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "ee892260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:16.551487Z",
     "start_time": "2023-05-01T16:29:15.636465Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_tokens=[]\n",
    "for i,v in frases_papers_treino.items():\n",
    "    for j in v:\n",
    "        lista_tokens.append(j.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dacef5",
   "metadata": {},
   "source": [
    "### Input FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "faa997cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:16.757220Z",
     "start_time": "2023-05-01T16:29:16.555480Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('preprocessado_treino_fasttext.txt', 'w', encoding='utf-8') as f:\n",
    "    for lista in frases_papers_treino.values():\n",
    "        for line in lista:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a846088",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdc3a2",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de884a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T19:58:49.674748Z",
     "start_time": "2023-03-18T19:53:38.336817Z"
    }
   },
   "source": [
    "model_w2v_cbow = Word2Vec(lista_tokens, min_count=1, epochs=100, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "f0cf7be8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:17.177333Z",
     "start_time": "2023-05-01T16:29:16.760284Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_w2v_cbow'):\n",
    "    model_w2v_cbow = joblib.load('model_w2v_cbow')\n",
    "else:\n",
    "    model_w2v_cbow = Word2Vec(lista_tokens, min_count=1, epochs=1000, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "0e36ffed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:17.193344Z",
     "start_time": "2023-05-01T16:29:17.180324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46323"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v_cbow.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f8589",
   "metadata": {},
   "source": [
    "### SKIP-GRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18baa1b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T20:10:46.886673Z",
     "start_time": "2023-03-18T19:58:49.691979Z"
    }
   },
   "source": [
    "model_w2v_sg = Word2Vec(lista_tokens, min_count=1, epochs=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e2834e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:17.571970Z",
     "start_time": "2023-05-01T16:29:17.196340Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_w2v_sg'):\n",
    "    model_w2v_sg = joblib.load('model_w2v_sg')\n",
    "else:\n",
    "    model_w2v_sg = Word2Vec(lista_tokens, min_count=1, epochs=1000, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "6a6d3bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:17.587375Z",
     "start_time": "2023-05-01T16:29:17.574296Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46323"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v_sg.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8154c",
   "metadata": {},
   "source": [
    "## Fast Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbbcd2d",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52149473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T03:46:40.841069Z",
     "start_time": "2023-03-20T01:28:01.328595Z"
    }
   },
   "source": [
    "model_ft_cbow = fasttext.train_unsupervised('preprocessado_treino_fasttext.txt', model='cbow', minCount=1, epoch=1000)\n",
    "model_ft_cbow.save_model('model_ft_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "83ff10a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:19.723081Z",
     "start_time": "2023-05-01T16:29:17.590310Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('model_ft_cbow.bin'):\n",
    "    model_ft_cbow = load_model('model_ft_cbow.bin')\n",
    "else:\n",
    "    model_ft_cbow = fasttext.train_unsupervised('preprocessado_treino_fasttext.txt', model='cbow', minCount=1, epoch=1000)\n",
    "    #model_ft_cbow.save_model('model_ft_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "47bf6eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:19.738779Z",
     "start_time": "2023-05-01T16:29:19.725054Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46324"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_ft_cbow.get_words( on_unicode_error='ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55261e",
   "metadata": {},
   "source": [
    "if os.path.exists('model_ft_cbow.bin'):\n",
    "    model_ft_cbow = load_model('model_ft_cbow.bin')\n",
    "else:\n",
    "    model_ft_cbow = fasttext.train_unsupervised('preprocessado_treino_fasttext.txt', model='cbow', minCount=1, epoch=100)\n",
    "    model_ft_cbow.save_model('model_ft_cbow.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4263e3f",
   "metadata": {},
   "source": [
    "### SKIP-GRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4bcc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T13:49:51.567566Z",
     "start_time": "2023-03-20T13:49:51.567566Z"
    }
   },
   "source": [
    "model_ft_sg = fasttext.train_unsupervised('preprocessado_treino_fasttext.txt', model='skipgram', minCount=1, epoch=1000)\n",
    "model_ft_sg.save_model('model_ft_sg.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "52994933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:21.637036Z",
     "start_time": "2023-05-01T16:29:19.740749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('model_ft_sg.bin'):\n",
    "    model_ft_sg = load_model('model_ft_sg.bin')\n",
    "else:\n",
    "    model_ft_sg = fasttext.train_unsupervised('preprocessado_treino_fasttext.txt', model='skipgram', minCount=1, epoch=1000)\n",
    "    #model_ft_sg.save_model('model_ft_sg.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "71a1d640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:21.652616Z",
     "start_time": "2023-05-01T16:29:21.638038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46324"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_ft_sg.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed881bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:30:21.498788Z",
     "start_time": "2023-02-28T02:30:21.498788Z"
    }
   },
   "source": [
    "len(model_ft_sg.get_words( on_unicode_error='ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16362643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:15:49.857224Z",
     "start_time": "2023-02-28T02:15:49.830541Z"
    }
   },
   "source": [
    "print(model_ft_sg.get_subwords('tnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "eacf0c06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:21.705806Z",
     "start_time": "2023-05-01T16:29:21.654614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comparando os tokens dos 2 embeddings\n",
    "lista_w2v=[]\n",
    "for i in model_w2v_sg.wv.key_to_index:\n",
    "    lista_w2v.append(i)\n",
    "    \n",
    "lista_ft=[]\n",
    "for i in model_ft_sg.get_words( on_unicode_error='ignore'):\n",
    "    lista_ft.append(i)\n",
    "    \n",
    "list(set(lista_ft)-set(lista_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ce62d",
   "metadata": {},
   "source": [
    "# Medida de Similaridade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd82c6",
   "metadata": {},
   "source": [
    "## Detecção das Seções \n",
    "Abstract, Introdução e Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "f359dd3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:21.721805Z",
     "start_time": "2023-05-01T16:29:21.707806Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_abstract(corpus: str):\n",
    "    regIter = re.finditer(\"(<[Aa][Bb][sS][tT][Rr][aA][cC][tT]>)(.+?)(</[aA][bB][sS][tT][rR][aA][cC][tT]>)\", corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9a6bc61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:21.734378Z",
     "start_time": "2023-05-01T16:29:21.724198Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_intro(corpus: str):\n",
    "    regIter = re.finditer('(<section.*[Ii][nN][tT][rR][oO][dD][Uu][Cc][Tt][Ii][Oo][Nn].*?>(.*)<\\/section>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "654d7de9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:21.749888Z",
     "start_time": "2023-05-01T16:29:21.736764Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_conclusion(corpus: str):\n",
    "    regIter1 = re.finditer('(<section.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/section>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1 = [ t.group(2) for t in regIter1]\n",
    "    regIter2 = re.finditer('(<section.*[Aa][Cc][Kk][Nn][Oo][Ww][Ll][Ee][Dd][Gg][Mm][Ee][Nn][Tt][Ss].*?>(.*)<\\/section>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2 = [ t.group(2) for t in regIter2]\n",
    "    regIter3 = re.finditer('(<section.*[Aa][Cc][Kk][Nn][Oo][Ww][Ll][Ee][Dd][Gg][Ee][Mm][Ee][Nn][Tt][Ss].*?>(.*)<\\/section>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter3]\n",
    "    textos = textos1\n",
    "    for i in textos2:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    for i in textos3:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "cd26ab09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:22.568319Z",
     "start_time": "2023-05-01T16:29:21.760890Z"
    }
   },
   "outputs": [],
   "source": [
    "abstract        = {k: preprocessa_abstract(v) for k, v in papers_treino2.items()}\n",
    "abstract_frases = {k: separa_frases(j) for k, v in abstract.items() for j in v}\n",
    "abstract_frases = segundo_preprocessamento(abstract_frases)\n",
    "abstract_palavras = {k:[word_tokenize(f) for f in v] for k,v in abstract_frases.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "cdc3ba95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:38.896362Z",
     "start_time": "2023-05-01T16:29:22.571276Z"
    }
   },
   "outputs": [],
   "source": [
    "introducao = {k: preprocessa_intro(v) for k, v in papers_treino2.items()}\n",
    "introducao_frases = {k: separa_frases(j) for k, v in introducao.items() for j in v}\n",
    "introducao_frases = segundo_preprocessamento(introducao_frases)\n",
    "introducao_palavras = {k:[word_tokenize(f) for f in v] for k,v in introducao_frases.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "ea3998ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:41.143450Z",
     "start_time": "2023-05-01T16:29:38.896362Z"
    }
   },
   "outputs": [],
   "source": [
    "conclusao = {k: preprocessa_conclusion(v) for k, v in papers_treino2.items()}\n",
    "conclusao_frases = {k: separa_frases(j) for k, v in conclusao.items() for j in v}\n",
    "conclusao_frases = segundo_preprocessamento(conclusao_frases)\n",
    "conclusao_palavras = {k:[word_tokenize(f) for f in v] for k,v in conclusao_frases.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963dd1c",
   "metadata": {},
   "source": [
    "Validei que tudo mede 495"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e82e3",
   "metadata": {},
   "source": [
    "## Aplicação do Word Embedding\n",
    "\n",
    "`model_w2v_cbow`\n",
    "\n",
    "`model_w2v_sg`\n",
    "\n",
    "`model_ft_cbow`\n",
    "\n",
    "`model_ft_sg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63865daa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T18:40:40.888595Z",
     "start_time": "2023-03-29T18:40:40.888595Z"
    }
   },
   "source": [
    "model_ft_sg.get_nearest_neighbors('method', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed9b7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T18:40:40.898721Z",
     "start_time": "2023-03-29T18:40:40.898721Z"
    }
   },
   "source": [
    "model_ft_sg.get_word_vector('method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4ff35f98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:41.498274Z",
     "start_time": "2023-05-01T16:29:41.144488Z"
    }
   },
   "outputs": [],
   "source": [
    "dicio={}\n",
    "for k,v in abstract.items():\n",
    "    x=[]\n",
    "    for lista_frase in v:\n",
    "        soma=0\n",
    "        for palavra in lista_frase:\n",
    "            try:\n",
    "                soma = soma + modelo_embedding.wv[palavra]\n",
    "            except:\n",
    "                soma = soma\n",
    "        x.append(soma)\n",
    "    dicio[k] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c5f6f414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:41.514022Z",
     "start_time": "2023-05-01T16:29:41.501275Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_w2v(sessao_palavras, modelo_embedding):\n",
    "    \"\"\"input: dicionário de sessão das palavras\n",
    "        output: dicionário de vetores\"\"\"\n",
    "    dicio={}\n",
    "    for k,v in sessao_palavras.items():\n",
    "        x=[]\n",
    "        for lista_frase in v:\n",
    "            soma=0\n",
    "            for palavra in lista_frase:\n",
    "                try:\n",
    "                    soma = soma + modelo_embedding.wv[palavra]\n",
    "                except:\n",
    "                    soma = soma\n",
    "            x.append(soma)\n",
    "        dicio[k] = x\n",
    "    return dicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "9320de69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:41.520504Z",
     "start_time": "2023-05-01T16:29:41.516004Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_ft(sessao_palavras, modelo_embedding):\n",
    "    \"\"\"input: dicionário de sessão das palavras\n",
    "        output: dicionário de vetores\"\"\"\n",
    "    dicio={}\n",
    "    for k,v in sessao_palavras.items():\n",
    "        x=[]\n",
    "        for lista_frase in v:\n",
    "            soma=0\n",
    "            for palavra in lista_frase:\n",
    "                try:\n",
    "                    soma = soma + modelo_embedding.get_word_vector(palavra)\n",
    "                except:\n",
    "                    soma=soma\n",
    "            x.append(soma)\n",
    "        dicio[k] = x\n",
    "    return dicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "07852e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:47.060666Z",
     "start_time": "2023-05-01T16:29:41.523526Z"
    }
   },
   "outputs": [],
   "source": [
    "v_w2v_cbow_abstract   = vetor_frases_w2v(abstract_palavras,model_w2v_cbow)\n",
    "v_w2v_cbow_introducao = vetor_frases_w2v(introducao_palavras,model_w2v_cbow)\n",
    "v_w2v_cbow_conclusao  = vetor_frases_w2v(conclusao_palavras,model_w2v_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "c38f65d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:29:52.712910Z",
     "start_time": "2023-05-01T16:29:47.060666Z"
    }
   },
   "outputs": [],
   "source": [
    "v_w2v_sg_abstract   = vetor_frases_w2v(abstract_palavras,model_w2v_sg)\n",
    "v_w2v_sg_introducao = vetor_frases_w2v(introducao_palavras,model_w2v_sg)\n",
    "v_w2v_sg_conclusao  = vetor_frases_w2v(conclusao_palavras,model_w2v_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "9979a318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:30:24.226737Z",
     "start_time": "2023-05-01T16:29:52.712910Z"
    }
   },
   "outputs": [],
   "source": [
    "v_ft_cbow_abstract   = vetor_frases_ft(abstract_palavras,model_ft_cbow)\n",
    "v_ft_cbow_introducao = vetor_frases_ft(introducao_palavras,model_ft_cbow)\n",
    "v_ft_cbow_conclusao  = vetor_frases_ft(conclusao_palavras,model_ft_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3b0990e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:30:55.603085Z",
     "start_time": "2023-05-01T16:30:24.226737Z"
    }
   },
   "outputs": [],
   "source": [
    "v_ft_sg_abstract   = vetor_frases_ft(abstract_palavras,model_ft_sg)\n",
    "v_ft_sg_introducao = vetor_frases_ft(introducao_palavras,model_ft_sg)\n",
    "v_ft_sg_conclusao  = vetor_frases_ft(conclusao_palavras,model_ft_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d1d99156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:30:55.616032Z",
     "start_time": "2023-05-01T16:30:55.603738Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_sim_coseno(a,b):\n",
    "    if ((numpy.linalg.norm(a)==0) |(numpy.linalg.norm(b)==0)):\n",
    "        return 0\n",
    "    else:\n",
    "        valor_coseno = (numpy.dot(a, b))/(numpy.linalg.norm(a)* numpy.linalg.norm(b))\n",
    "        return valor_coseno\n",
    "    \n",
    "def dicionario_similaridade(vetor_a,vetor_b):\n",
    "    \"\"\"Input: vetor correspondente às frases das seções a e b\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    dicionario={}\n",
    "    for i in range(len(vetor_a)):\n",
    "        for j in range(len(vetor_b)):\n",
    "            dicionario[i,j]= funcao_sim_coseno(vetor_a[i],vetor_b[j])\n",
    "    return dicionario\n",
    "\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "bbd028eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:30:55.632114Z",
     "start_time": "2023-05-01T16:30:55.616032Z"
    }
   },
   "outputs": [],
   "source": [
    "def aplicando_dicionario_similaridade(v1,v2):\n",
    "    \"\"\"cálculo da similaridade, pegando os 3 vetores mais similares\"\"\"\n",
    "    x = []\n",
    "    for i,j in zip(range(len(v1)),range(len(v2))):\n",
    "        x.append(dicionario_similaridade(v1[i], v2[j]))\n",
    "        \n",
    "    top_3_indices = []\n",
    "    for i in range(len(x)):\n",
    "        top_3_indices.append(sorted(x[i], key=x[i].get, reverse=True)[:3])\n",
    "        \n",
    "    lista_1=[]\n",
    "    for i in top_3_indices:\n",
    "        top3=[]\n",
    "        for j in i:\n",
    "            top3.append(j[0])\n",
    "        lista_1.append(list(set(top3)))\n",
    "        \n",
    "    return x,top_3_indices,lista_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b465f52e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:30:55.651844Z",
     "start_time": "2023-05-01T16:30:55.634112Z"
    }
   },
   "outputs": [],
   "source": [
    "def frases_interseccao(lista_1,lista_2):\n",
    "    \"\"\"comparando se os índices similares pertencem às duas listas (abstract e conclusão) e (abstract e intro) \n",
    "    e retorna os índices em comum\"\"\"\n",
    "    indices_objetivo=[]\n",
    "    for i,j in zip(lista_1,lista_2):\n",
    "        indices_objetivo.append(list(set.intersection(*map(set,[i,j]))))\n",
    "    return indices_objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "fb0d31b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:30:55.666933Z",
     "start_time": "2023-05-01T16:30:55.653851Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frases_objetivo(indices_objetivo, idx_paper):\n",
    "    \"\"\"retorna a frase objetivo [do abstract] a partir dos indices\"\"\"\n",
    "    frase_objetivo={}\n",
    "    for idx,idx_f,idx_ind in zip(idx_paper,range(len(list(abstract_frases.values()))),indices_objetivo):\n",
    "        lista_one=[]\n",
    "        for j in idx_ind:\n",
    "            lista_one.append(list(abstract_frases.values())[idx_f][j])\n",
    "            #print(idx_f)\n",
    "            #print(j)\n",
    "            #print(frases_abstract[idx_f][j])\n",
    "        frase_objetivo[idx] = lista_one\n",
    "    return frase_objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0066937",
   "metadata": {},
   "source": [
    "### Similaridade w2v cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "c2cdea0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:31:12.024110Z",
     "start_time": "2023-05-01T16:30:55.669594Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_cbow,indices_ab_intro_cbow,lista_idx_ab_intro_cbow =aplicando_dicionario_similaridade(list(v_w2v_cbow_abstract.values()),list(v_w2v_cbow_introducao.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_cbow,indices_ab_conc_cbow,lista_idx_ab_c_cbow = aplicando_dicionario_similaridade(list(v_w2v_cbow_abstract.values()),list(v_w2v_cbow_conclusao.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_cbow = frases_interseccao(lista_idx_ab_intro_cbow,lista_idx_ab_c_cbow)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_cbow_w2v = get_frases_objetivo(indices_objetivo_cbow, papers_treino.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564694a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T22:42:55.478574Z",
     "start_time": "2023-04-30T22:42:55.448565Z"
    }
   },
   "source": [
    "joblib.dump(frase_objetivo_cbow_w2v,'frase_objetivo_w2v_cbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64677996",
   "metadata": {},
   "source": [
    "### Similaridade w2v skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "382216cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:31:28.299677Z",
     "start_time": "2023-05-01T16:31:12.024110Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_sg,indices_ab_intro_sg,lista_idx_ab_intro_sg =aplicando_dicionario_similaridade(list(v_w2v_sg_abstract.values()),list(v_w2v_sg_introducao.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_sg,indices_ab_conc_sg,lista_idx_ab_c_sg = aplicando_dicionario_similaridade(list(v_w2v_sg_abstract.values()),list(v_w2v_sg_conclusao.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_sg = frases_interseccao(lista_idx_ab_intro_sg,lista_idx_ab_c_sg)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_sg_w2v = get_frases_objetivo(indices_objetivo_sg, papers_treino.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d61e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T22:43:19.231593Z",
     "start_time": "2023-04-30T22:43:19.199468Z"
    }
   },
   "source": [
    "joblib.dump(frase_objetivo_sg_w2v,'frase_objetivo_w2v_sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a82f3e",
   "metadata": {},
   "source": [
    "vai até 4 a 14, pois o abstract tem 5 frases e a conclusao tem 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31854dd3",
   "metadata": {},
   "source": [
    "### Similaridade fasttext cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "9ddeb858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:31:44.403263Z",
     "start_time": "2023-05-01T16:31:28.299677Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_cbow_ft,indices_ab_intro_cbow_ft,lista_idx_ab_intro_cbow_ft =aplicando_dicionario_similaridade(list(v_ft_cbow_abstract.values()),list(v_ft_cbow_introducao.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_cbow_ft,indices_ab_conc_cbow_ft,lista_idx_ab_c_cbow_ft = aplicando_dicionario_similaridade(list(v_ft_cbow_abstract.values()),list(v_ft_cbow_conclusao.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_cbow_ft = frases_interseccao(lista_idx_ab_intro_cbow_ft,lista_idx_ab_c_cbow_ft)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_cbow_ft  = get_frases_objetivo(indices_objetivo_cbow_ft, papers_treino.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6997b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T22:43:44.908683Z",
     "start_time": "2023-04-30T22:43:44.893061Z"
    },
    "scrolled": true
   },
   "source": [
    "joblib.dump(frase_objetivo_cbow_ft,'frase_objetivo_cbow_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b72db927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:31:44.456465Z",
     "start_time": "2023-05-01T16:31:44.403263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['contrary to claims found elsewhere in the literature we argue that a tagger based on markov models performs at least as well as other current approaches including the maximum entropy framework'],\n",
       " 2: ['intersentence similarity is replaced by rank in the local context'],\n",
       " 3: ['this paper presents a corpusbased approach to word sense disambiguation that builds an ensemble of naive bayesian classifiers each of which is based on lexical features that represent cooccurring words in varying sized windows of context'],\n",
       " 4: ['we present a new parser for parsing down to penn treebank style parse trees that achieves average precisionrecall for sentences of and less and for of length and less when trained and tested on the previously established standard sections of the wall street journal treebank',\n",
       "  'this represents a decrease in error rate over the best singleparser results on this corpus'],\n",
       " 5: ['the system was developed and tested using essaylength responses to prompts on the test of english as a foreign language toefl'],\n",
       " 6: [],\n",
       " 7: ['the first two systems called nlg and nlg require a corpus marked only with domainspecific semantic attributes while the last system called nlg requires a corpus marked with both semantic attributes and syntactic dependency information'],\n",
       " 8: ['since a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy as measured against the upenn treebank as a gold standard',\n",
       "  'in this paper we report adapting a lexic al ized probabilistic contextfree parser to information extraction and evaluate this new technique on muc template elements and template relations'],\n",
       " 9: ['a maximum entropy approach to natural lanprocessing'],\n",
       " 10: ['we propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms',\n",
       "  'the method is evaluated on the causative and conative alternations but is generally applicable and does not require a priori knowledge specific to the alternation'],\n",
       " 13: ['our approach we next describe how our choice of techniques satisfies the listed in section'],\n",
       " 14: ['perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging',\n",
       "  'the fact that a simple rulebased tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below'],\n",
       " 15: [],\n",
       " 16: ['the conclusions are broadly in agreement with those of merialdo but give greater detail about the contributions of different parts of the model',\n",
       "  'in two of the patterns the reestimation ultimately reduces the accuracy of the tagging rather than improving it'],\n",
       " 18: ['performance is comparable to or better than the performance of similar systems but we emphasize the simplicity of retraining for new domains'],\n",
       " 19: ['we first describe the older constraint grammar parser where many of the ideas come from'],\n",
       " 20: [],\n",
       " 31: ['we next determine the possible translations from among the candidates using one of the two methods that we have developed'],\n",
       " 32: ['therefore we present a method that makes the system substantially fasterthis approach can also be applied to other similar tasks such as chunking and partofspeech tagging'],\n",
       " 33: ['the graph model is built by linking pairs of words which participate in particular syntacticrelationships',\n",
       "  'we focus on the symmetric relationship between pairs of nouns which occur to gether in lists'],\n",
       " 35: ['evaluating cluster quality has always been a difficult task',\n",
       "  'we present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from wordnet the answer key'],\n",
       " 37: ['this paper presents a machine learning approach toquestion classification',\n",
       "  'we learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types and eventually classifies questions into finegrained classes'],\n",
       " 39: ['this paper presents a deterministic dependency parser based on memorybased learning which parses english text in linear time'],\n",
       " 41: ['this paper describes the role of supertagging in a widecoverage ccg parser which uses a loglinear model to select an analysis',\n",
       "  'we show that large increases in speedcan be obtained by tightly integrating the su pertagger with the ccg grammar and parserthis is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammarwe also further reduce the derivation space us ing constraints on category combination'],\n",
       " 43: ['analysis of pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase'],\n",
       " 45: ['in this paper we introduce a new evaluation method orange for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations'],\n",
       " 47: ['we present a new hmm tagger that exploits context on both sides of a word to be tagged and evaluate it in both the unsupervised and supervised case',\n",
       "  'along the way we present the first comprehensive comparison of unsupervised methods for partofspeech tagging noting that published results to date have not been comparable across corpora or lexicons'],\n",
       " 48: [],\n",
       " 50: ['we present an algorithm designed for the terascale for mining isa relations that achieves similar performance to a stateoftheart linguisticallyrich method',\n",
       "  'we focus on the accuracy of these two systems as a func tion of processing time and corpus size'],\n",
       " 51: ['we identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word',\n",
       "  'finally we consider theimpact that this has on one application of distributional similarity methods judging the composition ality of collocations'],\n",
       " 53: ['the system combines a machine learning technique with an inference procedurebased on integer linear programming that supports the incorporation of linguistic and struc tural constraints into the decision process'],\n",
       " 54: ['identifying sentiments the affective parts of opinions is a challenging problem',\n",
       "  'the system contains a module for determining word sentiment and another for combining sentiments within a sentence'],\n",
       " 55: ['we present a new corpus that is suitedto our task and a discriminative treeto tree transduction model that can naturallyaccount for structural and lexical mis matches'],\n",
       " 56: ['in this paper we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native l english writing',\n",
       "  'we show that models of use for these parts of speech can be learned with an accuracy of and respectively on l text and present first results in an error detection task for l writing'],\n",
       " 58: [],\n",
       " 59: ['in addition we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation',\n",
       "  'we present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems'],\n",
       " 60: [],\n",
       " 61: ['in addition to a high accuracy short parsing and training times are the most important properties of a parser'],\n",
       " 62: ['in this paper we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target',\n",
       "  'we propose a treebased simplification model tsm which to our knowledge is the first statistical simplification model covering splitting dropping reorderingand substitution integrally',\n",
       "  'the evaluation shows that our model achieves better readability scores than a set of baseline systems'],\n",
       " 63: ['in our experi ments we show that since our features areable to capture a more abstract representation of tweets our solution is more ef fective than previous ones and also more robust regarding biased and noisy data which is the kind of data provided by these sources'],\n",
       " 64: ['we evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences'],\n",
       " 74: [],\n",
       " 88: [],\n",
       " 97: ['in this paper we describe a new model for word alignment in statistical trans lation and present experimental results',\n",
       "  'the idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions',\n",
       "  'to achieve this goal the approach us es a firstorder hidden markov model hmm for the word alignment problem as they are used successfully in speech recognition for the time alignment prob lem'],\n",
       " 99: ['we introduce a general frame work for answer extraction which exploits semantic role annotations in the framenetparadigm'],\n",
       " 101: ['we show for the first time that incorporatingthe predictions of a word sense disambigua tion system within a typical phrasebased statistical machine translation smt model consistently improves translation qualityacross all three different iwslt chineseenglish test sets as well as producing sta tistically significant improvements on the larger nist chineseenglish mt task',\n",
       "  'instead of directly incor porating a sensevalstyle wsd system weredefine the wsd task to match the ex act same phrasal translation disambiguation task faced by phrasebased smt systemsour results provide the first known empirical evidence that lexical semantics are in deed useful for smt despite claims to the contrary'],\n",
       " 102: [],\n",
       " 103: ['v measure provides an elegant solution tomany problems that affect previously defined cluster evaluation measures includ ing dependence on clustering algorithm or data set the problem of matching where the clustering of only a portion of datapoints are evaluated and accurate evaluation and combination of two desirable aspects of clustering homogeneity and completeness'],\n",
       " 104: ['in our experiments the resultingrelatedness measure is the wordnetbased measure most highly correlated with human similar ity judgments by rank ordering at'],\n",
       " 105: ['we consider the problem of learning toparse sentences to lambdacalculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar ccg',\n",
       "  'we also present a new online algorithm for inducing a weighted ccg',\n",
       "  'results for the approach on atis data show fmeasure in recovering fully correct semantic analyses and fmeasure by a partialmatch criterion a more than improvement over the partialmatch figure reported by he and young'],\n",
       " 106: ['we present a nonparametric bayesian model of tree structures based on the hierarchical dirichlet process hdp',\n",
       "  'in addition to presenting a fully bayesianmodel for the pcfg we also develop an ef ficient variational inference procedure'],\n",
       " 108: ['moreover this paper evaluates the complementary nature between our tree kernel and a stateoftheart linear kernel',\n",
       "  'evaluation on the ace rdc corpora shows that our dynamic contextsensitive tree span is much more suitable for relation extraction than spt and our tree kernel outperforms the stateoftheart collins and duffys convolution tree kernel'],\n",
       " 110: [],\n",
       " 112: [],\n",
       " 113: ['in this paper we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages',\n",
       "  'in addition we characterize the different approaches of the participating systems report the test results and provide a first analysis of these results'],\n",
       " 114: ['we describe a twostage optimization of the maltparser system for the ten languages in the multilingual track of the conll shared task on dependency parsing',\n",
       "  'the first stage consists in tuning a singleparsersystem for each language by optimizing parameters of the parsing algorithm the fea ture model and the learning algorithm'],\n",
       " 115: [],\n",
       " 116: [],\n",
       " 117: ['a major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translationrulesets',\n",
       "  'wedescribe new lookup algorithms for hierar chical phrasebased translation that reduce the empirical computation time by nearly two orders of magnitude making onthefly lookup feasible for source phrases with gaps'],\n",
       " 118: ['we develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpusand learning the domains in which to consider each word'],\n",
       " 119: ['parser actions are determined by a classifier based on features that represent the current state of the parser',\n",
       "  'in the multilingual track we train three lr models for each of the ten languages and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme'],\n",
       " 121: ['the ihmmbased method significantly outperforms the stateoftheart terbased alignment model in our experiments on nist benchmark datasets',\n",
       "  'our combined smt system using the proposed method achieved the best chinesetoenglish translation result in the constrained training track of the'],\n",
       " 122: ['specifically we attempt to leverage on the resources available for english and by employing machine translation generate resources for subjectivity analysis in other languages',\n",
       "  'through comparative evaluations on two different languages romanian and spanish we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language'],\n",
       " 123: ['as a parsing algorithm bp is both asymptotically and empirically efficient'],\n",
       " 124: ['this is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text'],\n",
       " 125: ['we improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type'],\n",
       " 126: ['so we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses',\n",
       "  'when combined with our previous work on forestbased decoding it achieves a bleu points improvement over the baseline and even outperforms the hierarchical system of hiero by points'],\n",
       " 127: ['we then test the method on two classes of features that address deficiencies in the hiero hierarchical phrasebased model first we simultaneously train a large number of marton and resniks soft syntactic constraints and second we introduce a novel structural distortion model'],\n",
       " 128: ['we investigate five tasks affect recognition word similarity recognizing textual entailment event temporal ordering and word sense disambiguation',\n",
       "  'we propose a technique for bias correction that significantly improves annotation quality on two tasks'],\n",
       " 129: [],\n",
       " 130: ['we show that lexical cohesion can be placed in a bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment maximizing the observation likelihood in such a model yields a lexicallycohesive segmentation'],\n",
       " 131: ['recent papers have given contradictory results when comparing bayesian estimators to expectation maximization em for unsupervised hmm pos tagging and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the hmm',\n",
       "  'in terms of times of convergence we find that variational bayes was the fastest of all the estimators especially on large data sets and that explicit gibbs sampler both pointwise and sentenceblocked were generally faster than their collapsed counterparts on large data sets'],\n",
       " 132: ['we study both approaches under the framework of beamsearch',\n",
       "  'by developing a graphbased and a transitionbased dependency parser we show that a beamsearch decoder is a competitive choice for both methods'],\n",
       " 134: ['this is made possible by performing joint inference across mentions in contrast to the pairwise classification typically used in supervised methods and by using markov logic as a representation language which enables us to easily express relations like apposition and predicate nominals'],\n",
       " 136: ['in this paper we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures',\n",
       "  'in experiments we demonstrate that the model when coupled with a discriminative reranking technique achieves stateoftheart performance when tested on two publicly available corpora'],\n",
       " 137: ['in this paper we view such interactions in light of composiand present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure',\n",
       "  'our experiments show that simple heuristics based on compositional semantics can perform better than learningbased methods that do not incorporate compositional semantics accuracy of vs but a method that integrates compositional semantics into learning performs better than all other alternatives'],\n",
       " 138: ['in this paper we present a novel hierarchical phrase reordering model aimed at improvingnonlocal reorderings which seamlessly in tegrates with a standard phrasebased system with little loss of computational efficiency',\n",
       "  'we contrast our model with reordering models commonly used in phrasebased systems and show that our approach provides statistically significant bleu point gains for two language pairs chineseenglish on mt and on mt and arabicenglish on mt'],\n",
       " 139: ['we show that jointly parsing a bitext can substantially improve parse quality on both sides'],\n",
       " 140: ['we present the first unsupervised approach to the problem of learning a semantic parser using markov logic',\n",
       "  'the map semantic parse of a sentence is obtained by recursively assigning its parts to lambdaform clusters and composing them'],\n",
       " 141: ['under this paradigm we use weights from the expectation semiring eisner to compute firstorder statistics eg the expected hypothesis length or feature counts over packed forests of translations lattices or hypergraphs',\n",
       "  'we then introduce novel semiring which computes secondorder statistics eg the variance of the hypothesis length or the gradient of entropy'],\n",
       " 142: ['a significant portion of the worlds text is tagged by readers on social bookmarkwebsites attribution an inherent problem in these corpora because most pages have multiple tags but the tags do not always apply with equal specificity across the whole document'],\n",
       " 143: ['we find that when combined nonexpert judgments have a highlevel of agreement with the existing goldstandard judgments of machine translation quality and correlate more strongly with expert judgments than bleu does'],\n",
       " 144: ['we describe an extension of semisupervised structured conditional models ssscms to the dependency parsing problem whose framework is originally proposed in suzuki and isozaki'],\n",
       " 146: ['we explore the models characteristics using two large corpora each with over ten different languages and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages'],\n",
       " 147: ['we propose a highly scalable implementation based on distributional similarity implemented in the mapreduce framework and deployed over a billion word crawl of the web',\n",
       "  'the pairwise similarity between million terms is computed in hours using quadcore nodes'],\n",
       " 148: [],\n",
       " 149: ['primary contributions include the presentation of a simpletoreproduce highperforming baseline and the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon and perhaps best addressed by noncoreference systems'],\n",
       " 150: [],\n",
       " 151: ['by taking advantage of the observation that a lot of product features are phrases a concept of phrase dependency parsing is introduced which extends traditional dependency parsing to phrase level',\n",
       "  'experimental evaluations show that the mining task can benefit from phrase dependency parsing'],\n",
       " 152: [],\n",
       " 153: ['we describe a new approach to smt adaptation that weights outofdomain phrase pairs according to their relevance to the target domain determined by both how similar to it they appear to be and whether they belong to general language or not',\n",
       "  'this extends previous work on discriminative weighting by using a finer granularity focusing on the properties of instances rather than corpus components and using a simpler training procedure'],\n",
       " 154: ['this cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time'],\n",
       " 155: ['we show moreover that our approach provides two novel ways to represent adjective meanings alternative to its representation via corpusbased cooccurrence vectors both outperforming the latter in an adjective clustering task'],\n",
       " 156: ['we use higherorder unification to define a hypothesis space containing all grammars consistent with the training data and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a loglinear parsing model'],\n",
       " 157: ['during inference of the probabilistic model we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules'],\n",
       " 158: ['in this paper we present a multilevel generative model that reasons jointly about latent topics and geographical regions',\n",
       "  'the model also enables prediction of an authors geographic location from raw text outperforming both text regression and supervised topic models'],\n",
       " 159: ['this paper introduces algorithms for nonparsing based on decomposi we focus on parsing algorithms for nonhead a generalization of headautomata models to nonprojective structures'],\n",
       " 160: ['the projected parsers from our system result in stateoftheart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages'],\n",
       " 161: ['in sentiment prediction tasks these representations outperform other stateoftheart approaches on commonly used datasets such as movie reviews without using any predefined sentiment lexica or polarity shifting rules',\n",
       "  'our algorithm can more accurately predict distributions over such labels compared to several competitive baselines'],\n",
       " 162: ['we explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain',\n",
       "  'as these sentences are not themselves identical the indomain data we call them these subcorpora the size of the original can then used to train small domainadapted statistical machine translation smt systems which outperform systems trained on the entire corpus'],\n",
       " 163: ['in line with recent works emphasizing the need of largescale annotation efforts for textual entailment our work aims to the scarcity of data available to train evaluate systems and the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality'],\n",
       " 164: ['unlike the popular mert algorithm och our pairwise ranking optimization pro method is not limited to a handful of parameters and can easily handle systems with thousands of features',\n",
       "  'it uses offtheshelf linear binary classifier software and can be built on top of an existing mert framework in a matter of hours'],\n",
       " 166: ['our novel doubles compared with the ner system the redundancy inherent in tweets to achieve this performance using labeledlda to exploit freebase dictionaries as a source of distant supervision'],\n",
       " 167: [],\n",
       " 168: ['in this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods'],\n",
       " 169: ['we present a transitionbased system for joint partofspeech tagging and labeled dependency parsing with nonprojective trees',\n",
       "  'experimental evaluation on chinese czech english and german shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system which lead to improved stateoftheart results for all languages'],\n",
       " 170: ['two apparently opposing dop models exist in the literature one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank',\n",
       "  'together with a pcfgreduction of dop we obtain improved accuracy and efficiency on the wall street journal treebank our results show an relative reduction in error rate over previous models and an average processing time of seconds per wsj sentence'],\n",
       " 171: ['in addition we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material'],\n",
       " 172: ['we show how the use of morphological information can improve the performance on rare words and that this is robust across a wide range of languages'],\n",
       " 173: ['we also explore the use of a gaussian prior and a simple cutoff for smoothing'],\n",
       " 174: ['we introduce methods to learn splitting rules from monolingual and parallel corpora',\n",
       "  'we evaluate them against a gold standard and measure their impact on performance of statistical mt systems'],\n",
       " 175: ['a disambiguation svm kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia',\n",
       "  'the resultingmodel significantly outperforms a less in formed baseline'],\n",
       " 176: ['this paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation mt systems',\n",
       "  'the context of a whole document of translations rather than a single sentence is taken into account to produce the alignment'],\n",
       " 177: ['we show that those extensions can make the mst framework computationally intractable but that the intractability can be circumvented with new approximate parsing algorithms',\n",
       "  'we conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for czech and danish'],\n",
       " 178: ['in this paper we show that tree kernels are very helpful in the processing of natural language as a we provide a simple algorithm to compute tree kernels in linear average running time and b our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods'],\n",
       " 179: ['our results show that determining subjectivity is a much harder problem than determining orientation alone'],\n",
       " 180: ['many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semanticfeatures',\n",
       "  'we demonstrate that net overlap score can be used as ameasure of the words degree of member ship in the fuzzy category of sentimentthe core adjectives which had the high est net overlap scores were identifiedmost accurately both by step and by hu man annotators while the words on the periphery of the category had the lowest scores and were associated with low rates of interannotator agreement'],\n",
       " 181: ['our measure can be exactly calculated in quadratic time',\n",
       "  'furthermore we will show how some evaluation measures can be improved'],\n",
       " 182: ['we argue that the machine translation community is overly reliant on the bleu machine translation evaluation metric',\n",
       "  'we show that an improved bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality and give two significant counterexamples to bleus correlation with human judgments of quality'],\n",
       " 183: ['we argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly',\n",
       "  'this differs from current stateoftheart models knight and marcu that treat noisy parse trees for both compressed and uncompressed sentences as gold standard when calculating model parameters'],\n",
       " 184: ['we compare evaluation results for these systems by human domain experts human nonexperts and several automatic evaluation metrics inand we that correlate best with human judgments but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone'],\n",
       " 185: ['in this paper we present trofi trope finder a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised wordsense disambiguation and clustering techniques',\n",
       "  'using the trofi algorithm we also build the trofi example base an extensible resource of annotated literalnonliteral examples which is freely available to the nlp research community'],\n",
       " 186: ['we investigate the lexical and syntactic flexibility of a class of idiomatic expressions',\n",
       "  'we also propose a means for automatically determining which syntactic forms a particular idiom can appear in and hence should be included in its lexical representation'],\n",
       " 187: ['we performed experiments on extracting gene and protein interactions from two different data sets',\n",
       "  'the results show that our approach outperforms most of the previous methods based on syntactic and semantic information'],\n",
       " 188: ['our algorithm uses the full graph of the lkb efficiently performing better than previous approaches in english allwords datasets'],\n",
       " 189: ['our work places sense induction in a bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words'],\n",
       " 191: ['the principal intended area of application is the representation of lexical entries for natural language processing and we use examples from this domain throughout',\n",
       "  'the goal of the is the design of a simple language that i has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition ii can express all the evident generalizations about such entries iii has an explicit theory of inference iv is computationally tractable and v has an explicit declarative semantics'],\n",
       " 192: ['we sketch and illustrate an approach to machine translation that exploits the potential of simultaneous correspondences between separate levels of linguistic representation as in the of codescriptions'],\n",
       " 193: ['indeed the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of named entity recognition systems',\n",
       "  'we conclude with observations about the domain independence of the competition and of our experiments'],\n",
       " 194: [],\n",
       " 198: ['we present a discriminative large margin approach to featurebased matching for word alignment',\n",
       "  'even with only labeled train ing examples and simple features whichincorporate counts from a large unlabeled corpus we achieve aer perfor mance close to ibm model in muchless time',\n",
       "  'including model predic tions as features we achieve a relativeaer reduction of in over inter sected model alignments'],\n",
       " 200: ['this paper presents a maximum entropyword alignment algorithm for arabic english based on supervised training datawe demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of su pervised and unsupervised methods yields superior performance',\n",
       "  'the probabilisticmodel used in the alignment directly models the link decisions'],\n",
       " 202: ['opines novel use ofrelaxation labeling for finding the semantic orientation of words in con text leads to strong performance on the tasks of finding opinion phrases and their polarity'],\n",
       " 203: ['this paper presents a new approach to phraselevel sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions',\n",
       "  'with thisapproach the system is able to automat ically identify the contextual polarity for a large subset of sentiment expressionsachieving results that are significantly bet ter than baseline'],\n",
       " 204: ['we pursue another aspect of opinion analysis identi fying the sources of opinions emotions and sentiments'],\n",
       " 205: ['we also show that for an all words wsd task this automatic method is best focussed on words that are salient to the domain and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus'],\n",
       " 206: ['the algorithm can enumerate all possible decomposition structures andfind the highest probability sequence together with the corresponding decomposi tion structure in polynomial time',\n",
       "  'we also present an efficient decoding algorithm based on the easiestfirst strategy which gives comparably good performance tofull bidirectional inference with significantly lower computational cost',\n",
       "  'exper imental results of partofspeech tagging and text chunking show that the proposedbidirectional inference methods consis tently outperform unidirectional inference methods and bidirectional memms give comparable performance to that achievedby stateoftheart learning algorithms in cluding kernel support vector machines'],\n",
       " 207: ['using this representation the parsing algorithmof eisner is sufficient for search ing over all projective trees in on time'],\n",
       " 208: ['in addition we present plans for a more cognitively soundsequential model taking into considera tion a larger set of basic emotions'],\n",
       " 209: ['as the performance of theorem proving turnsout to be highly dependent on not read ily available background knowledge we incorporate model building a technique borrowed from automated reasoning and show that it is a useful robust method to approximate entailment',\n",
       "  'ourresults also show that the different techniques that we employ perform very dif ferently on some of the subsets of the rte corpus and as a result it is useful to use the nature of the dataset as a feature'],\n",
       " 210: ['we present a novel approach to relation extraction based on the observation thatthe information required to assert a rela tionship between two named entities in the same sentence is typically capturedby the shortest path between the two entities in the dependency graph'],\n",
       " 222: ['as the focus of information extraction is shifting from nominal information such as named entity to verbal information such as function and interaction of substances applica tion of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sen tences is in demand',\n",
       "  'interannotator agreement test indicated that the writ ing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation and that annotation can be stably done by linguists without much knowledge of bi ology with appropriate guidelines regarding to linguistic phenomena par ticular to scientific texts'],\n",
       " 223: ['the second international chinese word segmentation bakeoff was held in the summer of to evaluate the current state of the art in word segmentationtwenty three groups submitted result sets over two tracks and four different corpora'],\n",
       " 225: ['our segmenter was built using a condi tional random field sequence model that provides a framework to use a large number of linguistic features such as character identity morphological and character reduplication features',\n",
       "  'our final system achieved a fscore of as hk pk and msr'],\n",
       " 226: ['we use a decision tree approach inspired by contextual spelling systems for detection and correction suggestions and a large language model trained on the gigaword corpus to provide additional information to filter out spurious suggestions'],\n",
       " 228: ['my goal here is only to demonstrate the value of some previously unused kinds of information that are always available for translation modeling and to show how these information sources can be integrated with others'],\n",
       " 229: ['youre a so youre a senior now'],\n",
       " 231: ['the design of the system is based on the results of a corpus analysis previously reported which highlighted the prevalence of discoursenew descriptions in newspaper corpora'],\n",
       " 232: ['the paper discusses a number of problems with these annotations and concludes that rethinking of the coreference task is needed before the task is expanded',\n",
       "  'in particular it suggests a division of labor whereby annotation of the coreference relation proper is separated from other tasks such as annotation of bound anaphora and of the relation between a subject and a predicative np'],\n",
       " 234: ['we do this by means of experiments involving the task of morphosyntactic word class tagging on the basis of three different tagged corpora'],\n",
       " 235: ['a lexicalized probabilistic topdown parser is then presented which performs very well in terms of both the accuracy of returned parses and the efficiency with which they are found relative to the best broadcoverage statistical parsers'],\n",
       " 243: ['edinburgh in this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work'],\n",
       " 244: ['in the appendix we present an efficient training algorithm for the alignment models presented'],\n",
       " 245: ['this article describes a new approach to the generation of referring expressions',\n",
       "  'cost functions are used to guide the search process and to give preference to some solutions over others',\n",
       "  'the current approach has four main advantages graph structures have been studied extensively and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs many existing generation algorithms can be reformulated in terms of graphs and this enhances comparison and integration of the various approaches the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions and the combined use of graphs and cost functions paves the way for an integration of rulebased generation techniques with more recent stochastic approaches'],\n",
       " 246: [],\n",
       " 248: ['in this article we report on our work using the strand system for mining parallel text on the world wide webfirst reviewing the original algorithm and results and then presenting a set of significant enhancements',\n",
       "  'these enhancements include the use of supervised learning based on structural features of documents to improve classification performance a new contentbased measure of translational equivalence and adaptation of the system to take advantage of the internet archive for mining parallel text from the web on a large scale'],\n",
       " 258: [],\n",
       " 263: ['in the vsm approach the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus'],\n",
       " 272: ['it exposes the mathematics and underlying assumptions of agreement coefficients covering krippendorffs alpha as well as scotts pi and cohens kappa discusses the use of coefficients in several annotation tasks and argues that weighted alphalike coefficients traditionally less used than kappalike measures in computational linguistics may be more appropriate for many corpus annotation tasksbut that their use makes the interpretation of the value of the coefficient even harder'],\n",
       " 276: ['we propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others infer goals being sought and cooperate in their achievement'],\n",
       " 277: ['extraposition grammars are an extension of definite clause grammars and are similarly defined in terms of logic clauses',\n",
       "  'the extended formalism makes it easy to describe left extraposition of constituents an important feature of natural language syntax'],\n",
       " 279: [],\n",
       " 280: ['in several experiments with different english grammars and sentences timings indicate a fiveto tenfold speed advantage over earleys contextfree parsing algorithm algorithm parses a sentence strictly from left to right that is starts parsing as soon as the user types in the first word of a sentence without waiting for completion of the sentence'],\n",
       " 281: ['as a result many systems for representing the semantics of sentences have ignored scoping or generated scopings with mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow',\n",
       "  'the algorithm is not profligate as are those based on permutation of quantifiers and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy'],\n",
       " 282: [],\n",
       " 283: ['a semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed'],\n",
       " 284: ['they specify entities in an evolving model of the discourse that the listener is constructing'],\n",
       " 286: ['the enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semanticheaddriven fashion'],\n",
       " 296: [],\n",
       " 307: ['this model is associated with features identifiable from text analysis including orthography and part of speech to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech phrases and phrases that directly signal the structure of a discourse been variously termed words discourse markers discourse connectives particles the computational linguistic and conversational analysis',\n",
       "  'although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse the question of how speakers and hearers accomplish this disambiguation is rarely addressed'],\n",
       " 310: ['this paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology'],\n",
       " 313: [],\n",
       " 315: [],\n",
       " 316: ['we have also greatly shortened the discussion of criteria for and constraints on a possible semantic theory as a foundation for this work'],\n",
       " 318: [],\n",
       " 323: ['the algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of texts'],\n",
       " 326: ['technology introduce a novel inversion transduction formalism bilingual modeling of sentencepairs and the concept of parsing a variety of parallel corpus analysis applications',\n",
       "  'aside from the bilingual orientation three major features distinguish the formalism from the finitestate transducers more traditionally found in computational linguistics it skips directly to a contextfree rather than finitestate base it permits a minimal extra degree of ordering flexibility and its probabilistic formulation admits an efficient maximumlikelihood bilingual parsing algorithm'],\n",
       " 338: ['in this paper we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques'],\n",
       " 339: ['considering empirical evidence from a freewordorder language german we propose a revision of the principles guiding the ordering of discourse entities in the forwardlooking center list within the centering model'],\n",
       " 342: [],\n",
       " 345: ['in this paper we present a novel and realistic method for speeding up the training time of a transformationbased learner without sacrificing performance'],\n",
       " 347: ['this paper presents a corpusbased approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby'],\n",
       " 348: ['we present a simple architecture for parsing transcribed speech in which an editedword detector first removes such words from the sentence string and then a standard statistical parser trained on transcribed speech parses the remaining words',\n",
       "  'the edit detector achieves a misclassification rate on edited words of which marks everything as not edited has an error rate of',\n",
       "  'to evaluate our parsing results we introduce a new evaluation metric the purpose of which is to make evaluation of a parse tree relatively indifferent the exact tree position of by this metric the parser achieves precision and recall'],\n",
       " 350: ['this report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at point in a sentence the surprisal of word its prefix on a phrasestructural language model'],\n",
       " 351: ['the algorithm takes as input a small corpus sentences annotated with parse trees a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text',\n",
       "  'the algorithm iteratively labels the entire data set with parse trees'],\n",
       " 352: ['we propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input'],\n",
       " 355: ['our apapplies alignment sentences gathered from unannotated comparable corpora it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences'],\n",
       " 356: ['we present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical leftcorner parser',\n",
       "  'the resulting statistical parser achieves performance fmeasure on the penn treebank which is only below the best current parser for this task despite using a smaller vocabulary size and less prior linguistic knowledge'],\n",
       " 357: ['on averagelength penn treebank sentences our most detailed estimate reduces the total number of edges processed to less than of that required by exhaustive parsing and a simpler estimate which requires less than a minute of precomputation reduces the work to less than'],\n",
       " 358: ['we propose a new phrasebased translation model and decoding algorithm that enables us to evaluate and compare several previously proposed phrasebased translation models',\n",
       "  'our empirical results which hold for all examined language pairs suggest that the highest levels of performance can be obtained through relatively simple means heuristic learning of phrase translations from wordbased alignments and lexical weighting of phrase translations'],\n",
       " 359: ['the results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations based on various statistical metrics while direct application of the bleu evaluation procedure does not always give good results'],\n",
       " 362: ['our fsas can also predict the correctness of alternative semantic renderings which may be used to evaluate the quality of translations'],\n",
       " 363: ['we present an application of ambiguity packing and stochastic disambiguation techniques for lexicalfunctional grammars lfg to the domain of sentence condensation',\n",
       "  'overall summarization quality of the proposed system is stateoftheart with guaranteed grammaticality of the system output due to the use of a constraintbased parsergenerator'],\n",
       " 364: ['we show here how to train a conditional random field to achieve performance as good as any reported base nounphrase chunking method on the conll task and better than any reported single model',\n",
       "  'we present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximumentropy models'],\n",
       " 365: ['a set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches nearhuman levels of performance'],\n",
       " 366: ['using these ideas together the resulting tagger gives a accuracy on the penn treebank wsj an error reduction of on the best previous single automatically learned tagging result'],\n",
       " 369: ['the proposed framework is evaluated with several experiments run in arabic chinese and english texts a system based on the approach described here and submitted to the latest automatic content extraction ace evaluation achieved toptier results in all three evaluation languages'],\n",
       " 370: ['this paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems',\n",
       "  'we measured the accuracy of both systems against a gold standard of the parc dependency bank and also measured their processing times'],\n",
       " 371: ['the theory of tree transducer automata provides a possible framework to draw on as it has been worked out in an extensive literature',\n",
       "  'we motivate the use of tree transducers for natural language and address the training problem for probabilistic treetotree and treetostring transducers'],\n",
       " 373: ['previous work demonstrated that web counts can be used to approximate bigram frequencies and thus should be useful for a wide variety of nlp tasks',\n",
       "  'the present paper investigates if these results generalize to tasks covering both syntax and semantics both generation and analysis and a larger of for the majority of tasks we find that simple unsupervised models perform when frequencies are obtained from the web rather than from a large corpus'],\n",
       " 374: ['it incorporates the idea that no single best model summary for a collection of documents exists'],\n",
       " 375: ['feature values were combined in a loglinear model to select the highest scoring candidate from an list'],\n",
       " 376: ['we describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings wordtoword alignments from an mt system and syntactic structure from parsetrees of source and target language sentences'],\n",
       " 377: ['we provide experimental results on the nist chineseenglish large data track evaluation'],\n",
       " 378: [],\n",
       " 379: ['we show perfor mance improvements through a number of newfeatures and measure their ability to general ize to a new test set drawn from the aquaint corpus'],\n",
       " 380: [],\n",
       " 381: ['we use our theory to introduce a linear algorithm that can be used to derive from wordaligned parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data'],\n",
       " 382: ['systems that automatically discover semantic classes have emerged in part to address the limitations of broadcoverage lexical resources such as wordnet and cyc',\n",
       "  'the current state of the art discovers many semantic classes but fails to label their concepts',\n",
       "  'we propose an algorithm labeling semantic and for leveraging them to extract relationships using a topdown approach'],\n",
       " 383: ['the basic theory of crfs is becoming wellunderstood but bestpractices for applying them to realworld data requires additional exploration'],\n",
       " 385: ['it provides six measures of similarity and three measures of relatedness all of which are based on the lexical database wordnet'],\n",
       " 389: ['for a training corpus with sentence pairs we increase the coverage of unique test set unigrams from to with more than half of the newly covered items accurately translated as opposed to none in current approaches'],\n",
       " 390: ['current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text using a locally decomposable matching score',\n",
       "  'we report results on data from the pascal rte challenge which surpass previously reported results for alignmentbased systems'],\n",
       " 391: ['most current approaches employ machine learning techniques and require supervised data'],\n",
       " 392: [],\n",
       " 393: ['we show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker improved model achieves an of an absolute improvement error reduction over the previous best result for wall street journal parsing',\n",
       "  'finally we provide some analysis to better understand the phenomenon'],\n",
       " 394: ['in this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources',\n",
       "  'we show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns'],\n",
       " 395: ['we devise a lineartime algorithm for factoring syntactic reorderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a stateoftheart syntaxbased machine translation system'],\n",
       " 397: ['for example we can achieve an english partofspeech tagging accuracy of using only three examples of each tag and no dictionary constraints'],\n",
       " 398: ['we present a novel statistical approach to parsing for constructing a complete formal meaning representation of a sentence',\n",
       "  'a semantic parser is learned given a set of sentences annotated with their correct meaning represen the main innovation of is its use of stateoftheart statistical machine translation techniques',\n",
       "  'a word alignment model is used for lexical acquisition and the parsing model itself can be seen as a syntaxbased translation model show that favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision and shows better robustness to variations in task complexity and word order'],\n",
       " 399: ['we also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation'],\n",
       " 400: ['in this paper we study the effect of different wordlevel preprocessing decisions for arabic on smt quality',\n",
       "  'moreover choosing the appropriate preprocessing produces a significant increase in bleu score if there is a change in genre between training and test data'],\n",
       " 403: ['traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases',\n",
       "  'we outline a set of approximations that make this approach practical and apply our method to the ace coreference dataset achieving a error reduction over a comparable method that only considers features of pairs of noun phrases'],\n",
       " 404: ['we illus trate these methods by estimating a sparse grammar describing the morphology ofthe bantu language sesotho demonstrat ing that with suitable priors bayesian techniques can infer linguistic structure in situations where maximum likelihoodmethods such as the insideoutside algo rithm only produce a trivial grammar'],\n",
       " 405: ['we present a sentence compression system based on synchronous contextfree grammars scfg following the successful noisychannel approach of knight and marcu'],\n",
       " 406: ['this paper describes three different approaches to mt system combination'],\n",
       " 407: ['in this paper we propose an integer linear programming ilp formulation for coreference resolution which models anaphoricity and coreference as a joint task such that each local model informs the other for the final assignments joint ilp formulation provides score improvements of over a base coreference classifier on the ace datasets'],\n",
       " 408: [],\n",
       " 409: ['we present a novel technique of training with manytomany alignments'],\n",
       " 410: [],\n",
       " 411: ['however existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering',\n",
       "  'this papresents a collection of methods for automatically learning admissible argument values to which an inference rule be applied which we call and methods for filtering out incorrect inferences'],\n",
       " 413: ['each of our methods independently provide the best results in their class on the rg and wordsim datasets and a supervised combination of them yields the best published results on all datasets'],\n",
       " 414: ['this family extends the partitioned logistic normal distribution enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar providing a new way to encode prior knowledge about an unknown grammar',\n",
       "  'we then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a nonparallel multilingual corpus'],\n",
       " 415: ['unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts',\n",
       "  'in this paper we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing'],\n",
       " 416: ['we use the margin infused relaxed algorithm of crammer et al to add a large number of new features to two machine translation systems the hiero hierarchical phrasebased translation system and our syntaxbased translation system',\n",
       "  'we analyze the impact of the new features and the performance of the learning algorithm'],\n",
       " 417: ['we introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems',\n",
       "  'for a set of five subjectobjectverb sov order languages we show significant improvements in bleu scores when translating from english compared to other reordering approaches in stateoftheart phrasebased smt systems'],\n",
       " 418: ['this paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task',\n",
       "  'with appropriate adaptor grammars and inference procedures we achieve an word token fscore on the standard brent version of the bernstein ratner corpus which is an error reduction of over over the best previously reported results for this corpus'],\n",
       " 419: ['this easily results in inconsistent annotations which are harmful to the performance of the aggregate system'],\n",
       " 420: [],\n",
       " 421: ['in this work we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding'],\n",
       " 422: ['we present results on a novel hybrid semantic smt model that incorporates the strengths of both semantic role labeling and phrasebased statistical machine translation',\n",
       "  'the first pass is performed using a conventional phrasebased smt model',\n",
       "  'evaluation on a wall street journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in bleu score over a strong pure phrasebased smt baseline to our knowledge the first successful application of semantic role labeling to smt'],\n",
       " 423: ['current vectorspace models of lexical semantics create a single prototype vector to represent the meaning of a word',\n",
       "  'experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vectorspace models'],\n",
       " 424: ['since the metaclassifier requires errorannotated data for training we investigate how much training data is needed to improve results over the baseline of not using a metaclassifier'],\n",
       " 425: ['we propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain'],\n",
       " 427: ['we present a generative modelbased approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner',\n",
       "  'by sharing lexical statistics at the level of abstract entity types our model is able to substantially reduce semantic compatibility errors resulting in the best results to date on the complete endtoend coreference task'],\n",
       " 428: ['we advance the state of the art in parallel sentence extraction by modeling the document level alignment motivated by the observation that parallel sentence pairs are often found in close proximity',\n",
       "  'we also include features which make use of the additional annotation given by wikipedia and features using an automatically induced lexicon model'],\n",
       " 430: [],\n",
       " 431: ['we analyze a number of these algorithms in terms of their sentencelevel loss functions which motivates several new approaches including a structured svm',\n",
       "  'among other results we find that a simple and efficient batch version of mira performs at least as well as training online and consistently outperforms other options'],\n",
       " 432: ['second and more interestingly we provide an algorithm for inducing crosslingual clusters and we show that features derived from these clusters significantly improve the accuracy of crosslingual structure prediction',\n",
       "  'specifically we show that by augmenting directtransfer systems with crosslingual cluster features the relative error of delexicalized dependency parsers trained on english treebanks and transferred to foreign languages can be reduced by up to'],\n",
       " 433: ['the core of method which we call is an algorithm for efficiently computing the sequence of phraselevel edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation',\n",
       "  'this optimal edit seis subsequently scored using mea we test our on the helping our own hoo shared task data and show that our method results in more accurate evaluation for grammatical error correction'],\n",
       " 434: ['additionally we contribute the first pos annotation guidelines for such text and release a new dataset of english language tweets annotated using these guidelines',\n",
       "  'we systematically evaluate the use of largescale unsupervised word clustering and new lexical features to improve tagging accuracy'],\n",
       " 435: ['we find that these representations are surprisingly good at capturing syntactic and semantic regularities in language and that each relationship is characterized by a relationspecific vector offset',\n",
       "  'for example the malefemale relationship is automatically learned and with the induced vector representations king man woman results in a vector very close to queen we demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions provided with this paper and are able to correctly answer almost of the questions'],\n",
       " 436: ['we introduce an annotation scheme for temporal expressions and describe a method for resolving temporal expressions in print and broadcast news',\n",
       "  'the system which is based on both handcrafted and machinelearnt rules achieves an accuracy fmeasure against handannotated data'],\n",
       " 437: ['results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than handcrafted rule writing at a comparable level of human labor investment'],\n",
       " 439: ['the noisy channel model has been applied to a wide range of problems including spelling correction',\n",
       "  'these models consist of two components a source model and a channel model'],\n",
       " 440: ['this paper presents results on experiments using this approach in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus'],\n",
       " 442: ['we discuss the advantages of lexicalized treeadjoining grammar as an alternative to lexicalized pcfg for statistical parsing describing the induction of a probabilistic ltag model from the penn treebank and evaluating its parsing performance',\n",
       "  'we find that this induction method is an improvement over the embased method of hwa and that the induced model yields results comparable to lexicalized pcfg'],\n",
       " 443: [],\n",
       " 445: ['in this paper we evaluate the performance of different learning methods on a prototypical natural language disambiguation task confusion set disambiguation when trained on orders of magnitude more labeled data than has previously been used'],\n",
       " 446: ['we present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple english translations of the same source text'],\n",
       " 448: ['we develop a framework for formalizing semantic construction within grammars expressed in typed feature struclogics including the approach provides an alternative to the lambda calculus it maintains much of the desirable flexibility of unificationbased approaches to composition while constraining the allowable operations in order to capture basic generalizations and improve maintainability'],\n",
       " 449: ['in our approach we compare the entire list of candidates sorted according to the particular measures to a reference set of manually identified true positives'],\n",
       " 450: ['in this paper we compare the speed and output quality of a traditional stackbased decoding algorithm with two new decoders a fast greedy decoder and a slow but optimal decoder that treats decoding as an integerprogramming optimization problem'],\n",
       " 451: ['we propose a statistical method that finds the maximumprobability segmentation of a given text'],\n",
       " 452: ['our model transforms a sourcelanguage parse tree into a targetlanguage string by applying stochastic operations at each node'],\n",
       " 454: [],\n",
       " 455: ['we present a noun phrase coreference system that extends the work of soon et al and to our knowledge produces the best results to date on the muc and muc coreference resolution data sets fmeasures of and respectively',\n",
       "  'improvements arise from two sources extralinguistic changes to the learning framework and a largescale expansion of the feature set to include more sophisticated linguistic knowledge'],\n",
       " 456: ['we present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts',\n",
       "  'we compare distributionally induced and actual partofspeech tags as input data and examine extensions to the basic model'],\n",
       " 457: ['this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their coindexed antecedents in phrase structure trees that do not contain this information',\n",
       "  'this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus'],\n",
       " 458: ['this paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction',\n",
       "  'by modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction'],\n",
       " 460: [],\n",
       " 464: ['we present a framework for statistical machine translation of natural languages based on direct maximum entropy models which contains the widely used sourcechannel approach as a special case',\n",
       "  'this approach allows a baseline machine translation system to be extended easily by adding new feature functions'],\n",
       " 465: ['in contrast to a conventional wordtoword statistical model a decoder for the syntaxbased model builds up an english parse tree given a sentence in a foreign language',\n",
       "  'as the model size becomes huge in a practical setting and the decoder considers multiple syntactic structures for each word alignment several pruning techniques are necessary'],\n",
       " 466: ['we present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent'],\n",
       " 467: ['a set of dependency structures used for training and testing the parser is obtained from a treebank of ccg normalform derivations which have been derived semi automatically from the penn treebank'],\n",
       " 470: ['we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as even when the relations are not explicitly marked by cue phrases'],\n",
       " 472: ['we also compare our results with the results obtained from human translations and a commercial system for the same task'],\n",
       " 473: ['this paper presents a simple unsupervised learning algorithm for classifying reviews up or recdown'],\n",
       " 474: ['it shows that the performance is significantly better than reported by any other machinelearning system'],\n",
       " 475: [],\n",
       " 476: ['we present an alternative strategy in which patterns are used to extract highly precise relational information offline creating a data repository that is used to efficiently answer questions'],\n",
       " 477: ['in this paper we present a novel customizable ie paradigm that takes advantage of predicateargument structures',\n",
       "  'we also introduce a new way of automatically identifying predicate argument structures which is central to our ie paradigm',\n",
       "  'it is based on an extended set of features and inductive decision tree learning'],\n",
       " 478: ['we introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an endtoend qa system',\n",
       "  'we also show that the model we propose is flexible enough to accommodate within one mathematical framework many qaspecific resources and techniques which range from the exploitation of wordnet structured and semistructured databases to reasoning and paraphrasing'],\n",
       " 479: ['in nlp although feature combinations are crucial to improving performance they are heuristically selected',\n",
       "  'in this paper we extend mining to convert a kernelbased classifier into a simple and fast linear classifier'],\n",
       " 480: ['a novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters offering us a good insight into the potential and limitations of semantically classifying'],\n",
       " 481: ['to remove these we propose two measures scores that evaluate the validity of alignments',\n",
       "  'the measure for article alignment uses similarities in sentences aligned by dp matching and that for sentence alignment uses similarities in articles aligned by clir',\n",
       "  'using these measures we have successfully constructed a largescale article and sentence alignment corpus available to the public'],\n",
       " 482: ['we augment a model of translation based on reordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure while keeping computational complexity polynomial in the sentence length'],\n",
       " 483: ['we present a statistical model for computing the probability of an alignment given a sentence pair',\n",
       "  'this model allows easy integration of contextspecific features',\n",
       "  'our experiments show that this model can be an effective tool for improving an existing word alignment'],\n",
       " 484: ['we present a probabilistic parsing model for german trained on the negra treebank',\n",
       "  'this model outperforms the baseline achieving a labeled precision and recall of up to'],\n",
       " 485: ['the experiments will show that the baseline itg constraints are not sufficient on the canadian hansards task'],\n",
       " 486: ['a general problem of this approach is that there is only a loose relation to the final translation quality on unseen text',\n",
       "  'we show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure'],\n",
       " 487: ['we apply a decision tree based approach to pronoun resolution in spoken dialogue',\n",
       "  'we present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features'],\n",
       " 488: ['in this paper we propose a competition learning approach to coreference resolution',\n",
       "  'furthermore our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution'],\n",
       " 489: ['several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction',\n",
       "  'each approach is based on a particular model for the patterns to be acquired such as a predicateargument structure or a dependency chain'],\n",
       " 490: ['this paper presents a chinese word segmentation system that uses improved sourcechannel models of chinese sentence generation',\n",
       "  'our system provides a unified approach to the four fundamental features of wordlevel chinese language processing word segmentation morphological analysis factoid detection and named entity recognition',\n",
       "  'the performance of the system is evaluated on a manually annotated test set and is also compared with several stateoftheart systems taking into account the fact that the definition of chinese words often varies from system to system'],\n",
       " 491: ['one common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns with gradually degrading precision'],\n",
       " 494: ['starred examples are correct in corpus alternates are parse errors string speculator richard denthese structures are typically bracketed flat in the etb underspecifying the semantic relations relative to the ctb',\n",
       "  'in ctb parsing this type of ambiguity is difficult to resolve different compound np parses differ in dependency structure so the dependency model resolves errors when word frequencies are large enough to be reliable but this is often not possible'],\n",
       " 495: ['our investigation reveals that this method of acquiring sensetagged data is promising'],\n",
       " 496: [],\n",
       " 497: ['this segmentation algorithm uses automatically induced decision rules to combine the different features',\n",
       "  'the embedded textbased algorithm builds on lexical cohesion and has performance comparable to stateoftheart algorithms based on lexical information'],\n",
       " 498: ['this paper describes a method of detecting grammatical and lexical errors made by japanese learners of english and other techniques that improve the accuracy of error detection with a limited amount of training data',\n",
       "  'in this paper we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus which contains information on learners errors'],\n",
       " 500: ['this paper describes a noisy channel model of speech repairs which can identify and correct repairs in speech transcripts'],\n",
       " 501: ['we show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model',\n",
       "  'we present three methods for training a neural network to estimate the probabilities for a statistical parser one generative one discriminative and one where the probability model is generative but the training criteria is discriminative'],\n",
       " 502: ['this paper describes and evaluates loglinear parsing models for combinatory categorial a parallel implementation of algorithm is described which runs on a beowulf cluster allowing the complete penn treebank to be used for estimation',\n",
       "  'we also develop a new efficient parsing for maximises expected recall of dependencies'],\n",
       " 503: ['the perceptron approach was implemented with the same feature set as that of an existing generative model roark a and experimental results show that it gives competitive performance to the generative model on parsing the penn treebank'],\n",
       " 504: ['this paper proposes a new approach for resolution which uses the tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the bell tree to the leaf nodes'],\n",
       " 505: ['this paper presents a new framework that allows direct orthographical mapping dom between two different languages through a joint sourcechannel model also transliteration model tm the tm model we automate the orthographic alignment process to derive the aligned transliteration units from bilingual dictionary'],\n",
       " 506: ['to determine this powe propose a novel machinelearning method that applies textcategorization techniques to just the subjective portions of the document',\n",
       "  'extracting these portions can be implemented using efficient for finding cuts in this greatly facilitates incorporation of crosssentence contextual constraints'],\n",
       " 507: ['we present work on the use of a thesaurus acquired from raw textual corpora and the wordnet similarity package to find predominant noun senses automatically'],\n",
       " 508: [],\n",
       " 509: ['in this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments'],\n",
       " 510: ['the key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities',\n",
       "  'our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision but also that appropriate labels could be automatically provided for the relations'],\n",
       " 511: ['we examine the utility of different features such as wordnet hypernyms parts of speech and entity types and find that the dependency tree kernel achieves a f improvement over a bagofwords kernel'],\n",
       " 512: ['experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach'],\n",
       " 513: ['present a generative model for the learning of dependency structures',\n",
       "  'we also describe the multiplicative combination of this dependency model with a model of linear constituency',\n",
       "  'we also demonstrate that the combined model works and is robust crosslinguistically being able to exploit either attachment or distributional regularities that are salient in the data'],\n",
       " 514: ['we demonstrate reduction in alignment error rate of approximately resulting from giving extra weight to the probability of alignment to the null word smoothing probability estimates for rare words and using a simple heuristic estimation method to initialize or replace em training of model parameters'],\n",
       " 515: ['more comprehensively we incorporate all the criteria using two selection strategies both of which result in less labeling cost than singlecriterionbased method',\n",
       "  'the results of the named entity recognition in both muc and genia show that the labeling cost can be reduced by at least without degrading the performance'],\n",
       " 516: [],\n",
       " 517: ['in an ordinary syntactic parser the input is a string and the grammar ranges over strings'],\n",
       " 518: ['our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical durational and structural features that look both forward and backward in the discourse',\n",
       "  'we then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance'],\n",
       " 519: ['we employ maximum entropy models to combine diverse lexical syntactic and semantic features derived from the text',\n",
       "  'here we present our general approach and describe our ace results'],\n",
       " 520: ['the method produces performance higher than the previous best results on conll syntactic chunking and conll named entity chunking english and german'],\n",
       " 523: ['we present an effective training algorithm for linearlyscored dependency parsers that implements online largemargin multiclass training crammer and singer crammer et al on top of efficient parsing techniques for dependency trees eisner'],\n",
       " 524: ['experiments using data from the prague dependency treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy',\n",
       "  'this leads to the best reported performance for robust nonprojective parsing of czech'],\n",
       " 525: ['then we apply a based on a labeling formulation of the problem that alters a givenary classifiers output in an explicit attempt to ensure that similar items receive similar labels'],\n",
       " 526: ['we propose a method for extracting semantic orientations of words desirable or undesirable',\n",
       "  'regarding semantic orientations as spins of electrons we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function',\n",
       "  'given only a small number of seed words the proposed method extracts semantic orientations with high accuracy in the experiments on english lexicon'],\n",
       " 527: ['we view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function'],\n",
       " 528: ['we propose a set of partitionbased features to learn a ranking model for distinguishing good and bad partitions'],\n",
       " 529: ['this paper describes a simple yet novel method for constructing sets of best parses based on a coarsetofine generative parser charniak'],\n",
       " 530: ['the model is formally a synchronous contextfree grammar but is learned from a bitext without any syntactic information',\n",
       "  'thus it can be seen as shift to the of syntaxtranslation systems without any lin in our experiments using bleu as a metric the hierarchical phrasebased model achieves a relative improvement of over pharaoh a stateoftheart phrasebased system'],\n",
       " 532: ['summarization step sentence knight and marcu knight and marcu kampm present a noisychannel model for sentence compression',\n",
       "  'the main difficulty in using this method is the lack of data knight and marcu use a corpus of training sentences',\n",
       "  'more data is not easily available so in addition to improving the original kampm noisychannel model we create unsupervised and semisupervised models of the task'],\n",
       " 533: ['applied to a sequence labeling problempos tagging given a tagging dictionary and unlabeled textcontrastive estimation outperforms em with the same feature set is more robust to degradations of the dictionary and can largely recover by modeling additional features'],\n",
       " 534: ['most current statistical natural language processing models use only local features so as to permit dynamic programming in inference but this makes them unable to fully account for the long distance structure that is prevalent in language use'],\n",
       " 535: ['evaluation shows this algorithm performs well when compared with a previously reported documentcentric approach'],\n",
       " 537: ['we also demonstrate how semantic information such as wordnet and name list can be used in featurebased relation extraction to further improve the performance',\n",
       "  'evaluation on the ace corpus shows that effective incorporation of diverse features enables our system outperform previously bestreported systems on the ace relation subtypes and significantly outperforms tree kernelbased systems by over in fmeasure on the ace relation types'],\n",
       " 538: ['we present a framework for word alignment based on loglinear models',\n",
       "  'in this paper we use ibm model alignment probabilities pos correspondence and bilingual dictionary coverage as features'],\n",
       " 539: ['we present a version of inversion transduction grammar where rule probabilities are lexicalized throughout the synchronous parse tree along with pruning techniques for efficient training'],\n",
       " 540: ['in this paper we also use support vector machines to combine features from traditional reading level measures statistical language models and other language processing tools to produce a better method of assessing reading level'],\n",
       " 541: ['the first step of the method is to parse the source language string that is being translated',\n",
       "  'the second step is to apply a series of transformations to the parse tree effectively reordering the surface string on the source language side of the translation system',\n",
       "  'the goal of this step is to recover an underlying word order that is closer to the target language wordorder than the original string'],\n",
       " 542: ['in this paper we present a syntaxbased statistical matranslation system based on a probabilistic synchronous dependency insertion grammar',\n",
       "  'we evaluate the outputs of our mt system using the nist and bleu automatic mt evaluation software',\n",
       "  'the result shows that our system outperforms the baseline system based on the ibm models in both translation speed and quality'],\n",
       " 543: ['we present an approach to using a morphological analyzer for tokenizing and morphologically tagging including partofspeech tagging arabic words in one process',\n",
       "  'we learn classifiers for individual morphological features as well as ways of using these classifiers to choose among entries from the output of the analyzer'],\n",
       " 544: ['in this paper we present a stateoftheart baseline semantic role labeling system based on support vector machine classifiers',\n",
       "  'we show improvements on this system by i adding new features including features extracted from dependency parses ii performing feature selection and calibration and iii combining parses obtained from semantic parsers trained using different syntactic views',\n",
       "  'in order to address this problem we combined semantic parses from a minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on charniak parses'],\n",
       " 545: ['this stands in stark contrast to the linguistic observation that a core argument frame is with strong dependencies between arguments'],\n",
       " 546: ['using alignment techniques from phrasebased statistical machine translation we show how paraphrases in one language can be identified using a phrase in another language as a pivot',\n",
       "  'we define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities and show how it can be refined to take contextual information into account'],\n",
       " 547: ['in this paper we explore the power of randomized algorithm to address the challenge of working with very large amounts of data',\n",
       "  'we reduce the running time from quadratic to practically linear in the number of elements to be computed'],\n",
       " 548: ['traditional machine learning techniques have been applied to this problem with reasonable success but they have been shown to work well only when there is a good match between the training and test data with respect to topic',\n",
       "  'this paper demonstrates that match with respect to domain and time is also important and presents preliminary experiments with training data labeled with emoticons which has the potential of being independent of domain topic and time'],\n",
       " 549: [],\n",
       " 550: ['we consider the task of unsupervised lecture segmentation',\n",
       "  'our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors'],\n",
       " 551: ['through a simple bootstrapping procedure we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities'],\n",
       " 552: ['the crf is conditioned on both the source and target texts and thus allows for the use of arbitrary and overlapping features over these data'],\n",
       " 553: [],\n",
       " 554: ['by analyzing potentially similar sentence pairs using a signal processinginspired approach we detect which segments of the source sentence are translated into segments in the target sentence and which are not'],\n",
       " 555: ['in this paper we present a method for reducing the granularity of the wordnet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies namely the oxford dictionary of english'],\n",
       " 556: ['this paper we present a weaklysupervised generalpurpose and accurate algorithm for harvesting semantic relations',\n",
       "  'the main contributions are i a method for exploiting generic patterns by filtering incorrect instances using the web and ii a principled measure of pattern and instance reliability enabling the filtering algorithm'],\n",
       " 557: ['this paper presents a pilot study of the use of phrasal statistical machine translation smt techniques to identify and correct writing errors made by learners of english as a second language esl',\n",
       "  'using examples of mass noun errors in the learner error cor clec guide creation of an engineered training set we show that application of the smt paradigm can capture errors not well addressed by widelyused proofing tools designed for native speakers',\n",
       "  'our system was able to correct of mistakes in a set of naturallyoccurring examples of mass noun errors found on the world wide web suggesting that efforts to collect alignable corpora of preand postediting esl writing samples offer can enable the development of smtbased writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of esl learners'],\n",
       " 559: ['in particular we show that the reranking parser described in charniak and johnson improves performance of the parser on brown to'],\n",
       " 560: ['we present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank',\n",
       "  'in contrast with previous work we are able to split various terminals to different degrees as appropriate to the actual complexity in the data'],\n",
       " 561: ['the model provides contentdependent hierarchical phrasal reordering with generalization based on features automatically learned from a realworld bitext'],\n",
       " 562: ['we propose a new distortion model that can be used with existing phrasebased smt decoders to address those ngram language model limitations',\n",
       "  'we also propose a novel metric to measure word order similarity or difference between any pair of languages based on word alignments'],\n",
       " 563: ['we relate this approach to contrastive estimation smith and eisner a apply the latter to grammar induction in six languages and show that our new approach improves accuracy by absolute over ce and over em achieving to our knowledge the best results on this to date'],\n",
       " 564: ['the model is linguistically syntaxbased because tats are extracted automatically from wordaligned source side parsed parallel texts'],\n",
       " 565: ['we present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules which are quite restricted in hebrew helps in the disambiguation'],\n",
       " 566: ['we propose two new bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively',\n",
       "  'the bigram model greatly outperforms the unigram model and previous probabilistic models demonstrating the importance of such dependencies for word segmentation'],\n",
       " 568: ['to address data sparseness we used temporal reasoning as an oversampling method to dramatically expand the amount of training data resulting in predictive accuracy on link labeling as high as using a maximum entropy classifier on human annotated data',\n",
       "  'this method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions'],\n",
       " 569: ['we introduce a semisupervised approach to training for statistical machine translation that alternates the traditional expectation maximization step that is applied on a large training corpus with a discriminative step aimed at increasing wordalignment quality on a small manually wordaligned subcorpus',\n",
       "  'we show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality'],\n",
       " 570: ['we apply our algorithm on the problem of sensedisambiguated noun hyponym acquisition where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantaxonomy wordnet'],\n",
       " 571: ['the algorithm makes use of a new frequency based metric for time distributions and a resource free discriminative approach to transliteration'],\n",
       " 572: ['our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering and can also easily scale to include more features',\n",
       "  'evaluation on the ace corpus shows that our method outperforms the previous bestreported methods and significantly outperforms previous two dependency tree kernels for relation extraction'],\n",
       " 573: ['unsupervised dop models assign all possible binary trees to a set of sentences and next use a large random subset of all subtrees from these binary trees to compute the most probable parse trees',\n",
       "  'to the best of our knowledge this is the first paper which tests a maximum likelihood estimator for dop on the wall street journal leading the surprising result that unsupervised parsing model beats a widely used supervised model a treebank pcfg'],\n",
       " 574: ['work on the semantics of questions has argued that the relation between a question and its answers can be cast in terms of logical entailment',\n",
       "  'in this paper we demonstrate how computational systems to recognize entailment can be used to enhance the accuracy of current opendomain automatic question answering qa systems'],\n",
       " 575: [],\n",
       " 576: ['syn tactic approaches seek to remedy these problemsin this paper we take the framework for acquiring multilevel syntactic translation rules of gal ley et al from aligned treestring pairs and present two main extensions of their approach first instead of merely computing a single derivation that minimally explains a sentence pair we constructa large number of derivations that include contextually richer rules and account for multiple interpretations of unaligned words',\n",
       "  'we contrast differentapproaches on real examples show that our esti mates based on multiple derivations favor phrasal reorderings that are linguistically better motivated and establish that our larger rules provide a bleu point increase over minimal rules'],\n",
       " 577: ['the paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations even in relatively simple real bitexts in syntactically similar languages with rigid word order'],\n",
       " 579: ['this paper explores their interaction and brings empirical evidence in support of the hypotheses that subjectivity is a property that can be associated with word senses and word sense disambiguation can directly benefit from subjectivity annotations'],\n",
       " 581: ['this makes it more practical to use statistical parsers in applications that need access to aspects of predicateargument structure'],\n",
       " 582: ['we use a publicly available structured output svm to create a maxmargin syntactic aligner with a soft cohesion constraint',\n",
       "  'the resulting aligner is the first to our knowledge to use a discriminative learning method to train an itg bitext parser'],\n",
       " 583: ['in this paper we review and compare the different constraints theoretically and provide an experimental evaluation using data from two treebanks investigating how large a proportion of the structures found in the treebanks are permitted under different constraints',\n",
       "  'the results indicate that a combination of the wellnestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data'],\n",
       " 584: ['we propose a new paradigm of information extraction which operates on demand in response to a users query',\n",
       "  'given a users query the system will automatically create patterns to extract salient relations in the text of the topic and build tables from the extracted information using paraphrase discovery technology'],\n",
       " 586: ['unlike in current stateoftheart approaches the kind and number of different tags is generated by the method itself',\n",
       "  'the approach is evaluated on three different languages by measuring agreement with existing taggers'],\n",
       " 587: ['we describe the new release of the rasp robust accurate statistical parsing system designed for syntactic annotation of free text'],\n",
       " 588: ['we propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure while retaining the robustness and efficiency of the hmm alignment model'],\n",
       " 590: ['recent research presents conflicting evidence on whether word sense disambiguation wsd systems can help to improve the performance of statistical machine translation mt systems',\n",
       "  'we show for the first time that integrating a wsd system improves the performance of a stateoftheart statistical mt system on an actual translation task',\n",
       "  'furthermore the improvement is statistically significant'],\n",
       " 591: ['in this paper we first show that an active learning approach can be successfully used to perform domain adaptation of wsd systems'],\n",
       " 592: ['in both cases our methods achieve significant speed improvements often by more than a factor of ten over the conventional beamsearch method at the same levels of search error and translation accuracy'],\n",
       " 593: [],\n",
       " 594: ['we evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision'],\n",
       " 596: ['evaluating a parser on the same resource used to create it can lead to noncomparable accuracy scores and an overoptimistic view of parser performance this paper we evaluate a on depbank and demonstrate the difficulties in converting the parser output into dep bank grammatical relations',\n",
       "  'in addition we present a method for measuring the effectiveness of the conversion which provides an upper bound on parsing accuracy'],\n",
       " 597: ['in this paper we study the domain adaptation problem from the instance weighting perspective',\n",
       "  'we formally analyze and characterize the domain adaptation problem from a distributional view and show that there are two distinct needs for adaptation corresponding to the different distributions of instances and classification functions in the source and the target domains'],\n",
       " 598: ['in this paper we suggest a method for incorporating domain knowledge in semisupervised learning algorithms',\n",
       "  'our novel framework unifies can exploit several kinds of specific the experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks'],\n",
       " 599: ['in addition to supertagging we also explore the utility of a surface global grammaticality measure based on combinatory operators'],\n",
       " 600: [],\n",
       " 601: [],\n",
       " 602: ['in this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity',\n",
       "  'the primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another'],\n",
       " 603: ['this measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains'],\n",
       " 604: [],\n",
       " 605: ['here we explore the use of bfs for language modelling in statistical machine translation show how a bf containing can enable us to use much larger corpora and higherorder models complementing a con lm within an smt system',\n",
       "  'our solutions in both cases retain the onesided error guarantees of the bf while taking advantage of the zipflike distribution of word frequencies to reduce the space requirements'],\n",
       " 606: ['given a few pairs of named entities known to exhibit or not exhibit a particular relation bags of sentences containing the pairs are extracted from the web',\n",
       "  'we extend an existing relation extraction method to handle this weaker form of supervision and present experimental results demonstrating that our approach can reliably extract relations from web documents'],\n",
       " 607: ['chl dozhangmicrosoftcom mhliinsunhiteducn muli mingzhoumicrosoftcom guanyiinsunhiteducn abstract inspired by previous preprocessing approaches to smt this paper proposes a novel probabilistic approach to reordering which combines the merits of syntax and phrasebased smt',\n",
       "  'experiments show that for the nist mt task of chineseto english translation the proposal leads to bleu improvement of'],\n",
       " 608: ['this paper presents a method which alleviates this problem by exploiting multiple translations of the same source central to our approach is triangulathe process of translating from a source to a target language via an intermediate third language'],\n",
       " 609: ['this difference ensures that the learned structure will have high probability over a range of possible parameters and permits the use of priors favoring the sparse distributions that are typical of natural language'],\n",
       " 610: ['in this paper we propose guided learning a new learning framework for bidirectional sequence classification',\n",
       "  'the tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single perceptron like learning algorithm',\n",
       "  'we apply this novel learning algorithm to pos tagging'],\n",
       " 611: ['we study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer reranking',\n",
       "  'our experiments suggest that syntactic information helps tasks such as questionanswer classification and that shallow semantics gives remarkable contribution when a reliable set of pass can be extracted eg from answers'],\n",
       " 612: ['closed tests on the first and show that our system is competitive with the best in the literature achieving the highest reported fscores for a number of corpora'],\n",
       " 613: ['we present an unsupervised nonparametric bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document',\n",
       "  'while most existing coreference work is driven by pairwise decisions our model is fully generative producing each mention from a combination of global entity properties and local attentional state'],\n",
       " 614: ['using statistical machine translation techniques a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms',\n",
       "  'the resulting parser is shown to be the bestperforming system so far in a database query domain'],\n",
       " 615: ['this paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in english',\n",
       "  'given a bridge between english and the selected target language eg a bilingual dictionary or a parallel corpus the methods can be used to rapidly create tools for subjectivity analysis in the new language'],\n",
       " 616: ['our contributions include a precise description of the task with annotation guidelines analysis and discussion a probabilistic weakly supervised learning model and experimental evaluation of the methods presented'],\n",
       " 619: ['the structured space of a synchronous grammar is a natural fit for phrase pair probability estimation though the search space can be prohibitively large'],\n",
       " 620: ['among syntaxbased translation models the which takes as input a parse tree of the source sentence is a promising direction being faster and simpler than its stringbased counterpart',\n",
       "  'we a that translates a packed forest of exponentially many parses which encodes many more alternatives standard lists'],\n",
       " 621: ['we present a translation model which models derivations as a latent variable in both training and decoding and is fully discriminative and globally optimised'],\n",
       " 623: ['we apply the hypothesis of one sense per discourse yarowsky to information extraction ie and extend the scope of discourse from one single document to a cluster of topicallyrelated documents',\n",
       "  'combining global evidence from related documents with local decisions we design a simple scheme to conduct crossdocument inference for improving the ace event ex without using any additional labeled data this new approach obtained higher fmeasure in trigger labeling and higher fmeasure in argument labeling over a stateoftheart ie system which extracts events independently for each sentence'],\n",
       " 624: ['we propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings a fundamental problem in aspectbased sentiment summarization hu and liu a'],\n",
       " 625: ['these words are in turn highly ambiguous breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance',\n",
       "  'using a treebank grammar a datadriven lexicon and a linguistically motivated unknowntokens handling technique our model outperforms previous pipelined integrated or factorized systems for hebrew morphological and syntactic processing yielding an error reduction of over the best published results so far'],\n",
       " 626: ['compared with previous models it not only captures nonsyntactic phrases and discontinuous phrases with linguistically structured features but also supports multilevel structure reordering of tree typology with larger span',\n",
       "  'experimental results on the nist mt chineseenglish translation task show that our method statistically significantly outperforms the baseline systems'],\n",
       " 627: ['in this paper we propose a novel stringtodependency algorithm for statistical machine translation',\n",
       "  'with this new framework we employ a target dependency language model during decoding to exploit long distance word relations which are unavailable with a traditional ngram language model',\n",
       "  'our experiments show that the stringtodependency decoder achieves point improvement in bleu and point improvement in ter compared to a standard hierarchical stringtostring system on the nist chineseenglish evaluation set'],\n",
       " 628: ['our final result an fscore of outperforms both best and best reranking baselines and is better than any previously reported systems trained on the treebank'],\n",
       " 629: ['for example in the case of english unlabeled secondorder parsing we improve from a baseline accuof in the case of czech unlabeled secondorder parsing we from a baseline accuracy of addition we demonstrate that our method also improves performance when small amounts of training data are available and can roughly halve the amount of supervised data required to reach a desired level of performance'],\n",
       " 630: ['we incorporate up to gwords one billion tokens of unlabeled data which is the largest amount of unlabeled data ever used for these tasks to investigate the performance improvement',\n",
       "  'in addition our results are superior to the best reported results for all of the above test collections'],\n",
       " 631: ['in particular we study the task of morphological segmentation of multiple languages',\n",
       "  'we present a nonparametric bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies crosslingual morpheme pator we apply our model to three semitic languages arabic hebrew aramaic as well as to english'],\n",
       " 632: ['we demonstrate that good results can be obtained using the robust emhmm learner when provided with good initial conditions even with incomplete dictionaries',\n",
       "  'we present a family of algorithms to compute effective estimations we test the method on the task of full morphological disambiguation in hebrew achieving an error reduction of over a strong uniform distribution baseline',\n",
       "  'we also test the same method on the standard wsj unsupervised pos tagging task and obtain results competitive with recent stateoftheart methods while using simple and efficient learning methods'],\n",
       " 633: ['we introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially classbased models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words ussuch large training corpora billion tokens',\n",
       "  'we show that combining them with wordmodels in the loglinear model of a stateoftheart statistical machine translation system leads to improvements in translation quality as indicated by the bleu score'],\n",
       " 634: ['translations are induced using a generative model based on canonical correlation analysis which explains the monolingual lexicons in terms of latent matchings',\n",
       "  'we show that highprecision lexicons can be learned in a variety of language pairs and from a range of corpus types'],\n",
       " 636: ['in this paper propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and fast decoding is achieved by using a novel multiplebeam search algorithm'],\n",
       " 637: ['with a characterbased perceptron as the core combined with realvalued features such as language models the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly'],\n",
       " 638: ['by letting one model generate features for the other we consistently improve accuracy for both models resulting in a significant improvement of the state of the art when evaluated on data sets from the conllx shared task'],\n",
       " 639: ['on sentences of length our system achieves an fscore of a relative reduction in error over a generative baseline'],\n",
       " 640: ['in adding syntax to statistical mt there is a tradeoff between taking advantage of linguistic analysis versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data',\n",
       "  'we present an approach that explores the tradeoff from the other direction starting with a contextfree translation model learned directly from aligned parallel text and then adding soft constituentlevel constraints based on parses of the source language'],\n",
       " 641: ['we show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous contextfree grammarbased models',\n",
       "  'our experiments evaluating the approach demonstrate substantial gains for chinese english and arabicenglish translation'],\n",
       " 642: ['together these two measures capture not only frequency of occurrence but also crosschecking that the candidate occurs both near the class name and near other class members'],\n",
       " 644: ['we train a coreference classifier over pairs of mentions and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments'],\n",
       " 645: [],\n",
       " 646: ['in this paper we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions',\n",
       "  'during training the learner repeatedly constructs action sequences for a set of documents executes those actions and observes the resulting reward'],\n",
       " 647: ['a central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state',\n",
       "  'to deal with the high degree of ambiguity present in this setting we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state'],\n",
       " 649: ['this paper presents an unsupervised opinanalysis method for clasie recognizing which stance a person is taking in an online debate',\n",
       "  'in order to handle the complexities of this genre we mine the web to learn associations that are indicative of opinion stances in debates'],\n",
       " 650: ['this paper focuses on the problem of crosslingual sentiment classification which leverages an available english corpus for chinese sentiment classification by using the english corpus as training data',\n",
       "  'experimental results show the effectiveness of the proposed approach which can outperform the standard inductive classifiers and the transductive classifiers'],\n",
       " 651: ['the model parameters are learned in a maxmargin framework by employing a linear programming relaxation',\n",
       "  'we evaluate the performance of our parser on data in several natural languages achieving improvements over existing stateoftheart methods'],\n",
       " 652: ['adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora'],\n",
       " 653: ['we consider generative and discriminative models for dependency grammar induction that use wordlevel alignments and a source language parser english to constrain the space of possible target trees'],\n",
       " 654: ['we describe a novel method for the task of unsupervised pos tagging with a dictionary one that uses integer programming to explicitly search for the smallest model that explains the data and then uses em to set parameter values'],\n",
       " 655: ['in this paper we present a discriminative wordcharacter hybrid model for joint chinese word segmentation and pos tagging',\n",
       "  'we describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus',\n",
       "  'we describe an efficient framework for training our model based on the margin infused relaxed algorithm mira evaluate our approach on the penn chinese treebank and show that it achieves superior performance compared to the stateoftheart approaches reported in the literature'],\n",
       " 656: ['we describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury unlike most previous work in event structure or semantic role learning our system does not use supervised techniques handbuilt knowledge or predefined classes of events or roles',\n",
       "  'our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles'],\n",
       " 657: ['first we examine three subproblems that play a role in coreference resolution named entity recognition anaphoricity determination and coreference element detection',\n",
       "  'second we measure the performance of a stateoftheart coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets'],\n",
       " 658: ['we work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses'],\n",
       " 659: ['we present a phrasal synchronous grammar model of translational equivalence',\n",
       "  'this sampler sidesteps the intractability issues of previous models which required inference over derivation forests'],\n",
       " 660: ['in our experiments we use the proposed method to generate paraphrases for three different applications'],\n",
       " 661: ['we consider maximum margin and conditional likelihood objectives including the presentation of a new normal form grammar for canonicalizing derivations',\n",
       "  'even for nonitg sentence pairs we show that it is possible learn itg alignment models by simple relaxations of structured discriminative learning objectives',\n",
       "  'altogether our method results in the best reported aer numbers for chineseenglish and a performance improvement of bleu over giza alignments'],\n",
       " 662: ['we also analyze feature performance showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression'],\n",
       " 663: ['we present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers',\n",
       "  'to demonstrate the power and generality of this approach we apply the method in two very different applications named entity recognition and query classification'],\n",
       " 664: [],\n",
       " 665: [],\n",
       " 666: ['we present algorithms for higherorder dependency parsing that are thirdorder in the sense that they can evaluate substructures containing three dependencies and efficient in the sense that they reonly importantly our new parsers can utilize both siblingstyle and grandchildstyle interactions'],\n",
       " 667: [],\n",
       " 668: ['by simultaneously inferring latent topics and topic distributions over relations the benefits of previous approaches like traditional classbased approaches it produces humaninterpretable classes describing each relations preferences but it is competitive with nonclassbased methods in predictive power compare several stateoftheart methods achieving an increase in recall at precision over mutual information erk'],\n",
       " 669: ['our experiments demonstrate that very large crfs can be trained efficiently and that very large models are able to improve the accuracy while delivering compact parameter sets'],\n",
       " 670: ['incremental parsing techniques such as shiftreduce have gained popularity thanks to their efficiency but there remains a problem the search is only explores a tiny fraction of the whole space even with beam search as opposed to dynamic programming'],\n",
       " 671: ['the research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade'],\n",
       " 672: ['but progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language'],\n",
       " 673: ['our approach is based on comparing the crossentropy according to domainspecific and nondomainspecifc language models for each sentence of the text source used to produce the latter language model'],\n",
       " 674: ['using a single unified internal representation for translation forests the decoder strictly separates modelspecific translation logic from general rescoring pruning and inference algorithms',\n",
       "  'from this unified representation the decoder can not only the or translations but also alignments to a reference or the quantities necessary to drive discriminative training using gradientbased or gradientfree optimization techniques'],\n",
       " 675: ['in this paper we propose to improve targetdependent twitter sentiment classification by incorporating targetdependent features and taking related tweets into consideration',\n",
       "  'according to the experimental results our approach greatly improves the performance of targetdependent sentiment classification'],\n",
       " 676: ['a number of different features are extracted and ablation tests are used to investigate their contribution to overall performance'],\n",
       " 677: ['a lack of standard datasets and evaluation has prevented the field of paraphrasmaking the kind of rapid progress enjoyed by the machine translation community over the last years',\n",
       "  'the highly parallel nature of this data allows us to use simple ngram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates',\n",
       "  'in addition to being simple and efficient to compute experiments show that these metrics correlate highly with human judgments'],\n",
       " 678: ['both word similarity and context are then exploited to select the most probable correction candidate for the word'],\n",
       " 679: ['recently researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling but their models assume are for example they extract the pair this paper presents a novel approach for multiinstance learning with overlapping relations that combines a sentencelevel extraction model with a simple corpuslevel component for aggregating the individual facts'],\n",
       " 680: ['in tackling this challenging learning problem we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms'],\n",
       " 681: ['we describe a novel approach for inducing unsupervised partofspeech taggers for languages that have no labeled training data but have translated text in a resourcerich language',\n",
       "  'our method does not assume any knowledge about the target language in particular no tagging dictionary is assumed making it applicable to a wide array of resourcepoor languages'],\n",
       " 682: ['this paper describes an approach to templatebased ie that removes this requirement and performs extraction without knowing the template structure in advance',\n",
       "  'we also solve the standard ie task using the induced syntactic patterns to extract role fillers from specific documents'],\n",
       " 684: ['we address the problem of partofspeech tagging for english data from the popular microblogging service twitter',\n",
       "  'the data and tools have been made available to the research community with the goal of enabling richer text analysis of twitter and related social media data sets'],\n",
       " 685: ['we provide a systematic analysis of the effects of optimizer instabilityan extraneous variable that is seldom controlled foron experimental outcomes and make recommendations for reporting results more accurately'],\n",
       " 687: ['this is problematic becausewords are often polysemous and global con text can also provide useful information for learning word meanings',\n",
       "  'we introduce a new dataset with human judgments on pairs of words in sentential context and evaluate ourmodel on it showing that our model outper forms competitive baselines and other neural language models'],\n",
       " 688: ['natural language parsing has typically been done with small sets of discrete categories such as np and vp but this representation does not capture the full syntactic nor semantic richness of linguistic phrases and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness',\n",
       "  'the cvg learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as pp attachments'],\n",
       " 703: ['in this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns',\n",
       "  'as described in gjw the process of centering attention on entities in the discourse gives rise to the intersentential states of retaining shiftpropose an extension to these states which handles some additional cases of multiple ambiguous pronouns'],\n",
       " 704: ['this paper describes a method of unification by successive approximation resulting in better average performance'],\n",
       " 705: ['to interpret a sentence an approach to abductive inference developed in the tac itus project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized',\n",
       "  'its use in solving the local pragmatics problems of reference compound nominals syntactic ambiguity and metonymy is described and illustrated'],\n",
       " 706: ['we applied control criteria to four dialogues and identified levels of discourse structure',\n",
       "  'we investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control',\n",
       "  'participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not'],\n",
       " 708: ['the enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semanticheaddriven fashion'],\n",
       " 711: ['in order to take steps towards establishing a methodology for evaluating natural language systems we conducted a case study',\n",
       "  'we attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the cospecifiers of pronouns in naturally occurring texts and dialogues',\n",
       "  'we present the quantitative results of handsimulating these algorithms but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general'],\n",
       " 712: ['cdg parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraintpropagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees'],\n",
       " 714: ['we describe a program xtract that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism'],\n",
       " 716: ['this approach exploits the differences between mappings of words to senses in different languages'],\n",
       " 719: ['this paper describes an implemented program that takes a raw untagged text corpus as its only input no openclass dictionary and generates a partial list of verbs occurring in the text and the subcategorization frames sfs in which they occur',\n",
       "  'the completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus'],\n",
       " 720: ['we propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb estimated on the basis of word distribution in a large corpus'],\n",
       " 723: ['we present here criteria and techniques for automatically detecting the presence of a repair its location and making the appropriate correction'],\n",
       " 728: [],\n",
       " 732: [],\n",
       " 733: ['this paper discusses how to estimate the probability of cooccurrences that do not occur in the training data',\n",
       "  'we present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words as determined by an appropriate word similarity metric'],\n",
       " 735: ['we describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts',\n",
       "  'words are represented by the relative frequency distributions of contexts in which they appear and relative entropy between those distributions is used as the similarity measure for clustering'],\n",
       " 736: ['further it is argued that this method can be used to learn all subcategorization frames whereas previous methods are not extensible to a general solution to the problem'],\n",
       " 737: ['by repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus the system learns a set of simple structural transformations that can be applied to reduce error'],\n",
       " 738: ['this paper proposes a new indicator of text structure called the lexical cohesion profile lcp which locates segment boundaries in a text'],\n",
       " 739: ['this paper describes texttiling an algorithm for partitioning expository texts into coherent multiparagraph discourse units which reflect the subtopic structure of the texts',\n",
       "  'the algorithm uses domainindependent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes',\n",
       "  'two fullyimplemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts'],\n",
       " 740: ['we describe our experience with automatic alignment of sentences in parallel englishchinese texts',\n",
       "  'our report concerns three related topics progress on the hkust englishchinese parallel bilingual corpus experiments addressing applicability of gale lengthbased statistical method to the task of alignment involving a nonindoeuropean language and an improved statistical method that also incorporates domainspecific lexical cues'],\n",
       " 741: ['by identifying and utilizing only the single best disambiguating evidence in a target context the algorithm avoids the problematic complex modeling of statistical dependencies',\n",
       "  'although directly applicable to a wide class of ambiguities the algorithm is described and evaluated in a realistic case study the problem of restoring missing accents in spanish and french text'],\n",
       " 743: ['further approximating the joint distribution of all variables with a model identifying only the most important systematic interactions among variables limits the number of parameters to be estimated supports computational efficiency and provides an understanding of the data',\n",
       "  'in this paper a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun describe a method for formulating probabilistic models that use multiple contextual features for wordsense disambiguation without requiring untested assumptions regarding the form of the model'],\n",
       " 745: []}"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo_cbow_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "9ebc8479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:31:44.556879Z",
     "start_time": "2023-05-01T16:31:44.458413Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(frase_objetivo_cbow_ft.items()).to_excel('frase_obj.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc58566",
   "metadata": {},
   "source": [
    "### Similaridade fasttext skip-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "e8406692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:00.679914Z",
     "start_time": "2023-05-01T16:31:44.558792Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_sg_ft,indices_ab_intro_sg_ft,lista_idx_ab_intro_sg_ft =aplicando_dicionario_similaridade(list(v_ft_sg_abstract.values()),list(v_ft_sg_introducao.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_sg_ft,indices_ab_conc_sg_ft,lista_idx_ab_c_sg_ft = aplicando_dicionario_similaridade(list(v_ft_sg_abstract.values()),list(v_ft_sg_conclusao.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_sg_ft = frases_interseccao(lista_idx_ab_intro_sg_ft,lista_idx_ab_c_sg_ft)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_sg_ft = get_frases_objetivo(indices_objetivo_sg_ft, papers_treino.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "4d0cb341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:00.695236Z",
     "start_time": "2023-05-01T16:32:00.679914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18, 19, 20, 31, 32, 33, 35, 37, 39, 41, 43, 45, 47, 48, 50, 51, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 74, 88, 97, 99, 101, 102, 103, 104, 105, 106, 108, 110, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 198, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 222, 223, 225, 226, 228, 229, 231, 232, 234, 235, 243, 244, 245, 246, 248, 258, 263, 272, 276, 277, 279, 280, 281, 282, 283, 284, 286, 296, 307, 310, 313, 315, 316, 318, 323, 326, 338, 339, 342, 345, 347, 348, 350, 351, 352, 355, 356, 357, 358, 359, 362, 363, 364, 365, 366, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 389, 390, 391, 392, 393, 394, 395, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 410, 411, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 427, 428, 430, 431, 432, 433, 434, 435, 436, 437, 439, 440, 442, 443, 445, 446, 448, 449, 450, 451, 452, 454, 455, 456, 457, 458, 460, 464, 465, 466, 467, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 523, 524, 525, 526, 527, 528, 529, 530, 532, 533, 534, 535, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 559, 560, 561, 562, 563, 564, 565, 566, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 579, 581, 582, 583, 584, 586, 587, 588, 590, 591, 592, 593, 594, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 619, 620, 621, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 636, 637, 638, 639, 640, 641, 642, 644, 645, 646, 647, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 684, 685, 687, 688, 703, 704, 705, 706, 708, 711, 712, 714, 716, 719, 720, 723, 728, 732, 733, 735, 736, 737, 738, 739, 740, 741, 743, 745])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo_sg_ft.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70939c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T22:44:02.451091Z",
     "start_time": "2023-04-30T22:44:02.432675Z"
    }
   },
   "source": [
    "joblib.dump(frase_objetivo_sg_ft,'frase_objetivo_sg_ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731a8ab",
   "metadata": {},
   "source": [
    "As frases que extrai foram da variavel `abstract_frases`, ou seja, são as frases do abstract já pré-processadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a578680",
   "metadata": {},
   "source": [
    "## Validação - Exp1 - ROUGE-N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa2de7",
   "metadata": {},
   "source": [
    "### Pré-Processamento Ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "d79219af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:01.438272Z",
     "start_time": "2023-05-01T16:32:00.697231Z"
    }
   },
   "outputs": [],
   "source": [
    "def separa_frases_ouro(txt: str):\n",
    "    \"\"\"Separa as frases dos papers\"\"\"\n",
    "    txt = txt.split('\\n')\n",
    "    return txt\n",
    "\n",
    "padrao_ouro_treino_frases = {k: separa_frases_ouro(v) for k, v in padrao_ouro_treino2.items()}\n",
    "padrao_ouro_treino_frases = segundo_preprocessamento(padrao_ouro_treino_frases)\n",
    "padrao_ouro_treino_palavras = {k:[word_tokenize(f) for f in v] for k,v in padrao_ouro_treino_frases.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e1f5f",
   "metadata": {},
   "source": [
    "### Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8da15c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:01.454360Z",
     "start_time": "2023-05-01T16:32:01.440705Z"
    }
   },
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "fe87826d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:01.464455Z",
     "start_time": "2023-05-01T16:32:01.456340Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max(dic, coord, val):\n",
    "    return max(filter(lambda item: item[0][coord] == val, dic.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "ebdff2e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:01.479953Z",
     "start_time": "2023-05-01T16:32:01.465022Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_rouge(rouge_total):\n",
    "    \"\"\"pega o máximo rouge score para cada frase selecionada como pertencente ao objetivo, em comparação com cada frase\n",
    "    do padrão ouro\"\"\"\n",
    "    maximo_rouge_por_paper =[]\n",
    "    for h in rouge_total:\n",
    "        x=[]\n",
    "        for i in set([i[0] for i in h]):\n",
    "            x.append(get_max(h,0,i))\n",
    "        maximo_rouge_por_paper.append(x)\n",
    "    return maximo_rouge_por_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4f179f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:01.499523Z",
     "start_time": "2023-05-01T16:32:01.482234Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculo_rouge(frase_objetivo,lista_frases_padrao_ouro):\n",
    "    \n",
    "    rouge_precisao=[]\n",
    "    for i,j in zip(frase_objetivo,lista_frases_padrao_ouro):\n",
    "        rouge_1={}\n",
    "        for h in range(len(i)):\n",
    "            for z in range(len(j)):\n",
    "                rouge_1[(h,z)] = (scorer.score(j[z],i[h])['rouge1'].precision)\n",
    "        rouge_precisao.append(rouge_1)\n",
    "    \n",
    "    \n",
    "    rouge_recall=[]\n",
    "    for i,j in zip(frase_objetivo,lista_frases_padrao_ouro):\n",
    "        rouge_2={}\n",
    "        for h in range(len(i)):\n",
    "            for z in range(len(j)):\n",
    "                rouge_2[(h,z)] = (scorer.score(j[z],i[h])['rouge1'].recall)\n",
    "        rouge_recall.append(rouge_2)\n",
    "     \n",
    "    \n",
    "    rouge_fmeasure=[]\n",
    "    for i,j in zip(frase_objetivo,lista_frases_padrao_ouro):\n",
    "        rouge_3={}\n",
    "        for h in range(len(i)):\n",
    "            for z in range(len(j)):\n",
    "                rouge_3[(h,z)] = (scorer.score(j[z],i[h])['rouge1'].fmeasure)\n",
    "        rouge_fmeasure.append(rouge_3)\n",
    "        \n",
    "    return rouge_precisao, rouge_recall, rouge_fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "ecfe36f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:01.513192Z",
     "start_time": "2023-05-01T16:32:01.501522Z"
    }
   },
   "outputs": [],
   "source": [
    "def media(maximo_rouge_por_paper):\n",
    "    media=[]\n",
    "    for i in maximo_rouge_por_paper:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            soma = soma + j[1]\n",
    "        #print(len(i))\n",
    "        try:\n",
    "            media.append(soma/len(i))\n",
    "        except:\n",
    "            media.append(0)\n",
    "    return media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "48b595cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:32:01.523275Z",
     "start_time": "2023-05-01T16:32:01.513192Z"
    }
   },
   "outputs": [],
   "source": [
    "def media2(x):\n",
    "    return round(np.mean(x),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "c4ce9b6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:20.999491Z",
     "start_time": "2023-05-01T16:32:01.523275Z"
    }
   },
   "outputs": [],
   "source": [
    "p_sg_w2v,r_sg_w2v,f_sg_w2v       = calculo_rouge(list(frase_objetivo_sg_w2v.values()),list(padrao_ouro_treino_frases.values()))\n",
    "p_cbow_w2v,r_cbow_w2v,f_cbow_w2v = calculo_rouge(list(frase_objetivo_cbow_w2v.values()),list(padrao_ouro_treino_frases.values()))\n",
    "p_sg_ft,r_sg_ft,f_sg_ft          = calculo_rouge(list(frase_objetivo_sg_ft.values()),list(padrao_ouro_treino_frases.values()))\n",
    "p_cbow_ft,r_cbow_ft,f_cbow_ft    =  calculo_rouge(list(frase_objetivo_cbow_ft.values()),list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "a0ecc29f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:03:54.432303Z",
     "start_time": "2023-05-01T21:03:54.422303Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_80_e_0(precisao,recall):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    z=[]\n",
    "    for i in media(max_rouge(precisao)):\n",
    "        if i>=0.8:\n",
    "            x.append(i)\n",
    "    for i in media(max_rouge(recall)):\n",
    "        if i>=0.8:\n",
    "            z.append(i)\n",
    "    for i in media(max_rouge(precisao)):\n",
    "        if i==0:\n",
    "            y.append(i)\n",
    "    print('precisao acima de 80%:', round(len(x)/len(precisao)*100,0))\n",
    "    print('recall acima de 80%:', round(len(z)/len(precisao)*100,0))\n",
    "    print('igual a 0:', round(len(y)/len(precisao)*100,0))\n",
    "    #return len(x),len(z), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "1dd2c0fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:03:54.632426Z",
     "start_time": "2023-05-01T21:03:54.604364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg_w2v\n",
      "precisão: 0.829\n",
      "revocação: 0.85\n",
      "f-score: 0.834\n",
      "precisao acima de 80%: 80.0\n",
      "recall acima de 80%: 86.0\n",
      "igual a 0: 12.0\n"
     ]
    }
   ],
   "source": [
    "print('sg_w2v')\n",
    "print('precisão:',media2(media(max_rouge(p_sg_w2v))))\n",
    "print('revocação:',media2(media(max_rouge(r_sg_w2v))))\n",
    "print('f-score:',media2(media(max_rouge(f_sg_w2v))))\n",
    "funcao_80_e_0(p_sg_w2v,r_sg_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "317285fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:03:54.839001Z",
     "start_time": "2023-05-01T21:03:54.776510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow_w2v\n",
      "precisão: 0.808\n",
      "revocação: 0.829\n",
      "f-score: 0.814\n",
      "precisao acima de 80%: 78.0\n",
      "recall acima de 80%: 83.0\n",
      "igual a 0: 14.0\n"
     ]
    }
   ],
   "source": [
    "print('cbow_w2v')\n",
    "print('precisão:',media2(media(max_rouge(p_cbow_w2v))))\n",
    "print('revocação:',media2(media(max_rouge(r_cbow_w2v))))\n",
    "print('f-score:',media2(media(max_rouge(f_cbow_w2v))))\n",
    "funcao_80_e_0(p_cbow_w2v,r_cbow_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "e5713247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:03:55.388847Z",
     "start_time": "2023-05-01T21:03:55.341735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg_ft  \n",
      "precisão: 0.813\n",
      "revocação: 0.835\n",
      "f-score: 0.819\n",
      "precisao acima de 80%: 78.0\n",
      "recall acima de 80%: 83.0\n",
      "igual a 0: 14.0\n"
     ]
    }
   ],
   "source": [
    "print('sg_ft  ')\n",
    "print('precisão:',media2(media(max_rouge(p_sg_ft))))\n",
    "print('revocação:',media2(media(max_rouge(r_sg_ft))))\n",
    "print('f-score:',media2(media(max_rouge(f_sg_ft))))\n",
    "funcao_80_e_0(p_sg_ft,r_sg_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "83d4f6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:04:17.105561Z",
     "start_time": "2023-05-01T21:04:17.059048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow_ft\n",
      "precisão: 0.832\n",
      "revocação: 0.855\n",
      "f-score: 0.838\n",
      "precisao acima de 80%: 79.0\n",
      "recall acima de 80%: 86.0\n",
      "igual a 0: 11.0\n"
     ]
    }
   ],
   "source": [
    "print('cbow_ft')\n",
    "print('precisão:',media2(media(max_rouge(p_cbow_ft))))\n",
    "print('revocação:',media2(media(max_rouge(r_cbow_ft))))\n",
    "print('f-score:',media2(media(max_rouge(f_cbow_ft))))\n",
    "funcao_80_e_0(p_cbow_ft,r_cbow_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "5e30d117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.321499Z",
     "start_time": "2023-05-01T16:33:21.081678Z"
    }
   },
   "outputs": [],
   "source": [
    "padrao_ouro_teste_frases   = {k: separa_frases_ouro(v) for k, v in padrao_ouro_teste2.items()}\n",
    "padrao_ouro_teste_frases   = segundo_preprocessamento(padrao_ouro_teste_frases)\n",
    "padrao_ouro_teste_palavras = {k:[word_tokenize(f) for f in v] for k,v in padrao_ouro_teste_frases.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "332a8724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.336929Z",
     "start_time": "2023-05-01T16:33:21.324409Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_padrao_ouro_treino  = list(padrao_ouro_treino_frases.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "47c0e16a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.352392Z",
     "start_time": "2023-05-01T16:33:21.338910Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_padrao_ouro_teste = list(padrao_ouro_teste_frases.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2592b",
   "metadata": {},
   "source": [
    "# Experimento 1 - Conclusão\n",
    "Rouge Usado foi o N=1 e a precisão e recall foram calculadas a partir do f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "3dde4ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:51:15.991123Z",
     "start_time": "2023-05-01T16:51:15.972084Z"
    }
   },
   "outputs": [],
   "source": [
    "def analise_maior(precisao, revocacao):\n",
    "    x=[]\n",
    "    qtd_zero=[]\n",
    "    for i in precisao:\n",
    "        if i>=0.8:\n",
    "            x.append(i)\n",
    "        if i==0:\n",
    "            qtd_zero.append(i)\n",
    "                       \n",
    "    y=[]\n",
    "    qtd_zero=[]\n",
    "    for i in revocacao:\n",
    "        if i>=0.8:\n",
    "            y.append(i)\n",
    "            \n",
    "    print('qtd de papers com precisão>=0.8:',len(x))\n",
    "    print('qtd de papers com revocação>=0.8:',len(y))\n",
    "    print('qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções:',len(qtd_zero))\n",
    "    return len(x), len(y), len(qtd_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944e880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:39:01.037732Z",
     "start_time": "2023-05-01T04:39:01.022096Z"
    }
   },
   "source": [
    "def analise_revocacao(revocacao,lista_frases_padrao_ouro,media_ouro):\n",
    "    x=[]\n",
    "    qtd_zero=[]\n",
    "    for i in revocacao:\n",
    "        if i>=(1/media_ouro): #valor de 1/média qtd frases padrao ouro\n",
    "            x.append(i)\n",
    "        if i==0:\n",
    "            qtd_zero.append(i)\n",
    "    print('qtd de papers com revocação>=(1/media_ouro):',len(x))\n",
    "    print('% do total:',len(x)/len(lista_frases_padrao_ouro))\n",
    "    print('qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções:',len(qtd_zero))\n",
    "    print('% do total:',len(qtd_zero)/len(lista_frases_padrao_ouro))\n",
    "    print('media recall', np.mean(revocacao))\n",
    "    return len(x), np.mean(revocacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "cdc83040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.367849Z",
     "start_time": "2023-05-01T16:33:21.354379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.642424242424243"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Média de palavras do Padrão Ouro\n",
    "x=[]\n",
    "for i in lista_padrao_ouro_treino:\n",
    "    x.append(len(i))\n",
    "media_ouro=np.mean(x)\n",
    "media_ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "b911573e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.383924Z",
     "start_time": "2023-05-01T16:33:21.369850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.783018867924528"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Média de palavras do Padrão Ouro\n",
    "x=[]\n",
    "for i in lista_padrao_ouro_teste:\n",
    "    x.append(len(i))\n",
    "media_ouro_teste=np.mean(x)\n",
    "media_ouro_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822e13e",
   "metadata": {},
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8c537",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T03:05:37.976743Z",
     "start_time": "2023-03-30T03:05:37.836769Z"
    }
   },
   "source": [
    "import collections\n",
    "\n",
    "def funcao_ngramas(lista_frases, n):\n",
    "    todos_papers=[]\n",
    "    for lista_frases_paper in lista_frases:\n",
    "        perp=[]\n",
    "        x=[]\n",
    "        for line in lista_frases_paper:\n",
    "            token = word_tokenize(line)\n",
    "            ngrama = list(ngrams(token, n))\n",
    "            x.append(ngrama)\n",
    "    return x\n",
    "\n",
    "bigramas = funcao_ngramas(list(frase_objetivo_cbow_ft.values()),5)\n",
    "\n",
    "x=[]\n",
    "for i in bigramas:\n",
    "    for j in i:\n",
    "        x.append(j)\n",
    "        \n",
    "counter = collections.Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "73438d41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.555316Z",
     "start_time": "2023-05-01T16:33:21.385846Z"
    }
   },
   "outputs": [],
   "source": [
    "todos_papers=[]\n",
    "for lista_frases_paper in list(frase_objetivo_cbow_ft.values()):\n",
    "    for line in lista_frases_paper:\n",
    "        token = word_tokenize(line)\n",
    "        #print(token)\n",
    "        ngrama = list(ngrams(token, 5))\n",
    "        #print(ngrama)\n",
    "        todos_papers.append(ngrama)\n",
    "\n",
    "x=[]\n",
    "for i in todos_papers:\n",
    "    for j in i:\n",
    "        x.append(j)  \n",
    "        \n",
    "counter = collections.Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "576a7fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.570797Z",
     "start_time": "2023-05-01T16:33:21.555925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('in', 'this', 'paper', 'we', 'present'), 14),\n",
       " (('this', 'paper', 'we', 'present', 'a'), 11),\n",
       " (('in', 'this', 'paper', 'we', 'propose'), 5),\n",
       " (('this', 'paper', 'we', 'present', 'an'), 3),\n",
       " (('we', 'address', 'the', 'problem', 'of'), 3),\n",
       " (('the', 'current', 'state', 'of', 'the'), 3),\n",
       " (('paper', 'we', 'present', 'a', 'novel'), 3),\n",
       " (('an', 'error', 'reduction', 'of', 'over'), 3),\n",
       " (('performs', 'at', 'least', 'as', 'well'), 2),\n",
       " (('at', 'least', 'as', 'well', 'as'), 2),\n",
       " (('this', 'paper', 'presents', 'a', 'corpusbased'), 2),\n",
       " (('paper', 'presents', 'a', 'corpusbased', 'approach'), 2),\n",
       " (('presents', 'a', 'corpusbased', 'approach', 'to'), 2),\n",
       " (('a', 'corpusbased', 'approach', 'to', 'word'), 2),\n",
       " (('corpusbased', 'approach', 'to', 'word', 'sense'), 2),\n",
       " (('approach', 'to', 'word', 'sense', 'disambiguation'), 2),\n",
       " (('the', 'wall', 'street', 'journal', 'treebank'), 2),\n",
       " (('we', 'propose', 'a', 'method', 'for'), 2),\n",
       " (('we', 'present', 'a', 'method', 'that'), 2),\n",
       " (('present', 'a', 'method', 'that', 'makes'), 2)]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "322925c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.720239Z",
     "start_time": "2023-05-01T16:33:21.572241Z"
    }
   },
   "outputs": [],
   "source": [
    "todos_papers=[]\n",
    "for lista_frases_paper in list(frase_objetivo_cbow_ft.values()):\n",
    "    for line in lista_frases_paper:\n",
    "        token = word_tokenize(line)\n",
    "        #print(token)\n",
    "        ngrama = list(ngrams(token, 4))\n",
    "        #print(ngrama)\n",
    "        todos_papers.append(ngrama)\n",
    "\n",
    "x=[]\n",
    "for i in todos_papers:\n",
    "    for j in i:\n",
    "        x.append(j)  \n",
    "        \n",
    "counter = collections.Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "d840ebb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.751877Z",
     "start_time": "2023-05-01T16:33:21.724155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('in', 'this', 'paper', 'we'), 44),\n",
       " (('this', 'paper', 'we', 'present'), 15),\n",
       " (('this', 'paper', 'presents', 'a'), 14),\n",
       " (('paper', 'we', 'present', 'a'), 11),\n",
       " (('can', 'be', 'used', 'to'), 7),\n",
       " (('we', 'present', 'a', 'novel'), 6),\n",
       " (('this', 'paper', 'describes', 'a'), 6),\n",
       " (('and', 'show', 'that', 'our'), 5),\n",
       " (('this', 'paper', 'we', 'propose'), 5),\n",
       " (('we', 'present', 'a', 'new'), 4),\n",
       " (('we', 'present', 'a', 'method'), 4),\n",
       " (('we', 'introduce', 'a', 'new'), 4),\n",
       " (('on', 'the', 'basis', 'of'), 4),\n",
       " (('state', 'of', 'the', 'art'), 4),\n",
       " (('an', 'error', 'reduction', 'of'), 4),\n",
       " (('the', 'wall', 'street', 'journal'), 3),\n",
       " (('paper', 'we', 'present', 'an'), 3),\n",
       " (('we', 'present', 'an', 'approach'), 3),\n",
       " (('we', 'address', 'the', 'problem'), 3),\n",
       " (('address', 'the', 'problem', 'of'), 3)]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7402b7",
   "metadata": {},
   "source": [
    "# Experimento 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0a903",
   "metadata": {},
   "source": [
    "`frases_objetivo_modelo` depende do modelo selecionado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46473e4b",
   "metadata": {},
   "source": [
    "https://www.cs.brandeis.edu/~cs136a/CS136a_Slides/CS136a_Lect11_PerplexityAndSmoothing.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "4845cdeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.766869Z",
     "start_time": "2023-05-01T16:33:21.754893Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_start_end_sentences(frase_objetivo_ft_formato_ml):\n",
    "    lista=[]\n",
    "    for i in frase_objetivo_ft_formato_ml:\n",
    "        x = ['<s>'] + i + ['</s>']\n",
    "        lista.append(x)\n",
    "    return lista\n",
    "\n",
    "def funcao_aplicar_es_sentencer(dicionario):\n",
    "    \"\"\"Essa função aplica o <s> e o </s> em cada frase\n",
    "    entrada: dicionario de palavras (ex: palavras_papers_teste ou palavras_papers_treino)\n",
    "    saida: lista com os tokens de palavras de frases com o start e end of sentences\"\"\"\n",
    "    x=[]\n",
    "    for i in range(len(dicionario.values())):\n",
    "        x.append(funcao_start_end_sentences(list(dicionario.values())[i]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109a1c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T02:20:17.778783Z",
     "start_time": "2023-04-02T02:20:17.465271Z"
    }
   },
   "source": [
    "frases_objetivo_modelo = frase_objetivo_cbow_ft.copy()\n",
    "palavras_objetivo_modelo = {k:[word_tokenize(f) for f in v] for k,v in frases_objetivo_modelo.items()}\n",
    "\n",
    "#O N-gramas é calculado a partir de uma lista com listas de palavras de cada frase\n",
    "frase_objetivo_ft_formato_ml=[]\n",
    "for i in list(palavras_objetivo_modelo.values()):\n",
    "    for j in i:\n",
    "        frase_objetivo_ft_formato_ml.append(j)\n",
    "frase_objetivo_ft_formato_ml = funcao_start_end_sentences(frase_objetivo_ft_formato_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea41a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T13:28:13.601090Z",
     "start_time": "2023-04-02T13:28:13.366682Z"
    }
   },
   "source": [
    "frases_objetivo_modelo = frase_objetivo_cbow_ft.copy()\n",
    "palavras_objetivo_modelo = {k:[word_tokenize(f) for f in v] for k,v in frases_objetivo_modelo.items()}\n",
    "\n",
    "#O N-gramas é calculado a partir de uma lista com listas de palavras de cada frase\n",
    "frase_objetivo_ft_formato_ml=[]\n",
    "for i in list(palavras_objetivo_modelo.values()):\n",
    "    for j in i:\n",
    "        frase_objetivo_ft_formato_ml.append(j)\n",
    "frase_objetivo_ft_formato_ml = (frase_objetivo_ft_formato_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "8743504e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.781889Z",
     "start_time": "2023-05-01T16:33:21.768874Z"
    }
   },
   "outputs": [],
   "source": [
    "frases_objetivo_modelo = frase_objetivo_cbow_ft.copy()\n",
    "\n",
    "x=[]\n",
    "for i in list(frase_objetivo_cbow_ft.values()):\n",
    "    for j in i:\n",
    "        x.append(j)\n",
    "frase_objetivo_ft_formato_ml = x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "71d332be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.802491Z",
     "start_time": "2023-05-01T16:33:21.783870Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_perplexidade(lista_frases, n, modelo):\n",
    "    todos_papers=[]\n",
    "    for lista_frases_paper in lista_frases:\n",
    "        perp=[]\n",
    "        x=[]\n",
    "        for line in lista_frases_paper:\n",
    "            token = word_tokenize(line)\n",
    "            ngrama = list(ngrams(token, n, pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"))\n",
    "            x.append(ngrama)\n",
    "\n",
    "        for bigramas_frase in x:\n",
    "            #print(model_laplace_bi.perplexity(bigramas_frase))\n",
    "            perp.append(modelo.perplexity(bigramas_frase))\n",
    "        todos_papers.append(perp)\n",
    "    return todos_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "ee552d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.818477Z",
     "start_time": "2023-05-01T16:33:21.805031Z"
    }
   },
   "outputs": [],
   "source": [
    "def media_perplexidade(lista_perplexidade_todos_papers):\n",
    "    media = [np.mean(i) for i in lista_perplexidade_todos_papers]\n",
    "    return (np.mean(media))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "7b91c752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.833802Z",
     "start_time": "2023-05-01T16:33:21.819457Z"
    }
   },
   "outputs": [],
   "source": [
    "def lista_sem_infinito(lista_perplexidade):\n",
    "    lista_sem_infinito=[]\n",
    "    for i in range(len(lista_perplexidade)):\n",
    "        x = [v for v in lista_perplexidade[i] if not np.isnan(v) and not np.isinf(v)]\n",
    "        lista_sem_infinito.append(x)\n",
    "    return lista_sem_infinito\n",
    "\n",
    "def media_lista_sem_infinito(input_lista_sem_infinito):\n",
    "    x=[]\n",
    "    for i in input_lista_sem_infinito:\n",
    "        x.append(np.mean(i))\n",
    "    x = [v for v in x if not np.isnan(v) and not np.isinf(v)]\n",
    "    return np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "3796f26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.849806Z",
     "start_time": "2023-05-01T16:33:21.835802Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_frases(lista_frases):\n",
    "    \"\"\"lista_frases = frases_papers_treino ou frases_papers_teste\"\"\"\n",
    "    x=[]\n",
    "    for j in lista_frases:\n",
    "        h=[]\n",
    "        for z in j:\n",
    "            h.append(nltk.tokenize.word_tokenize(z))\n",
    "        x.append(h)\n",
    "    return x\n",
    "\n",
    "\n",
    "def gerador_ngramas(token_frases, n):\n",
    "    x=[]\n",
    "    for i in token_frases:\n",
    "        z=[]\n",
    "        for j in i:\n",
    "            #print(list(nltk.ngrams(j, n, pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\")))\n",
    "            z.append(list(nltk.ngrams(j, n, pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\")))\n",
    "        x.append(z)\n",
    "    return x\n",
    "\n",
    "def gerador_probabilidades(ngramas, modelo):\n",
    "    \"\"\"exemplo:\n",
    "    ngramas=bigramas_treino\n",
    "    modelo = model_bi\"\"\"\n",
    "    geral=[]\n",
    "    for a in ngramas:\n",
    "        x=[]\n",
    "        for b in a:\n",
    "            prob=1\n",
    "            for i in b:\n",
    "                #print(i, model_bi.score(i[-1], i[:-1]))\n",
    "                prob = prob * modelo.score(i[-1], i[:-1])\n",
    "            #print(prob)\n",
    "            x.append(prob)\n",
    "        geral.append(x)\n",
    "    return geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b9d1ae68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.865801Z",
     "start_time": "2023-05-01T16:33:21.851803Z"
    }
   },
   "outputs": [],
   "source": [
    "def selecao_frases_obj_mle(probabibilidade, frases_papers):\n",
    "    \"\"\"função para selecionar até 3 maiores elementos da lista\"\"\"\n",
    "    #Pega os 3 elementos de maior probabilidade\n",
    "    lista=[]\n",
    "    dicionario={}\n",
    "    x=[]\n",
    "    for i,j in zip(range(len(probabibilidade)),range(len(frases_papers))):\n",
    "        lista.append(sorted(zip(probabibilidade[i],frases_papers[j]), reverse=True)[:3])\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "d43a88f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.881802Z",
     "start_time": "2023-05-01T16:33:21.867801Z"
    }
   },
   "outputs": [],
   "source": [
    "def filtro_selecao_frases_mle(frase_objetivo_trigramas_treino):\n",
    "    lista_frase=[]\n",
    "    lista_prob=[]\n",
    "    lista_geral=[]\n",
    "    for i in frase_objetivo_trigramas_treino:\n",
    "        frase=[]\n",
    "        prob=[]\n",
    "        geral=[]\n",
    "        for j,h in i:\n",
    "            if j!=0:\n",
    "                frase.append(h)\n",
    "                prob.append(j)\n",
    "                geral.append((h,j))\n",
    "        lista_frase.append(frase)\n",
    "        lista_prob.append(prob)\n",
    "        lista_geral.append(geral)\n",
    "    return lista_frase,lista_prob,lista_geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "0526ecbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:21.896801Z",
     "start_time": "2023-05-01T16:33:21.882802Z"
    }
   },
   "outputs": [],
   "source": [
    "frases_papers_treino2 = list(frases_papers_treino.values())\n",
    "frases_papers_teste2 = list(frases_papers_teste.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "ff509083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:35.424543Z",
     "start_time": "2023-05-01T16:33:21.898802Z"
    }
   },
   "outputs": [],
   "source": [
    "token_frases_treino     = token_frases(frases_papers_treino2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "3faad834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:41.093293Z",
     "start_time": "2023-05-01T16:33:35.424543Z"
    }
   },
   "outputs": [],
   "source": [
    "token_frases_teste     = token_frases(frases_papers_teste2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c313a",
   "metadata": {},
   "source": [
    "## Unigrama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4e4cd",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "72f083db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:33:41.130288Z",
     "start_time": "2023-05-01T16:33:41.095259Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_uni'):\n",
    "    model_uni = joblib.load('model_uni')\n",
    "else:\n",
    "    n = 1\n",
    "    paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "    model_uni = MLE(n)\n",
    "    model_uni.fit(train_data, padded_sents)\n",
    "\n",
    "    print(model_uni.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde5a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T14:13:12.412673Z",
     "start_time": "2023-04-02T14:13:11.200601Z"
    }
   },
   "source": [
    "n = 1\n",
    "paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "model_uni = MLE(n)\n",
    "model_uni.fit(train_data, padded_sents)\n",
    "\n",
    "print(model_uni.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb6209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T15:28:13.417222Z",
     "start_time": "2023-04-02T15:28:13.352731Z"
    }
   },
   "source": [
    "joblib.dump(model_uni,'model_uni')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff021d3b",
   "metadata": {},
   "source": [
    "#### Perplexidade Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "59bce72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:34:16.876521Z",
     "start_time": "2023-05-01T16:33:41.152463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: inf\n",
      "média sem infinito: 994.6652048610426\n"
     ]
    }
   ],
   "source": [
    "unigramas_treino          = gerador_ngramas(token_frases_treino,1)\n",
    "probabilidades_unigramas  = gerador_probabilidades(unigramas_treino, model_uni)\n",
    "\n",
    "unigramas_perp_treino       = funcao_perplexidade(list(frases_papers_treino.values()), 1 , model_uni)\n",
    "media_unigramas_perp_treino = media_perplexidade(unigramas_perp_treino)\n",
    "\n",
    "unigramas_perp_treino_s_inf = lista_sem_infinito(unigramas_perp_treino)\n",
    "media_unigramas_perp_treino_s_inf = media_lista_sem_infinito(unigramas_perp_treino_s_inf)\n",
    "\n",
    "print('média:',media_unigramas_perp_treino)\n",
    "print('média sem infinito:', media_unigramas_perp_treino_s_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8001b",
   "metadata": {},
   "source": [
    "#### Rouge Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "a1990350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:34:16.917424Z",
     "start_time": "2023-05-01T16:34:16.878769Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_unigramas_treino         = selecao_frases_obj_mle(probabilidades_unigramas, frases_papers_treino2)\n",
    "lista_frase_uni,lista_prob_uni,lista_geral_uni      = filtro_selecao_frases_mle(frase_objetivo_unigramas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "7cb6cbe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:34:42.233554Z",
     "start_time": "2023-05-01T16:34:16.917424Z"
    }
   },
   "outputs": [],
   "source": [
    "p_uni_treino,r_uni_treino,f_uni_treino  = calculo_rouge(lista_frase_uni,list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "c172356b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:04:33.474945Z",
     "start_time": "2023-05-01T21:04:33.378058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrama treino sem laplace\n",
      "precisão: 0.447\n",
      "revocação: 0.223\n",
      "f-score: 0.254\n",
      "precisao acima de 80%: 5.0\n",
      "recall acima de 80%: 1.0\n",
      "igual a 0: 4.0\n"
     ]
    }
   ],
   "source": [
    "print('unigrama treino sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_uni_treino))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni_treino))))\n",
    "print('f-score:',media2(media(max_rouge(f_uni_treino))))\n",
    "funcao_80_e_0(p_uni_treino,r_uni_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d158f",
   "metadata": {},
   "source": [
    "rouge_unigramas                         = calculo_rouge(lista_frase_uni,list(padrao_ouro_treino_frases.values()))\n",
    "maximo_rouge_unigramas                  = max_rouge(rouge_unigramas) #lista de índices e maior rouge\n",
    "precisao_unigramas, revocacao_unigramas = rouge_precisao_revocacao(maximo_rouge_unigramas, lista_padrao_ouro_treino)\n",
    "analise_revocacao(revocacao_unigramas, lista_padrao_ouro_treino, media_ouro)\n",
    "print('----')\n",
    "analise_precisao(precisao_unigramas, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5438e1",
   "metadata": {},
   "source": [
    "#### Perplexidade Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "15d2d0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:34:56.097552Z",
     "start_time": "2023-05-01T16:34:42.289845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: inf\n",
      "média sem infinito: 998.6757506011328\n"
     ]
    }
   ],
   "source": [
    "unigramas_teste          = gerador_ngramas(token_frases_teste,1)\n",
    "probabilidades_unigramas  = gerador_probabilidades(unigramas_teste, model_uni)\n",
    "\n",
    "unigramas_perp_teste       = funcao_perplexidade(list(frases_papers_teste.values()), 1 , model_uni)\n",
    "media_unigramas_perp_teste = media_perplexidade(unigramas_perp_teste)\n",
    "\n",
    "unigramas_perp_teste_s_inf = lista_sem_infinito(unigramas_perp_teste)\n",
    "media_unigramas_perp_teste_s_inf = media_lista_sem_infinito(unigramas_perp_teste_s_inf)\n",
    "\n",
    "print('média:',media_unigramas_perp_teste)\n",
    "print('média sem infinito:', media_unigramas_perp_teste_s_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5754ba8",
   "metadata": {},
   "source": [
    "#### Rouge Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "ceb6d99c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:34:56.129102Z",
     "start_time": "2023-05-01T16:34:56.098672Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_unigramas_teste                     = selecao_frases_obj_mle(probabilidades_unigramas, frases_papers_teste2)\n",
    "lista_frase_uni,lista_prob_uni,lista_geral_uni     = filtro_selecao_frases_mle(frase_objetivo_unigramas_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "284bb8d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:35:06.104319Z",
     "start_time": "2023-05-01T16:34:56.132026Z"
    }
   },
   "outputs": [],
   "source": [
    "p_uni_teste,r_uni_teste,f_uni_teste  = calculo_rouge(lista_frase_uni,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "8570767b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:04:44.067916Z",
     "start_time": "2023-05-01T21:04:44.012153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrama teste sem laplace\n",
      "precisão: 0.419\n",
      "revocação: 0.23\n",
      "f-score: 0.258\n",
      "precisao acima de 80%: 3.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 4.0\n"
     ]
    }
   ],
   "source": [
    "print('unigrama teste sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_uni_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_uni_teste))))\n",
    "funcao_80_e_0(p_uni_teste,r_uni_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165b114",
   "metadata": {},
   "source": [
    "rouge_unigramas                         = calculo_rouge(lista_frase_uni,list(padrao_ouro_teste_frases.values()))\n",
    "maximo_rouge_unigramas                  = max_rouge(rouge_unigramas) #lista de índices e maior rouge\n",
    "precisao_unigramas, revocacao_unigramas = rouge_precisao_revocacao(maximo_rouge_unigramas, lista_padrao_ouro_teste)\n",
    "analise_revocacao(revocacao_unigramas, lista_padrao_ouro_teste, media_ouro_teste)\n",
    "print('----')\n",
    "analise_precisao(precisao_unigramas, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766adeff",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "5f9d2f63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:35:06.155112Z",
     "start_time": "2023-05-01T16:35:06.126301Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_uni_laplace'):\n",
    "    model_uni_laplace = joblib.load('model_uni_laplace')\n",
    "else:\n",
    "    n = 1\n",
    "    paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "    model_uni_laplace = Laplace(n)\n",
    "    model_uni_laplace.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42745f9f",
   "metadata": {},
   "source": [
    "#### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "d5e353ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:35:44.367179Z",
     "start_time": "2023-05-01T16:35:06.155942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: 1329.0739210268796\n",
      "média sem infinito: 1329.0739210268796\n"
     ]
    }
   ],
   "source": [
    "unigramas_treino          = gerador_ngramas(token_frases_treino,1)\n",
    "probabilidades_unigramas_laplace  = gerador_probabilidades(unigramas_treino, model_uni_laplace)\n",
    "\n",
    "unigramas_perp_treino       = funcao_perplexidade(list(frases_papers_treino.values()), 1 , model_uni_laplace)\n",
    "media_unigramas_perp_treino = media_perplexidade(unigramas_perp_treino)\n",
    "\n",
    "unigramas_perp_treino_s_inf = lista_sem_infinito(unigramas_perp_treino)\n",
    "media_unigramas_perp_treino_s_inf = media_lista_sem_infinito(unigramas_perp_treino_s_inf)\n",
    "\n",
    "print('média:',media_unigramas_perp_treino)\n",
    "print('média sem infinito:', media_unigramas_perp_treino_s_inf)\n",
    "\n",
    "frase_objetivo_unigramas_treino         = selecao_frases_obj_mle(probabilidades_unigramas_laplace, frases_papers_treino2)\n",
    "lista_frase_uni,lista_prob_uni,lista_geral_uni      = filtro_selecao_frases_mle(frase_objetivo_unigramas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "d1868271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:36:07.241655Z",
     "start_time": "2023-05-01T16:35:44.368171Z"
    }
   },
   "outputs": [],
   "source": [
    "p_uni_treino_laplace,r_uni_treino_laplace,f_uni_treino_laplace  = calculo_rouge(lista_frase_uni,list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "ecb2aa8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:04:55.125907Z",
     "start_time": "2023-05-01T21:04:55.036358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrama treino com laplace\n",
      "precisão: 0.347\n",
      "revocação: 0.138\n",
      "f-score: 0.167\n",
      "precisao acima de 80%: 3.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 11.0\n"
     ]
    }
   ],
   "source": [
    "print('unigrama treino com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_uni_treino_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni_treino_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_uni_treino_laplace))))\n",
    "funcao_80_e_0(p_uni_treino_laplace,r_uni_treino_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4265bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:40:56.388049Z",
     "start_time": "2023-05-01T04:40:56.372295Z"
    }
   },
   "source": [
    "\n",
    "rouge_unigramas                         = calculo_rouge(lista_frase_uni,list(padrao_ouro_treino_frases.values()))\n",
    "maximo_rouge_unigramas                  = max_rouge(rouge_unigramas) #lista de índices e maior rouge\n",
    "precisao_unigramas, revocacao_unigramas = rouge_precisao_revocacao(maximo_rouge_unigramas, lista_padrao_ouro_treino)\n",
    "analise_revocacao(revocacao_unigramas, lista_padrao_ouro_treino, media_ouro)\n",
    "print('----')\n",
    "analise_precisao(precisao_unigramas, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41246527",
   "metadata": {},
   "source": [
    "#### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4dbe9a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:36:21.123516Z",
     "start_time": "2023-05-01T16:36:07.316864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: 1294.3986490998302\n",
      "média sem infinito: 1294.3986490998302\n"
     ]
    }
   ],
   "source": [
    "unigramas_teste          = gerador_ngramas(token_frases_teste,1)\n",
    "probabilidades_unigramas_laplace_teste  = gerador_probabilidades(unigramas_teste, model_uni_laplace)\n",
    "\n",
    "unigramas_perp_teste       = funcao_perplexidade(list(frases_papers_teste.values()), 1 , model_uni_laplace)\n",
    "media_unigramas_perp_teste = media_perplexidade(unigramas_perp_teste)\n",
    "\n",
    "unigramas_perp_teste_s_inf = lista_sem_infinito(unigramas_perp_teste)\n",
    "media_unigramas_perp_teste_s_inf = media_lista_sem_infinito(unigramas_perp_teste_s_inf)\n",
    "\n",
    "print('média:',media_unigramas_perp_teste)\n",
    "print('média sem infinito:', media_unigramas_perp_teste_s_inf)\n",
    "\n",
    "frase_objetivo_unigramas_teste         = selecao_frases_obj_mle(probabilidades_unigramas_laplace_teste, frases_papers_teste2)\n",
    "lista_frase_uni,lista_prob_uni,lista_geral_uni      = filtro_selecao_frases_mle(frase_objetivo_unigramas_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "d521f652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:36:29.860270Z",
     "start_time": "2023-05-01T16:36:21.125509Z"
    }
   },
   "outputs": [],
   "source": [
    "p_uni_teste_laplace,r_uni_teste_laplace,f_uni_teste_laplace  = calculo_rouge(lista_frase_uni,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "a3ac82d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:05:02.637898Z",
     "start_time": "2023-05-01T21:05:02.593945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrama teste com laplace\n",
      "precisão: 0.351\n",
      "revocação: 0.164\n",
      "f-score: 0.191\n",
      "precisao acima de 80%: 2.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 9.0\n"
     ]
    }
   ],
   "source": [
    "print('unigrama teste com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_uni_teste_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni_teste_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_uni_teste_laplace))))\n",
    "funcao_80_e_0(p_uni_teste_laplace,r_uni_teste_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c03646",
   "metadata": {},
   "source": [
    "rouge_unigramas                         = calculo_rouge(lista_frase_uni,list(padrao_ouro_teste_frases.values()))\n",
    "maximo_rouge_unigramas                  = max_rouge(rouge_unigramas) #lista de índices e maior rouge\n",
    "precisao_unigramas, revocacao_unigramas = rouge_precisao_revocacao(maximo_rouge_unigramas, lista_padrao_ouro_teste)\n",
    "analise_revocacao(revocacao_unigramas, lista_padrao_ouro_teste, media_ouro_teste)\n",
    "print('----')\n",
    "analise_precisao(precisao_unigramas, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8296f",
   "metadata": {},
   "source": [
    "## Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "8352293b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:36:30.044225Z",
     "start_time": "2023-05-01T16:36:29.905367Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_bi'):\n",
    "    model_bi = joblib.load('model_bi')\n",
    "else:\n",
    "    n = 2\n",
    "    paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "    model_bi = MLE(n)\n",
    "    model_bi.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd8cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T14:14:48.105382Z",
     "start_time": "2023-04-02T14:14:47.197602Z"
    }
   },
   "source": [
    "n = 2\n",
    "paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "model_bi = MLE(n)\n",
    "model_bi.fit(train_data, padded_sents)\n",
    "\n",
    "print(model_bi.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ca013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T15:26:27.450513Z",
     "start_time": "2023-04-02T15:26:23.650074Z"
    }
   },
   "source": [
    "joblib.dump(model_bi,'model_bi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58a89f",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29ac0f",
   "metadata": {},
   "source": [
    "#### Perplexidade Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "654a6f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:37:29.475549Z",
     "start_time": "2023-05-01T16:36:30.047125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: inf\n",
      "média sem infinito: 20.823422762126768\n"
     ]
    }
   ],
   "source": [
    "bigramas_treino          = gerador_ngramas(token_frases_treino,2)\n",
    "probabilidades_bigramas  = gerador_probabilidades(bigramas_treino, model_bi)\n",
    "\n",
    "bigramas_perp_treino       = funcao_perplexidade(list(frases_papers_treino.values()), 2 , model_bi)\n",
    "media_bigramas_perp_treino = media_perplexidade(bigramas_perp_treino)\n",
    "\n",
    "bigramas_perp_treino_s_inf       = lista_sem_infinito(bigramas_perp_treino)\n",
    "media_bigramas_perp_treino_s_inf = media_lista_sem_infinito(bigramas_perp_treino_s_inf)\n",
    "\n",
    "\n",
    "print('média:',media_bigramas_perp_treino)\n",
    "print('média sem infinito:', media_bigramas_perp_treino_s_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4ebc7",
   "metadata": {},
   "source": [
    "#### Rouge Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "e8e14e18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:37:29.534018Z",
     "start_time": "2023-05-01T16:37:29.478535Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_bigramas_treino    = selecao_frases_obj_mle(probabilidades_bigramas, frases_papers_treino2)\n",
    "lista_frase_bigramas,lista_prob_bigramas,lista_geral_bigramas = filtro_selecao_frases_mle(frase_objetivo_bigramas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "90a8ccad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:37:54.397772Z",
     "start_time": "2023-05-01T16:37:29.536013Z"
    }
   },
   "outputs": [],
   "source": [
    "p_bi_treino,r_bi_treino,f_bi_treino     = calculo_rouge(lista_frase_bigramas,list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "e594f7f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:05:17.123786Z",
     "start_time": "2023-05-01T21:05:17.077700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrama treino sem laplace\n",
      "precisão: 0.827\n",
      "revocação: 0.817\n",
      "f-score: 0.805\n",
      "precisao acima de 80%: 76.0\n",
      "recall acima de 80%: 77.0\n",
      "igual a 0: 10.0\n"
     ]
    }
   ],
   "source": [
    "print('bigrama treino sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_bi_treino))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi_treino))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi_treino))))\n",
    "funcao_80_e_0(p_bi_treino,r_bi_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9546f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:41:49.775350Z",
     "start_time": "2023-05-01T04:41:43.806827Z"
    }
   },
   "source": [
    "rouge_bigramas                         = calculo_rouge(lista_frase_bigramas,list(padrao_ouro_treino_frases.values()))\n",
    "maximo_rouge_bigramas                  = max_rouge(rouge_bigramas) #lista de índices e maior rouge\n",
    "precisao_bigramas, revocacao_bigramas = rouge_precisao_revocacao(maximo_rouge_bigramas, lista_padrao_ouro_treino)\n",
    "analise_revocacao(revocacao_bigramas, lista_padrao_ouro_treino, media_ouro)\n",
    "print('----')\n",
    "analise_precisao(precisao_bigramas, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18df81",
   "metadata": {},
   "source": [
    "#### Perplexidade Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "f0848c8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:38:16.593061Z",
     "start_time": "2023-05-01T16:37:54.431396Z"
    }
   },
   "outputs": [],
   "source": [
    "bigramas_teste          = gerador_ngramas(token_frases_teste,2)\n",
    "probabilidades_bigramas  = gerador_probabilidades(bigramas_teste, model_bi)\n",
    "\n",
    "bigramas_perp_teste       = funcao_perplexidade(list(frases_papers_teste.values()), 2 , model_bi)\n",
    "media_bigramas_perp_teste = media_perplexidade(bigramas_perp_teste)\n",
    "\n",
    "bigramas_perp_teste_s_inf       = lista_sem_infinito(bigramas_perp_teste)\n",
    "media_bigramas_perp_teste_s_inf = media_lista_sem_infinito(bigramas_perp_teste_s_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "30886d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:38:16.605413Z",
     "start_time": "2023-05-01T16:38:16.599061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: inf\n",
      "média sem infinito: 104.49078017502863\n"
     ]
    }
   ],
   "source": [
    "print('média:',media_bigramas_perp_teste)\n",
    "print('média sem infinito:', media_bigramas_perp_teste_s_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ed98c",
   "metadata": {},
   "source": [
    "#### Rouge Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "91870b71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:38:16.651494Z",
     "start_time": "2023-05-01T16:38:16.608850Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_bigramas_teste    = selecao_frases_obj_mle(probabilidades_bigramas, frases_papers_teste2)\n",
    "lista_frase_teste_bigramas,lista_prob_teste_bigramas,lista_geral_teste_bigramas = filtro_selecao_frases_mle(frase_objetivo_bigramas_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "d8514fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:38:16.962828Z",
     "start_time": "2023-05-01T16:38:16.656495Z"
    }
   },
   "outputs": [],
   "source": [
    "p_bi_teste,r_bi_teste,f_bi_teste  = calculo_rouge(lista_frase_teste_bigramas,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "3b7b4c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:05:30.467599Z",
     "start_time": "2023-05-01T21:05:30.448599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrama teste sem laplace\n",
      "precisão: 0.042\n",
      "revocação: 0.013\n",
      "f-score: 0.015\n",
      "precisao acima de 80%: 3.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 95.0\n"
     ]
    }
   ],
   "source": [
    "print('bigrama teste sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_bi_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi_teste))))\n",
    "funcao_80_e_0(p_bi_teste,r_bi_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a40ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:42:04.427672Z",
     "start_time": "2023-05-01T04:42:04.380258Z"
    }
   },
   "source": [
    "rouge_bigramas                         = calculo_rouge(lista_frase_teste_bigramas,list(padrao_ouro_teste_frases.values()))\n",
    "maximo_rouge_bigramas                  = max_rouge(rouge_bigramas) #lista de índices e maior rouge\n",
    "precisao_bigramas, revocacao_bigramas = rouge_precisao_revocacao(maximo_rouge_bigramas, lista_padrao_ouro_teste)\n",
    "analise_revocacao(revocacao_bigramas, lista_padrao_ouro_teste, media_ouro_teste)\n",
    "print('----')\n",
    "analise_precisao(precisao_bigramas, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e24a06",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "9e47a879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:38:17.194719Z",
     "start_time": "2023-05-01T16:38:17.001736Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_bi_laplace'):\n",
    "    model_bi_laplace = joblib.load('model_bi_laplace')\n",
    "else:\n",
    "    n = 2\n",
    "    paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "    model_bi_laplace = Laplace(n)\n",
    "    model_bi_laplace.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23878aa",
   "metadata": {},
   "source": [
    "#### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "712cc110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:39:22.988533Z",
     "start_time": "2023-05-01T16:38:17.201660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: 1835.620010581891\n",
      "média sem infinito: 1835.620010581891\n"
     ]
    }
   ],
   "source": [
    "bigramas_treino          = gerador_ngramas(token_frases_treino,2)\n",
    "probabilidades_bigramas_laplace  = gerador_probabilidades(bigramas_treino, model_bi_laplace)\n",
    "\n",
    "bigramas_perp_treino       = funcao_perplexidade(list(frases_papers_treino.values()), 2 , model_bi_laplace)\n",
    "media_bigramas_perp_treino = media_perplexidade(bigramas_perp_treino)\n",
    "\n",
    "bigramas_perp_treino_s_inf       = lista_sem_infinito(bigramas_perp_treino)\n",
    "media_bigramas_perp_treino_s_inf = media_lista_sem_infinito(bigramas_perp_treino_s_inf)\n",
    "\n",
    "\n",
    "print('média:',media_bigramas_perp_treino)\n",
    "print('média sem infinito:', media_bigramas_perp_treino_s_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e3703871",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:39:23.037819Z",
     "start_time": "2023-05-01T16:39:22.991475Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_bigramas_treino    = selecao_frases_obj_mle(probabilidades_bigramas_laplace, frases_papers_treino2)\n",
    "lista_frase_bigramas,lista_prob_bigramas,lista_geral_bigramas = filtro_selecao_frases_mle(frase_objetivo_bigramas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "4a2524f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:39:46.000155Z",
     "start_time": "2023-05-01T16:39:23.040381Z"
    }
   },
   "outputs": [],
   "source": [
    "p_bi_treino_laplace,r_bi_treino_laplace,f_bi_treino_laplace  = calculo_rouge(lista_frase_bigramas,list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "2469a447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:05:40.012839Z",
     "start_time": "2023-05-01T21:05:39.924866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrama treino com laplace\n",
      "precisão: 0.323\n",
      "revocação: 0.134\n",
      "f-score: 0.161\n",
      "precisao acima de 80%: 2.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 15.0\n"
     ]
    }
   ],
   "source": [
    "print('bigrama treino com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_bi_treino_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi_treino_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi_treino_laplace))))\n",
    "funcao_80_e_0(p_bi_treino_laplace,r_bi_treino_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e73a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:42:48.322174Z",
     "start_time": "2023-05-01T04:42:43.722725Z"
    }
   },
   "source": [
    "rouge_bigramas                         = calculo_rouge(lista_frase_bigramas,list(padrao_ouro_treino_frases.values()))\n",
    "maximo_rouge_bigramas                  = max_rouge(rouge_bigramas) #lista de índices e maior rouge\n",
    "precisao_bigramas, revocacao_bigramas = rouge_precisao_revocacao(maximo_rouge_bigramas, lista_padrao_ouro_treino)\n",
    "analise_revocacao(revocacao_bigramas, lista_padrao_ouro_treino, media_ouro)\n",
    "print('----')\n",
    "analise_precisao(precisao_bigramas, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91d439",
   "metadata": {},
   "source": [
    "#### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "601968bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:40:09.831729Z",
     "start_time": "2023-05-01T16:39:46.042541Z"
    }
   },
   "outputs": [],
   "source": [
    "bigramas_teste          = gerador_ngramas(token_frases_teste,2)\n",
    "probabilidades_bigramas_laplace  = gerador_probabilidades(bigramas_teste, model_bi_laplace)\n",
    "\n",
    "bigramas_perp_teste       = funcao_perplexidade(list(frases_papers_teste.values()), 2 , model_bi_laplace)\n",
    "media_bigramas_perp_teste = media_perplexidade(bigramas_perp_teste)\n",
    "\n",
    "bigramas_perp_teste_s_inf       = lista_sem_infinito(bigramas_perp_teste)\n",
    "media_bigramas_perp_teste_s_inf = media_lista_sem_infinito(bigramas_perp_teste_s_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4a36c3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:40:09.850465Z",
     "start_time": "2023-05-01T16:40:09.834412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "media_bigramas_perp_teste 1851.2007455554535\n",
      "media_bigramas_perp_teste_s_inf 1851.2007455554535\n"
     ]
    }
   ],
   "source": [
    "print('media_bigramas_perp_teste',media_bigramas_perp_teste)\n",
    "print('media_bigramas_perp_teste_s_inf',media_bigramas_perp_teste_s_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ad2549cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:40:09.875970Z",
     "start_time": "2023-05-01T16:40:09.852393Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_bigramas_teste    = selecao_frases_obj_mle(probabilidades_bigramas_laplace, frases_papers_teste2)\n",
    "lista_frase_teste_bigramas,lista_prob_teste_bigramas,lista_geral_teste_bigramas = filtro_selecao_frases_mle(frase_objetivo_bigramas_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "bc4e5c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:40:18.807804Z",
     "start_time": "2023-05-01T16:40:09.877336Z"
    }
   },
   "outputs": [],
   "source": [
    "p_bi_teste_laplace,r_bi_teste_laplace,f_bi_teste_laplace  = calculo_rouge(lista_frase_teste_bigramas,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "3b1ee645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:05:47.706085Z",
     "start_time": "2023-05-01T21:05:47.671310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrama teste sem laplace\n",
      "precisão: 0.325\n",
      "revocação: 0.153\n",
      "f-score: 0.179\n",
      "precisao acima de 80%: 2.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 12.0\n"
     ]
    }
   ],
   "source": [
    "print('bigrama teste sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_bi_teste_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi_teste_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi_teste_laplace))))\n",
    "funcao_80_e_0(p_bi_teste_laplace,r_bi_teste_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819276a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:43:04.473461Z",
     "start_time": "2023-05-01T04:43:02.294062Z"
    }
   },
   "source": [
    "rouge_bigramas                         = calculo_rouge(lista_frase_teste_bigramas,list(padrao_ouro_teste_frases.values()))\n",
    "maximo_rouge_bigramas                  = max_rouge(rouge_bigramas) #lista de índices e maior rouge\n",
    "precisao_bigramas, revocacao_bigramas = rouge_precisao_revocacao(maximo_rouge_bigramas, lista_padrao_ouro_teste)\n",
    "analise_revocacao(revocacao_bigramas, lista_padrao_ouro_teste, media_ouro_teste)\n",
    "print('----')\n",
    "analise_precisao(precisao_bigramas, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "f86e328a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:40:18.851019Z",
     "start_time": "2023-05-01T16:40:18.835706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1851.2007455554535"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_bigramas_perp_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c08d0d",
   "metadata": {},
   "source": [
    "## Trigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "54ec7cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:40:25.963917Z",
     "start_time": "2023-05-01T16:40:18.854023Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_tri'):\n",
    "    model_tri = joblib.load('model_tri')\n",
    "else:\n",
    "    n = 3\n",
    "    paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "    model_tri = MLE(n)\n",
    "    model_tri.fit(train_data, padded_sents)\n",
    "\n",
    "    print(model_tri.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a59e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T14:17:34.571574Z",
     "start_time": "2023-04-02T14:17:32.850030Z"
    }
   },
   "source": [
    "n = 3\n",
    "paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "model_tri = MLE(n)\n",
    "model_tri.fit(train_data, padded_sents)\n",
    "\n",
    "print(model_tri.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1863a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T15:27:36.242876Z",
     "start_time": "2023-04-02T15:27:19.335533Z"
    }
   },
   "source": [
    "joblib.dump(model_tri,'model_tri')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bdb79",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fee4ba",
   "metadata": {},
   "source": [
    "#### Perplexidade Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "2576d7de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:41:43.081243Z",
     "start_time": "2023-05-01T16:40:25.966887Z"
    }
   },
   "outputs": [],
   "source": [
    "trigramas_treino         = gerador_ngramas(token_frases_treino,3)\n",
    "probabilidades_trigramas = gerador_probabilidades(trigramas_treino, model_tri)\n",
    "\n",
    "trigramas_perp_treino       = funcao_perplexidade(list(frases_papers_treino.values()), 3 , model_tri)\n",
    "media_trigramas_perp_treino = media_perplexidade(trigramas_perp_treino)\n",
    "\n",
    "trigramas_perp_treino_s_inf       = lista_sem_infinito(trigramas_perp_treino)\n",
    "media_trigramas_perp_treino_s_inf = media_lista_sem_infinito(trigramas_perp_treino_s_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "13932dc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:41:43.093510Z",
     "start_time": "2023-05-01T16:41:43.081243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: inf\n",
      "média sem infinito: 2.3478837827173082\n"
     ]
    }
   ],
   "source": [
    "print('média:',media_trigramas_perp_treino)\n",
    "print('média sem infinito:', media_trigramas_perp_treino_s_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c41cf",
   "metadata": {},
   "source": [
    "#### Rouge Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "dea76888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:41:43.161648Z",
     "start_time": "2023-05-01T16:41:43.093510Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_trigramas_treino    = selecao_frases_obj_mle(probabilidades_trigramas, frases_papers_treino2)\n",
    "lista_frase,lista_prob,lista_geral = filtro_selecao_frases_mle(frase_objetivo_trigramas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "a877b7e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:42:07.048426Z",
     "start_time": "2023-05-01T16:41:43.164640Z"
    }
   },
   "outputs": [],
   "source": [
    "p_tri_treino,r_tri_treino,f_tri_treino    = calculo_rouge(lista_frase,list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "92e5f063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:05:59.101213Z",
     "start_time": "2023-05-01T21:05:59.056934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigrama treino sem laplace\n",
      "precisão: 0.832\n",
      "revocação: 0.854\n",
      "f-score: 0.838\n",
      "precisao acima de 80%: 79.0\n",
      "recall acima de 80%: 86.0\n",
      "igual a 0: 11.0\n"
     ]
    }
   ],
   "source": [
    "print('trigrama treino sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_tri_treino))))\n",
    "print('revocação:',media2(media(max_rouge(r_tri_treino))))\n",
    "print('f-score:',media2(media(max_rouge(f_tri_treino))))\n",
    "funcao_80_e_0(p_tri_treino,r_tri_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9239c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:43:58.526660Z",
     "start_time": "2023-05-01T04:43:53.680646Z"
    }
   },
   "source": [
    "rouge_trigramas                         = calculo_rouge(lista_frase,list(padrao_ouro_treino_frases.values()))\n",
    "maximo_rouge_trigramas                  = max_rouge(rouge_trigramas) #lista de índices e maior rouge\n",
    "precisao_trigramas, revocacao_trigramas = rouge_precisao_revocacao(maximo_rouge_trigramas, lista_padrao_ouro_treino)\n",
    "analise_revocacao(revocacao_trigramas, lista_padrao_ouro_treino, media_ouro)\n",
    "print('----')\n",
    "analise_precisao(precisao_trigramas, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7af490",
   "metadata": {},
   "source": [
    "#### Perplexidade Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "656bc830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:42:34.161032Z",
     "start_time": "2023-05-01T16:42:07.072762Z"
    }
   },
   "outputs": [],
   "source": [
    "trigramas_teste          = gerador_ngramas(token_frases_teste,3)\n",
    "probabilidades_trigramas = gerador_probabilidades(trigramas_teste, model_tri)\n",
    "\n",
    "trigramas_perp_teste       = funcao_perplexidade(list(frases_papers_teste.values()), 3 , model_tri)\n",
    "media_trigramas_perp_teste = media_perplexidade(trigramas_perp_teste)\n",
    "\n",
    "trigramas_perp_teste_s_inf       = lista_sem_infinito(trigramas_perp_teste)\n",
    "media_trigramas_perp_teste_s_inf = media_lista_sem_infinito(trigramas_perp_teste_s_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "4caf1023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:42:34.176026Z",
     "start_time": "2023-05-01T16:42:34.163016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "média: inf\n",
      "média sem infinito: 8.015579431135096\n"
     ]
    }
   ],
   "source": [
    "print('média:',media_trigramas_perp_teste)\n",
    "print('média sem infinito:', media_trigramas_perp_teste_s_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa25e11e",
   "metadata": {},
   "source": [
    "#### Rouge Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "162dad19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:42:40.797912Z",
     "start_time": "2023-05-01T16:42:40.769681Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_trigramas_teste    = selecao_frases_obj_mle(probabilidades_trigramas, frases_papers_teste2)\n",
    "lista_frase_teste,lista_prob_teste,lista_geral_teste = filtro_selecao_frases_mle(frase_objetivo_trigramas_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "467773df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:42:41.238266Z",
     "start_time": "2023-05-01T16:42:41.208263Z"
    }
   },
   "outputs": [],
   "source": [
    "p_tri_teste,r_tri_teste,f_tri_teste  = calculo_rouge(lista_frase_teste,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "1730aa11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:06:05.805836Z",
     "start_time": "2023-05-01T21:06:05.794835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigrama teste sem laplace\n",
      "precisão: 0.0\n",
      "revocação: 0.0\n",
      "f-score: 0.0\n",
      "precisao acima de 80%: 0.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 100.0\n"
     ]
    }
   ],
   "source": [
    "print('trigrama teste sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_tri_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_tri_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_tri_teste))))\n",
    "funcao_80_e_0(p_tri_teste,r_tri_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac5802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:44:14.512355Z",
     "start_time": "2023-05-01T04:44:14.480435Z"
    }
   },
   "source": [
    "frase_objetivo_trigramas_teste    = selecao_frases_obj_mle(probabilidades_trigramas, frases_papers_teste2)\n",
    "lista_frase_teste,lista_prob_teste,lista_geral_teste = filtro_selecao_frases_mle(frase_objetivo_trigramas_teste)\n",
    "\n",
    "rouge_trigramas                         = calculo_rouge(lista_frase_teste,list(padrao_ouro_teste_frases.values()))\n",
    "maximo_rouge_trigramas                  = max_rouge(rouge_trigramas) #lista de índices e maior rouge\n",
    "precisao_trigramas, revocacao_trigramas = rouge_precisao_revocacao(maximo_rouge_trigramas, lista_padrao_ouro_teste)\n",
    "analise_revocacao(revocacao_trigramas, lista_padrao_ouro_teste, media_ouro)\n",
    "print('----')\n",
    "analise_precisao(precisao_trigramas, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cd691",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "adf1cf96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:45:00.979903Z",
     "start_time": "2023-05-01T16:44:54.359121Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_tri_laplace'):\n",
    "    model_tri_laplace = joblib.load('model_tri_laplace')\n",
    "else:\n",
    "    n = 3\n",
    "    paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "    model_tri_laplace = Laplace(n)\n",
    "    model_tri_laplace.fit(train_data, padded_sents)\n",
    "\n",
    "    print(model_tri_laplace.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244459a",
   "metadata": {},
   "source": [
    "#### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "007b6b0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:46:18.867516Z",
     "start_time": "2023-05-01T16:45:00.981900Z"
    }
   },
   "outputs": [],
   "source": [
    "trigramas_treino         = gerador_ngramas(token_frases_treino,3)\n",
    "probabilidades_trigramas = gerador_probabilidades(trigramas_treino, model_tri_laplace)\n",
    "\n",
    "trigramas_perp_treino       = funcao_perplexidade(list(frases_papers_treino.values()), 3 , model_tri_laplace)\n",
    "media_trigramas_perp_treino = media_perplexidade(trigramas_perp_treino)\n",
    "\n",
    "trigramas_perp_treino_s_inf       = lista_sem_infinito(trigramas_perp_treino)\n",
    "media_trigramas_perp_treino_s_inf = media_lista_sem_infinito(trigramas_perp_treino_s_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ae581ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:46:18.888948Z",
     "start_time": "2023-05-01T16:46:18.867516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2593.6182312686974\n"
     ]
    }
   ],
   "source": [
    "print(media_trigramas_perp_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "8daf1022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:46:18.934647Z",
     "start_time": "2023-05-01T16:46:18.891116Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_trigramas_treino    = selecao_frases_obj_mle(probabilidades_trigramas, frases_papers_treino2)\n",
    "lista_frase,lista_prob,lista_geral = filtro_selecao_frases_mle(frase_objetivo_trigramas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "070e3cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:46:41.660810Z",
     "start_time": "2023-05-01T16:46:18.936627Z"
    }
   },
   "outputs": [],
   "source": [
    "p_tri_treino_laplace,r_tri_treino_laplace,f_tri_treino_laplace  = calculo_rouge(lista_frase,list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "d292f991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:06:15.348989Z",
     "start_time": "2023-05-01T21:06:15.272102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigrama treino com laplace\n",
      "precisão: 0.32\n",
      "revocação: 0.129\n",
      "f-score: 0.155\n",
      "precisao acima de 80%: 3.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 14.0\n"
     ]
    }
   ],
   "source": [
    "print('trigrama treino com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_tri_treino_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_tri_treino_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_tri_treino_laplace))))\n",
    "funcao_80_e_0(p_tri_treino_laplace,r_tri_treino_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e35db2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:45:13.431419Z",
     "start_time": "2023-05-01T04:45:08.857185Z"
    }
   },
   "source": [
    "rouge_trigramas                         = calculo_rouge(lista_frase,list(padrao_ouro_treino_frases.values()))\n",
    "maximo_rouge_trigramas                  = max_rouge(rouge_trigramas) #lista de índices e maior rouge\n",
    "precisao_trigramas, revocacao_trigramas = rouge_precisao_revocacao(maximo_rouge_trigramas, lista_padrao_ouro_treino)\n",
    "analise_revocacao(revocacao_trigramas, lista_padrao_ouro_treino, media_ouro)\n",
    "print('----')\n",
    "analise_precisao(precisao_trigramas, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db8b15",
   "metadata": {},
   "source": [
    "#### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "734be338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:10.119047Z",
     "start_time": "2023-05-01T16:46:41.698555Z"
    }
   },
   "outputs": [],
   "source": [
    "trigramas_teste          = gerador_ngramas(token_frases_teste,3)\n",
    "probabilidades_trigramas = gerador_probabilidades(trigramas_teste, model_tri_laplace)\n",
    "\n",
    "trigramas_perp_teste       = funcao_perplexidade(list(frases_papers_teste.values()), 3 , model_tri_laplace)\n",
    "media_trigramas_perp_teste = media_perplexidade(trigramas_perp_teste)\n",
    "\n",
    "trigramas_perp_teste_s_inf       = lista_sem_infinito(trigramas_perp_teste)\n",
    "media_trigramas_perp_teste_s_inf = media_lista_sem_infinito(trigramas_perp_teste_s_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "e47b73a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:10.134708Z",
     "start_time": "2023-05-01T16:47:10.119047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2615.8237276236473\n"
     ]
    }
   ],
   "source": [
    "print(media_trigramas_perp_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "18c12317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:10.153555Z",
     "start_time": "2023-05-01T16:47:10.136708Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_trigramas_teste    = selecao_frases_obj_mle(probabilidades_trigramas, frases_papers_teste2)\n",
    "lista_frase_teste,lista_prob_teste,lista_geral_teste = filtro_selecao_frases_mle(frase_objetivo_trigramas_teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ea6c14db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:19.204035Z",
     "start_time": "2023-05-01T16:47:10.156041Z"
    }
   },
   "outputs": [],
   "source": [
    "p_tri_teste_laplace,r_tri_teste_laplace,f_tri_teste_laplace  = calculo_rouge(lista_frase_teste,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "4782f9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:06:22.622034Z",
     "start_time": "2023-05-01T21:06:22.586951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigrama teste com laplace\n",
      "precisão: 0.33\n",
      "revocação: 0.153\n",
      "f-score: 0.18\n",
      "precisao acima de 80%: 2.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 11.0\n"
     ]
    }
   ],
   "source": [
    "print('trigrama teste com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_tri_teste_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_tri_teste_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_tri_teste_laplace))))\n",
    "funcao_80_e_0(p_tri_teste_laplace,r_tri_teste_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be597e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T04:45:32.546070Z",
     "start_time": "2023-05-01T04:45:30.684171Z"
    }
   },
   "source": [
    "rouge_trigramas                         = calculo_rouge(lista_frase_teste,list(padrao_ouro_teste_frases.values()))\n",
    "maximo_rouge_trigramas                  = max_rouge(rouge_trigramas) #lista de índices e maior rouge\n",
    "precisao_trigramas, revocacao_trigramas = rouge_precisao_revocacao(maximo_rouge_trigramas, lista_padrao_ouro_teste)\n",
    "analise_revocacao(revocacao_trigramas, lista_padrao_ouro_teste, media_ouro)\n",
    "print('----')\n",
    "analise_precisao(precisao_trigramas, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "b95ceaf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:19.257712Z",
     "start_time": "2023-05-01T16:47:19.235957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2615.8237276236473"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_trigramas_perp_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8a28c",
   "metadata": {},
   "source": [
    "## rouge geral\n",
    "\n",
    "Exemplos de frases e suas probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "2b99a0ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:07:00.590473Z",
     "start_time": "2023-05-01T21:07:00.242737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREINO SEM LAPLACE\n",
      "unigrama treino sem laplace\n",
      "precisão: 0.447\n",
      "revocação: 0.223\n",
      "f-score: 0.254\n",
      "precisao acima de 80%: 5.0\n",
      "recall acima de 80%: 1.0\n",
      "igual a 0: 4.0\n",
      "-----\n",
      "bigrama treino sem laplace\n",
      "precisão: 0.827\n",
      "revocação: 0.817\n",
      "f-score: 0.805\n",
      "precisao acima de 80%: 76.0\n",
      "recall acima de 80%: 77.0\n",
      "igual a 0: 10.0\n",
      "-----\n",
      "trigrama treino sem laplace\n",
      "precisão: 0.832\n",
      "revocação: 0.854\n",
      "f-score: 0.838\n",
      "precisao acima de 80%: 79.0\n",
      "recall acima de 80%: 86.0\n",
      "igual a 0: 11.0\n"
     ]
    }
   ],
   "source": [
    "print('TREINO SEM LAPLACE')\n",
    "print('unigrama treino sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_uni_treino))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni_treino))))\n",
    "print('f-score:',media2(media(max_rouge(f_uni_treino))))\n",
    "funcao_80_e_0(p_uni_treino,r_uni_treino)\n",
    "\n",
    "print('-----')\n",
    "print('bigrama treino sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_bi_treino))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi_treino))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi_treino))))\n",
    "funcao_80_e_0(p_bi_treino,r_bi_treino)\n",
    "\n",
    "print('-----')\n",
    "print('trigrama treino sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_tri_treino))))\n",
    "print('revocação:',media2(media(max_rouge(r_tri_treino))))\n",
    "print('f-score:',media2(media(max_rouge(f_tri_treino))))\n",
    "funcao_80_e_0(p_tri_treino,r_tri_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "dcfd0cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:09:32.154751Z",
     "start_time": "2023-05-01T21:09:32.096693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTE SEM LAPLACE\n",
      "unigrama teste sem laplace\n",
      "precisão: 0.419\n",
      "revocação: 0.23\n",
      "f-score: 0.258\n",
      "precisao acima de 80%: 3.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 4.0\n",
      "-----\n",
      "bigrama teste sem laplace\n",
      "precisão: 0.042\n",
      "revocação: 0.013\n",
      "f-score: 0.015\n",
      "precisao acima de 80%: 3.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 95.0\n",
      "-----\n",
      "trigrama teste sem laplace\n",
      "precisão: 0.0\n",
      "revocação: 0.0\n",
      "f-score: 0.0\n",
      "precisao acima de 80%: 0.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 100.0\n"
     ]
    }
   ],
   "source": [
    "print('TESTE SEM LAPLACE')\n",
    "print('unigrama teste sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_uni_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_uni_teste))))\n",
    "funcao_80_e_0(p_uni_teste,r_uni_teste)\n",
    "\n",
    "print('-----')\n",
    "print('bigrama teste sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_bi_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi_teste))))\n",
    "funcao_80_e_0(p_bi_teste,r_bi_teste)\n",
    "\n",
    "print('-----')\n",
    "print('trigrama teste sem laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_tri_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_tri_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_tri_teste))))\n",
    "funcao_80_e_0(p_tri_teste,r_tri_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "8645d07b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:10:05.900638Z",
     "start_time": "2023-05-01T21:10:05.636155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREINO COM LAPLACE\n",
      "unigrama treino com laplace\n",
      "precisão: 0.347\n",
      "revocação: 0.138\n",
      "f-score: 0.167\n",
      "precisao acima de 80%: 3.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 11.0\n",
      "-----\n",
      "bigrama treino com laplace\n",
      "precisão: 0.323\n",
      "revocação: 0.134\n",
      "f-score: 0.161\n",
      "precisao acima de 80%: 2.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 15.0\n",
      "-----\n",
      "trigrama treino com laplace\n",
      "precisão: 0.32\n",
      "revocação: 0.129\n",
      "f-score: 0.155\n",
      "precisao acima de 80%: 3.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 14.0\n"
     ]
    }
   ],
   "source": [
    "print('TREINO COM LAPLACE')\n",
    "print('unigrama treino com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_uni_treino_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni_treino_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_uni_treino_laplace))))\n",
    "funcao_80_e_0(p_uni_treino_laplace,r_uni_treino_laplace)\n",
    "print('-----')\n",
    "print('bigrama treino com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_bi_treino_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi_treino_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi_treino_laplace))))\n",
    "funcao_80_e_0(p_bi_treino_laplace,r_bi_treino_laplace)\n",
    "print('-----')\n",
    "print('trigrama treino com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_tri_treino_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_tri_treino_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_tri_treino_laplace))))\n",
    "funcao_80_e_0(p_tri_treino_laplace,r_tri_treino_laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "f73a156c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:10:06.094260Z",
     "start_time": "2023-05-01T21:10:05.976973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTE COM LAPLACE\n",
      "unigrama teste com laplace\n",
      "precisão: 0.351\n",
      "revocação: 0.164\n",
      "f-score: 0.191\n",
      "precisao acima de 80%: 2.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 9.0\n",
      "-----\n",
      "bigrama teste com laplace\n",
      "precisão: 0.325\n",
      "revocação: 0.153\n",
      "f-score: 0.179\n",
      "precisao acima de 80%: 2.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 12.0\n",
      "-----\n",
      "trigrama teste com laplace\n",
      "precisão: 0.33\n",
      "revocação: 0.153\n",
      "f-score: 0.18\n",
      "precisao acima de 80%: 2.0\n",
      "recall acima de 80%: 0.0\n",
      "igual a 0: 11.0\n"
     ]
    }
   ],
   "source": [
    "print('TESTE COM LAPLACE')\n",
    "print('unigrama teste com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_uni_teste_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni_teste_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_uni_teste_laplace))))\n",
    "funcao_80_e_0(p_uni_teste_laplace,r_uni_teste_laplace)\n",
    "print('-----')\n",
    "print('bigrama teste com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_bi_teste_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi_teste_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi_teste_laplace))))\n",
    "funcao_80_e_0(p_bi_teste_laplace,r_bi_teste_laplace)\n",
    "print('-----')\n",
    "print('trigrama teste com laplace')\n",
    "print('precisão:',media2(media(max_rouge(p_tri_teste_laplace))))\n",
    "print('revocação:',media2(media(max_rouge(r_tri_teste_laplace))))\n",
    "print('f-score:',media2(media(max_rouge(f_tri_teste_laplace))))\n",
    "funcao_80_e_0(p_tri_teste_laplace,r_tri_teste_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae621e",
   "metadata": {},
   "source": [
    "# Experimento 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe48b1",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "Aplicando o método da similaridade nos 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030880c",
   "metadata": {},
   "source": [
    "### Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "8c4f17e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:19.632560Z",
     "start_time": "2023-05-01T16:47:19.615357Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_ft_teste(sessao_palavras, modelo_embedding):\n",
    "    \"\"\"input: dicionário de sessão das palavras\n",
    "        output: dicionário de vetores\"\"\"\n",
    "    dicio={}\n",
    "    for k,v in sessao_palavras.items():\n",
    "        x=[]\n",
    "        for lista_frase in v:\n",
    "            soma=0\n",
    "            for palavra in lista_frase:\n",
    "                try:\n",
    "                    soma = soma + modelo_embedding.get_word_vector(palavra)\n",
    "                except:\n",
    "                    soma=soma\n",
    "            x.append(soma)\n",
    "        dicio[k] = x\n",
    "    return dicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "9be1f614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:19.647708Z",
     "start_time": "2023-05-01T16:47:19.633373Z"
    }
   },
   "outputs": [],
   "source": [
    "modelo_selecionado = model_ft_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "8692339f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:26.968581Z",
     "start_time": "2023-05-01T16:47:19.649803Z"
    }
   },
   "outputs": [],
   "source": [
    "abstract_teste        = {k: preprocessa_abstract(v) for k, v in papers_teste2.items()}\n",
    "abstract_frases_teste = {k: separa_frases(j) for k, v in abstract_teste.items() for j in v}\n",
    "abstract_frases_teste = segundo_preprocessamento(abstract_frases_teste)\n",
    "abstract_palavras_teste = {k:[word_tokenize(f) for f in v] for k,v in abstract_frases_teste.items()}\n",
    "\n",
    "introducao_teste = {k: preprocessa_intro(v) for k, v in papers_teste2.items()}\n",
    "introducao_frases_teste = {k: separa_frases(j) for k, v in introducao_teste.items() for j in v}\n",
    "introducao_frases_teste = segundo_preprocessamento(introducao_frases_teste)\n",
    "introducao_palavras_teste = {k:[word_tokenize(f) for f in v] for k,v in introducao_frases_teste.items()}\n",
    "\n",
    "conclusao_teste = {k: preprocessa_conclusion(v) for k, v in papers_teste2.items()}\n",
    "conclusao_frases_teste = {k: separa_frases(j) for k, v in conclusao_teste.items() for j in v}\n",
    "conclusao_frases_teste = segundo_preprocessamento(conclusao_frases_teste)\n",
    "conclusao_palavras_teste = {k:[word_tokenize(f) for f in v] for k,v in conclusao_frases_teste.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c01012",
   "metadata": {},
   "source": [
    "### ROUGE-N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888fbe6",
   "metadata": {},
   "source": [
    "#### model_ft_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "2b927061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:26.980741Z",
     "start_time": "2023-05-01T16:47:26.968581Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frases_objetivo_teste(indices_objetivo, idx_paper):\n",
    "    \"\"\"retorna a frase objetivo [do abstract] a partir dos indices\"\"\"\n",
    "    frase_objetivo={}\n",
    "    for idx,idx_f,idx_ind in zip(idx_paper,range(len(list(abstract_frases_teste.values()))),indices_objetivo):\n",
    "        lista_one=[]\n",
    "        for j in idx_ind:\n",
    "            lista_one.append(list(abstract_frases_teste.values())[idx_f][j])\n",
    "        frase_objetivo[idx] = lista_one\n",
    "    return frase_objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "87132ac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:44.355104Z",
     "start_time": "2023-05-01T16:47:26.983280Z"
    }
   },
   "outputs": [],
   "source": [
    "v_abstract_teste    = vetor_frases_ft(abstract_palavras_teste,model_ft_cbow)\n",
    "v_introducao_teste  = vetor_frases_ft(introducao_palavras_teste,model_ft_cbow)\n",
    "v_conclusao_teste   = vetor_frases_ft(conclusao_palavras_teste,model_ft_cbow)\n",
    "\n",
    "#Abtract e Introdução\n",
    "sim_ab_intro_teste,indices_ab_intro_teste,lista_idx_ab_intro_teste =aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_introducao_teste.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_teste,indices_ab_conc_teste,lista_idx_ab_c_teste = aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_conclusao_teste.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_teste = frases_interseccao(lista_idx_ab_intro_teste,lista_idx_ab_c_teste)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_teste  = get_frases_objetivo_teste(indices_objetivo_teste, papers_teste.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "6dd8c4cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:47:53.503503Z",
     "start_time": "2023-05-01T16:47:44.357034Z"
    }
   },
   "outputs": [],
   "source": [
    "p_ft_cbow_teste, r_ft_cbow_teste, f_ft_cbow_teste  =calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cee96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ded5250b",
   "metadata": {},
   "source": [
    "#w2v\n",
    "rouge_teste = calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))\n",
    "\n",
    "maximo_rouge_por_paper_ft_teste   = max_rouge(rouge_teste)\n",
    "\n",
    "\n",
    "precisao_sg_ft_teste, revocacao_sg_ft_teste = rouge_precisao_revocacao(maximo_rouge_por_paper_ft_teste, lista_padrao_ouro_teste)\n",
    "qtd_precisao_ft_teste, qtd_zero_ft_teste, media_ft_teste = analise_precisao(precisao_sg_ft_teste, lista_padrao_ouro_teste)\n",
    "print('------')\n",
    "qtd_revocacao_ft,  media_revocacao_ft       = analise_revocacao(revocacao_sg_ft_teste, lista_padrao_ouro_teste, media_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb648ef",
   "metadata": {},
   "source": [
    "#### model_ft_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "dae86fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:48:15.056875Z",
     "start_time": "2023-05-01T16:47:53.504660Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_abstract_teste    = vetor_frases_ft(abstract_palavras_teste,model_ft_sg)\n",
    "v_introducao_teste  = vetor_frases_ft(introducao_palavras_teste,model_ft_sg)\n",
    "v_conclusao_teste   = vetor_frases_ft(conclusao_palavras_teste,model_ft_sg)\n",
    "\n",
    "#Abtract e Introdução\n",
    "sim_ab_intro_teste,indices_ab_intro_teste,lista_idx_ab_intro_teste =aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_introducao_teste.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_teste,indices_ab_conc_teste,lista_idx_ab_c_teste = aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_conclusao_teste.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_teste = frases_interseccao(lista_idx_ab_intro_teste,lista_idx_ab_c_teste)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_teste  = get_frases_objetivo_teste(indices_objetivo_teste, papers_teste.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "ec3b505d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:48:25.427383Z",
     "start_time": "2023-05-01T16:48:15.056875Z"
    }
   },
   "outputs": [],
   "source": [
    "p_ft_sg_teste, r_ft_sg_teste, f_ft_sg_teste  =calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94f2a0",
   "metadata": {},
   "source": [
    "\n",
    "#w2v\n",
    "rouge_teste = calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))\n",
    "\n",
    "maximo_rouge_por_paper_ft_teste   = max_rouge(rouge_teste)\n",
    "\n",
    "\n",
    "precisao_sg_ft_teste, revocacao_sg_ft_teste = rouge_precisao_revocacao(maximo_rouge_por_paper_ft_teste, lista_padrao_ouro_teste)\n",
    "qtd_precisao_ft_teste, qtd_zero_ft_teste, media_ft_teste = analise_precisao(precisao_sg_ft_teste, lista_padrao_ouro_teste)\n",
    "print('------')\n",
    "qtd_revocacao_ft,  media_revocacao_ft       = analise_revocacao(revocacao_sg_ft_teste, lista_padrao_ouro_teste, media_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f4857",
   "metadata": {},
   "source": [
    "#### model_w2v_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "856fc94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:48:32.276105Z",
     "start_time": "2023-05-01T16:48:25.427383Z"
    }
   },
   "outputs": [],
   "source": [
    "v_abstract_teste    = vetor_frases_w2v(abstract_palavras_teste, model_w2v_sg)\n",
    "v_introducao_teste  = vetor_frases_w2v(introducao_palavras_teste, model_w2v_sg)\n",
    "v_conclusao_teste   = vetor_frases_w2v(conclusao_palavras_teste, model_w2v_sg)\n",
    "\n",
    "#Abtract e Introdução\n",
    "sim_ab_intro_teste,indices_ab_intro_teste,lista_idx_ab_intro_teste =aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_introducao_teste.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_teste,indices_ab_conc_teste,lista_idx_ab_c_teste = aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_conclusao_teste.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_teste = frases_interseccao(lista_idx_ab_intro_teste,lista_idx_ab_c_teste)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_teste  = get_frases_objetivo_teste(indices_objetivo_teste, papers_teste.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "3d5359b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:48:41.659902Z",
     "start_time": "2023-05-01T16:48:32.277042Z"
    }
   },
   "outputs": [],
   "source": [
    "p_w2v_sg_teste, r_w2v_sg_teste, f_w2v_sg_teste  = calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf099e15",
   "metadata": {},
   "source": [
    "\n",
    "rouge_teste = calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))\n",
    "\n",
    "maximo_rouge_por_paper_ft_teste   = max_rouge(rouge_teste)\n",
    "\n",
    "\n",
    "precisao_sg_ft_teste, revocacao_sg_ft_teste = rouge_precisao_revocacao(maximo_rouge_por_paper_ft_teste, lista_padrao_ouro_teste)\n",
    "qtd_precisao_ft_teste, qtd_zero_ft_teste, media_ft_teste = analise_precisao(precisao_sg_ft_teste, lista_padrao_ouro_teste)\n",
    "print('------')\n",
    "qtd_revocacao_ft,  media_revocacao_ft       = analise_revocacao(revocacao_sg_ft_teste, lista_padrao_ouro_teste, media_ouro_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "95ec52b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:48:41.688914Z",
     "start_time": "2023-05-01T16:48:41.659902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{746: ['this paper presents an unsupervised learning algorithm for sense disambiguation that when trained on unannotated english text rivals the performance of supervised techniques that require timeconsuming hand annotations',\n",
       "  'the algorithm is based on two powerful constraints that words tend to have one sense per discourse and one sense per collocation exploited in an iterative bootstrapping procedure'],\n",
       " 747: ['to attack these problems we have built a hybrid generator in which gaps in symbolic knowledge are filled by statistical methods',\n",
       "  'we also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability even when perfect knowledge is in principle obtainable'],\n",
       " 748: ['this work is based on the following premises grammars are too complex and detailed to develop manually for most interesting domains parsing models must rely heavily on lexical and contextual information to analyze sentences accurately and existing ngrain modeling techniques are inadequate for parsing models'],\n",
       " 749: ['this study suggests that the identification of word translations should also be possible with nonparallel and even unrelated texts',\n",
       "  'the method proposed is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different languages'],\n",
       " 750: ['this approach integrates a diverse set of knowledge sources to disambiguate word sense including part of speech of neighboring words morphological form the unordered set of surrounding words local collocations and verbobject syntactic relation',\n",
       "  'lexas achieves a higher accuracy on the common data set and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of wordnet'],\n",
       " 752: ['this paper addresses the problem for a fairly general form of combinatory categorial grammar by means of an efficient correct and easy to implement normalform parsing tech the parser is proved to find exone in each semantic equivalence class of allowable parses that is spurious ambiguity as carefully defined is shown to be both safely and completely eliminated'],\n",
       " 754: [],\n",
       " 755: ['tests using wall street journal data show that the method performs at least as well as spatter magerman jelinek et al which has the best published results for a statistical parser on this task'],\n",
       " 757: ['we examine the effects of speaking style read versus spontaneous and of discourse segmentation method textalone versus textandspeech on the nature of this relationship',\n",
       "  'we also compare the acousticprosodic features of initial medial and final utterances in a discourse segment'],\n",
       " 758: ['we present an extensive empirical comparison of several smoothing techniques in the domain of language modeling including those described by jelinek and mercer katz and church and gale',\n",
       "  'in addition we introduce two novel smoothing techniques one a variation of jelinekmercer smoothing and one a very simple linear interpolation technique both of which outperform existing methods'],\n",
       " 759: ['we describe a family of methods for committeebased sample selection and report experimental results for the task of stochastic partofspeech tagging'],\n",
       " 760: ['we then extend the model to include a probabilistic treatment of both subcategorisation and whmovement'],\n",
       " 762: ['most previous corpusbased algorithms disambiguate a word with a classifier trained from previous usages of the same word',\n",
       "  'the algorithm does not require a sensetagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts'],\n",
       " 763: ['we derive the rhetorical structures of texts by means of two new surfaceformbased algorithms one that identifies discourse usages of cue phrases and breaks sentences into clauses and one that produces valid rhetorical structure trees for unrestricted natural language texts'],\n",
       " 764: ['translating such items from japanese back to english is even more challenging and of practical interest as transliterated items make up the bulk of text phrases not found in bilingual dictionaries'],\n",
       " 765: ['combining the constraints across many adjectives a clustering algorithm separates the adjectives into groups of different orientations and finally adjectives are labeled positive or negative'],\n",
       " 766: ['this paper presents paradise paradigm for dialogue system evaluation a general framework for evaluating spoken rlialogue agents',\n",
       "  'the framework decouples task requirements from an agents dialogue behaviors supports comparisons among dialogue strategies enables the calculation of performance over subdialogues and whole dialogues specifies the relative contribution of various factors to performance and makes it possible to compare agents performing different tasks by normalizing for task complexity'],\n",
       " 767: ['this paper presents a trainable rulebased algorithm for performing word segmentation',\n",
       "  'in addition we show the transformationbased algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages'],\n",
       " 768: ['many multilingual nlp applications need to translate words between different languages but cannot afford the computational expense of inducing or applying a full translation model',\n",
       "  'for these applications we have designed a fast algorithm for estimating a partial translation model which accounts for translational equivalence only at the word level'],\n",
       " 769: ['generalization is performed online at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus'],\n",
       " 770: ['crossdocument coreference occurs when the same person place event or concept is discussed in more than one text source',\n",
       "  'computer recognition of this phenomenon is important because it helps break the document boundary by allowing a user to examine information about a particular entity from multiple text sources at the same time'],\n",
       " 771: ['the projects key features are a a commitment to corpus evidence for semantic and syntactic generalizations and b the representation of the valences of its target words mostly nouns adjectives and verbs in which the semantic portion makes use of frame semantics',\n",
       "  'the resulting database will contain a descriptions of the semantic frames underlying the meanings of the words described and b the valence representation semantic and syntactic of several thousand words and phrases each accompanied by c a representative collection of annotated corpus attestations which jointly exemplify the observed linkings between frame elements and their syntactic realizations eg grammatical function phrase type and other syntactic traits'],\n",
       " 772: ['in this paper we first show that the errors made from three different state of the art part of speech taggers are strongly complementary',\n",
       "  'by using contextual cues to guide tagger combination we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers'],\n",
       " 773: ['the training phase of the algorithm is based on two successful techniques first the base np grammar is read from a treebank corpus then the grammar is improved by selecting rules with high benefit scores'],\n",
       " 774: ['the paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history thus enabling the use of long distance dependencies',\n",
       "  'the model its probabilistic parameterization and a set of experiments meant to evaluate its predictive power are presented an improvement over standard trigram modeling is achieved'],\n",
       " 777: ['in this paper we examine how the differences in modelling between different data driven systems performing the same nlp task can be exploited to yield a higher accuracy than the best individual system'],\n",
       " 780: ['we then present a new evaluation methodology for the automatically constructed thesaurus',\n",
       "  'the evaluation results show that the thesaurus is significantly closer to wordnet than roget thesaurus is'],\n",
       " 783: ['several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task',\n",
       "  'our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only partofspeech tags and morphological base forms as opposed to attachment information',\n",
       "  'it is therefore less resourceintensive and more portable than previous corpusbased algorithm proposed for this task'],\n",
       " 785: ['additionally the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand',\n",
       "  'our algorithm finds many terms not included within wordnet many more than previous algorithms and could be viewed as an enhancer of existing broadcoverage resources'],\n",
       " 786: ['i propose a model for determining the hearers attentional state which depends solely on a list of salient discourse entities slist'],\n",
       " 787: ['distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences'],\n",
       " 788: ['we present a method for extracting parts of objects from wholes eg',\n",
       "  'the part list could be scanned by an enduser and added to an existing ontology such as wordnet or used as a part of a rough semantic lexicon'],\n",
       " 789: ['we present a technique for automatic induction of slot annotations for subcategorization frames based on induction of hidden classes in the em framework of statistical estimation',\n",
       "  'induction of slot labeling for subcategorization frames is accomplished by a further application of em and applied experimentally on frame observations derived from parsing large corpora'],\n",
       " 790: ['this work goes a step further by automatically creating not just clusters of related words but a hierarchy of nouns and their hypernyms akin to the handbuilt hierarchy in wordnet'],\n",
       " 791: ['this paper presents a case study of analyzing and improving intercoder reliability in discourse using statistical techniques corrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier'],\n",
       " 792: ['noncompositional expressions present a special challenge to nlp applications',\n",
       "  'we present a method for automatic identification of noncompositional expressions using their statistical properties in a text corpus'],\n",
       " 793: ['paper describes initial work on read an automated reading comprehension system that accepts arbitrary text input a story and answers questions about it',\n",
       "  'we have acquired a corpus of and test stories of to grade material each story is followed by shortanswer questions an answer key was also provided',\n",
       "  'we used these to construct and evaluate a baseline system that uses pattern matching bagofwords techniques augmented with additional automated linguistic processing stemming name identification semantic class identification and pronoun resolution'],\n",
       " 794: ['we have developed a corpusbased algorithm for automatically identifying definite noun phrases that are nonanaphoric which has the potential to improve the efficiency and accuracy of coreference resolution systems'],\n",
       " 796: ['this paper considers statistical parsing of czech which differs radically from english in at least two it is a inflected and it has relatively word order differences are likely to pose new problems for techniques that have been developed on english',\n",
       "  'our final results dependency accuracy represent good progress towards the accuracy of the parser on english wall street journal text'],\n",
       " 797: [],\n",
       " 798: ['strand resnik is a languageindependent system for automatic discovery of text in parallel translation on the world wide web',\n",
       "  'this paper extends the preliminary strand results by adding automatic language identification scaling up by orders of magnitude and formally evaluating performance'],\n",
       " 799: ['we describe two computationallytractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses and apply these to estimate a stochastic version of lexical'],\n",
       " 800: ['we present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents'],\n",
       " 801: ['tempeval comprises evaluation tasks for time expressions events and temporal relations the latter of which was split up in four sub tasks motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier'],\n",
       " 802: ['this paper presents the description and evaluation framework of semeval word sense induction amp disambiguation task as well as the evaluation results of participating systems',\n",
       "  'in this task participants were required to induce the senses of target words using a training set and then disambiguate unseen instances of the same words using the induced senses'],\n",
       " 803: ['participants were presented with datasets for different language pairs where multidirectional entailment relations forward backward bidirectional no entailment had to be identified'],\n",
       " 804: ['we also describe two new techniques based on sentence utility and subsumption which we have applied to the evaluation of both single and multiple document summaries',\n",
       "  'finally we describe two user studies that test our models of multidocument summarization'],\n",
       " 805: ['we introduce a semanticbased algorithm for learning morphology which only proposes affixes when the stem and stemplusaffix are sufficiently similar semantically'],\n",
       " 806: ['this paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora',\n",
       "  'previous techniques give good results but fail to cope well with ambiguity or rare words'],\n",
       " 809: ['this paper presents the firstever results of applying statistical parsing models to the newlyavailable chinese treebank'],\n",
       " 811: ['this paper presents results for a maximumentropybased part of speech tagger which achieves superior performance principally by enriching the information sources used for tagging',\n",
       "  'in particular we get improved results by incorporating these features i more extensive treatment of capitalization for unknown words ii features for the disambiguation of the tense forms of verbs iii features for disambiguating particles from prepositions and adverbs'],\n",
       " 813: ['we present such a component a fast and robust morphological generator for english based on finitestate techniques that generates a word form given a specification of the lemma partofspeech and the type of inflection required',\n",
       "  'we describe how this morphological generator is used in a prototype system for automatic simplification of english newspaper text and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application'],\n",
       " 814: ['cotraining is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data'],\n",
       " 815: ['in this paper we describe a classification algorithm for identifying relationships between twoword noun compounds',\n",
       "  'we find that a very simple approach using a machine learning algorithm and a domainspecific lexical hierarchy successfully generalizes from training instances performing better on previously unseen words than a baseline consisting of training on the words themselves'],\n",
       " 816: ['we seek a knowledgefree method for inducing multiword units from text corpora for use as machinereadable dictionary headwords',\n",
       "  'we use latent semantic analysis to make modest gains in performance but we show the significant challenges encountered in trying this approach'],\n",
       " 817: ['test results show lsa is a more accurate similarity measure than the'],\n",
       " 818: [],\n",
       " 819: ['we describe a procedure for arranging into a timeline the contents of news stories describing the development of some situation',\n",
       "  'we describe the parts of the system that deal with breaking sentences into eventclauses and resolving both explicit and implicit temporal references'],\n",
       " 821: ['nltk the natural language toolkit is a suite of open source program modules tutorials and problem sets providing readytouse computational linguistics courseware'],\n",
       " 822: ['in addition we explore new features such as word cache and the states of an hmm trained by unsupervised learning'],\n",
       " 824: ['the model utilized is especially suited for languages with a rich morphology such as finnish',\n",
       "  'experiments on both finnish and english corpora show that the presented methods perform well compared to a current stateoftheart system'],\n",
       " 825: ['open mind word expert is an implemented active learning system for collecting word sense tagging from the general public over the web',\n",
       "  'we expect the system to yield a large volume of highquality training data at a much lower cost than the traditional method of hiring lexicographers'],\n",
       " 826: ['this paper presents work on the task of constructing a wordlevel translation lexicon purely from unrelated monolingual corpora',\n",
       "  'noun translation accuracy of scored against a parallel test corpus could be achieved'],\n",
       " 827: ['the use of semantic resources is comin modern but methods to extract lexical semantics have only recently begun to perform well enough for practical use',\n",
       "  'we propose an approximation based on attributes and coarseand finegrained matching that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty'],\n",
       " 830: ['we consider the problem of classifying documents not by topic but by overall sentiment eg determining whether a review is positive or negative',\n",
       "  'however the three machine learning methods we employed naive bayes maximum entropy classification and support vector machines do not perform as well on sentiment classification as on traditional topicbased categorization'],\n",
       " 833: ['basilisk begins with an unannotated corpus and seed words for each semantic category which are then bootstrapped to learn new words for each category',\n",
       "  'basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts'],\n",
       " 834: ['we explore how well phrases cohere across two languages specifically english and french and examine the particular conditions under which they do not',\n",
       "  'we demonstrate that while there are cases where coherence is poor there are many regularities which can be exploited by a statistical machine translation system'],\n",
       " 835: ['we present a broad coverage japanese grammar written in the hpsg formalism with mrs semantics',\n",
       "  'the grammar is created for use in real world applications such that robustness and performance issues play an important role'],\n",
       " 836: ['the grammar matrix is an opensource starterkit for the development of broad by using a type hierarchy to represent crosslinguistic generalizations and providing compatibility with other opensource tools for grammar engineering evaluation parsing and generation it facilitates not only quick startup but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding'],\n",
       " 837: ['we report on the parallel grammar pargram project which uses the xle parser and grammar development platform for six languages english'],\n",
       " 838: ['in this paper we propose a new statistical japanese dependency parser using a cascaded chunking model',\n",
       "  'we propose a new method that is simple and efficient since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side'],\n",
       " 839: ['while parameter estimation for me models is conceptually straightforward in practice me models for typical natural language tasks are very large and may well contain many thousands of free parameters',\n",
       "  'surprisingly the standardly used iterative scaling algorithms perform quite poorly in comparison to the others and for all of the test problems a limitedmemory variable metric algorithm outperformed the other choices'],\n",
       " 843: ['then we train a naive bayes classifier using the subjective nouns discourse features and subjectivity clues identified in prior research'],\n",
       " 844: ['this paper presents a set of algorithms for distinguishing personal names with multiple real referents in text based on little or no supervision'],\n",
       " 845: ['we investigate selecting examples by directly maximising tagger agreement on unlabelled data a method which has been theoretically and empirically motivated in the cotraining literature',\n",
       "  'however we find that simply retraining on all the newly labelled data can in some cases yield comparable results to agreementbased cotraining with only a fraction of the computational cost'],\n",
       " 847: ['this paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy',\n",
       "  'the tagger uses features which can be obtained for a variety of languages and works effectively not only for english but also for other languages such as german and dutch'],\n",
       " 848: ['when no gazetteer or other additional training resources are used the combined system attains a performance of f on the english development data integrating name location and person gazetteers and named entity systems trained on additional more general data reduces the fmeasure error by a factor of to on the english data'],\n",
       " 849: ['we discuss two namedentity recognition models which use characters and character grams either exclusively or as an important part of their data representation',\n",
       "  'the first model is a characterlevel hmm with minimal context information and the second model is a maximumentropy conditional markov model with substantially richer context features'],\n",
       " 851: [],\n",
       " 852: ['we use deep linguistic features to predict semantic roles on syntactic arguments and show that these perform considerably better than surfaceoriented features',\n",
       "  'we also show that predicting labels from a lightweight parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features'],\n",
       " 853: ['we present a system for automatically identifying propbankstyle semantic roles based on the output of a statistical parser for combinatory categorial grammar',\n",
       "  'this system performs at least as well as a system based on a traditional treebank parser and outperforms it on core argument roles'],\n",
       " 854: ['we present a general framework for distributional similarity based on the concepts of precision and recall',\n",
       "  'different parameter settings within this framework approximate different existing similarity measures as well as many more which have until now been unexplored',\n",
       "  'we show that optimal parameter settings outperform two existing stateoftheart similarity measures on two evaluation tasks for high and low frequency nouns'],\n",
       " 855: ['highprecision classifiers label unannotated data to automatically create a large training set which is then given to an extraction pattern learning algorithm'],\n",
       " 856: ['in this paper we discuss a necessary component for an opinion question answering system separating opinions from fact at both the document and sentence level'],\n",
       " 857: ['in this paper experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed',\n",
       "  'in more detail exgives a better precithan and by adding the tags assigned to the term as a feature a dramatic improvement of the results is obtained independent of the term selection approach applied'],\n",
       " 859: ['this paper presents the results from the aclsighansponsored first international chinese word segmentation bakeoff held in and reported in conjunction with the second sighan workshop on chinese language processing sapporo japan'],\n",
       " 861: ['the bakeoff is interesting and helpful',\n",
       "  'through the first bakeoff we could learn more about the development in chinese word segmentation and become more confident on our hhmmbased approach'],\n",
       " 862: ['this paper describes a distributional approach to the semantics of verbparticle eg',\n",
       "  'we then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verbparticle constructions'],\n",
       " 863: ['we also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords the relationship is not as strong as that using the automatically constructed thesaurus'],\n",
       " 864: ['we test the model over english nounnoun compounds and verbparticles and evaluate its correlation with similarities and hyponymy values in wordnet',\n",
       "  'based on mean hyponymy over partitions of data ranked on similarity we furnish evidence for the calculated similarities being correlated with the semantic relational content of wordnet'],\n",
       " 865: ['in this paper we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework'],\n",
       " 867: ['this paper presents the task definition resources participating systems and comparative results for the english lexical sample task which was orgaas part of the evaluation exercise'],\n",
       " 869: ['this paper introduces four different included in the summarization evaluation package and their evaluations'],\n",
       " 871: ['data we describe a new corpus of over handannotated dialog act tags and accompanying adjacency pair annotations for roughly hours of speech from naturallyoccurring meetings',\n",
       "  'we provide a brief summary of the annotation system and labeling procedure interannotator reliability statistics overall distributional statistics a description of auxiliary files distributed with the corpus and information on how to obtain the data'],\n",
       " 872: ['we develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations',\n",
       "  'our approach allows us to efficiently incorporate domain and task specific constraints at decision time resulting in significant improvements in the accuracy and the humanlike quality of the inferences'],\n",
       " 873: ['second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context'],\n",
       " 874: ['this paper reports the results of experimentsusing memorybased learning to guide a de terministic dependency parser for unrestricted natural language text',\n",
       "  'using data from a small treebank of swedish memorybased classifiers for predicting the next action of the parser are constructed'],\n",
       " 876: ['this paper describes nombank a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus',\n",
       "  'nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus'],\n",
       " 877: ['results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans',\n",
       "  'distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language'],\n",
       " 878: ['crucial to this approach is the proper characterization of entities as relation components which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events'],\n",
       " 879: ['in particular it allows one to efficiently learn a model which discriminates among the entire space of parse trees as opposed to reranking the top few candidates',\n",
       "  'our models can condition on arbitrary features of input sentences thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness'],\n",
       " 880: ['we detect similarity strength antonymy enablement and temporal happensbefore relations between pairs of strongly associated verbs using lexicosyntactic patterns over the web'],\n",
       " 881: ['our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the web for related syntactic entailment templates'],\n",
       " 882: ['we describe how simple commonly understood statistical models such as statistical dependency parsers probabilistic contextfree grammars and wordtoword translation models can be effectively combined into a unified bilingual parser that jointly searches for the best english parse korean parse and word alignment where these hidden structures all constrain each other',\n",
       "  'the model used for parsing is completely factored into the two parsers and the tm allowing separate parameter estimation'],\n",
       " 884: ['this paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input generally a syntactic parse tree has yet to be fully exploited',\n",
       "  'we propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed'],\n",
       " 885: ['a novel aspect of our approach is the use of verb slot and noun class information as the basis for backing off in our probability model'],\n",
       " 886: ['we apply statistical machine translation smt tools to generate novel paraphrases of input sentences in the same language'],\n",
       " 887: [],\n",
       " 888: ['this paper presents an indepth study on such issues of processing architecture and feature representation for chinese pos tagging within a maximum entropy framework',\n",
       "  'we found that while the allatonce characterbased approach is the best the oneatatime characterbased approach is a worthwhile compromise performing only slightly worse in terms of accuracy but taking shorter time to train and run'],\n",
       " 889: ['a novel technique for maximum a posteriori map adaptation of maximum entropy maxent and maximum entropy markov models memm is presented',\n",
       "  'when evaluating on the mismatched outofdomain test data the gram baseline is outperformed by the improvement brought by the adaptation technique using a very small amount of matched bn data kwds is about relative'],\n",
       " 890: ['the proposal consists of i decision stumps that use subtrees as features and ii the boosting algorithm which employs the subtreebased decision stumps as weak learners'],\n",
       " 891: ['we are now considering an approach for computing sentence importance based on the concept of eigenvector centrality prestige that we call lexpagerank'],\n",
       " 892: ['if two translation systems differ differ in performance on a test set can we trust that this indicates a difference in true system quality',\n",
       "  'to answer this question we describe bootstrap resampling methods to compute statistical significance of test results and validate them on the concrete example of the even for small test sizes of only sentences our methods may give us assurances that test result differences are real'],\n",
       " 893: ['in this paper we introduce textrank a graphbased ranking model for text processing and show how this model can be successfully used in natural language applications',\n",
       "  'in particular we propose two innovative unsupervised methods for keyword and sentence extraction and show that the results obtained compare favorably with previously published results on established benchmarks'],\n",
       " 894: ['this paper introduces an approach to sentiment analysis which uses support vector machines svms to bring together diverse sources of potentially pertinent information including several favorability measures for phrases and adjectives and where available knowledge of the topic of the text'],\n",
       " 897: ['our results show that adding syntactic information to the evaluation metric improves both sentencelevel and corpuslevel correlation with human judgments'],\n",
       " 899: ['while there is a large body of previous work focused on finding the semantic similarity of concepts and words the application of these wordoriented methods to text similarity has not been yet explored',\n",
       "  'in this paper we introduce a method that combines wordtoword similarity metrics into a texttotext metric and we show that this method outperforms the traditional text similarity metrics based on lexical matching'],\n",
       " 900: ['to demonstrate the efficiency scal ability and accuracy of these algorithms we present experiments on bikels implementation of collins',\n",
       "  'we show in particu lar how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications'],\n",
       " 901: ['the parser uses a basic bottomup shiftreduce algorithm but employs a classifier to determine parser actions instead of a grammar'],\n",
       " 902: ['we introduce an approach of exploiting the semantic structure of a sentence anchored to an opinion bearing verb or adjective',\n",
       "  'this method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from framenet',\n",
       "  'our experimental results show that our system performs significantly better than the baseline'],\n",
       " 904: ['we introduce spmt a new class of statistical translation models that use syntactified target language phrases',\n",
       "  'the spmt models outperform a state of the art phrasebased baseline model by bleu points on the nist chineseenglish test corpus and points on a humanbased quality metric that ranks translations on a scale from to'],\n",
       " 905: ['we discuss different strategies for smoothing the phrasetable in statistical mt and give results over a range of translation settings',\n",
       "  'we show that any type of smoothing is a better idea than the relativefrequency estimates that are often used'],\n",
       " 906: ['we test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data as well as improvements in target domain parsing accuracy using our improved tagger'],\n",
       " 907: ['integer linear programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints',\n",
       "  'this approach is applied to dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateoftheart'],\n",
       " 908: ['we investigate whether one can determine from the transcripts of us congressional floor debates whether the speeches represent support of or opposition to proposed legislation',\n",
       "  'to address this problem we exploit the fact that these speeches occur as part of a discussion this allows us to use sources of information regarding relationships between discourse segments such as whether a given utterance indicates agreement with the opinion expressed by another'],\n",
       " 909: ['using the overall density and precision of coherency in the corpus the statistical estimation picks up appropriate polar atoms among candidates without any manual tuning of the threshold values',\n",
       "  'the experimental results show that the precision of polarity assignment with the automatically acquired lexicon was on average and our method is robust for corpora in diverse domains and for the size of the initial lexicon'],\n",
       " 910: ['we present an approach for the joint extraction of entities and relations in the con text of opinion recognition and analysis',\n",
       "  'the resulting system achieves fmeasuresof and for entity and relation extrac tion respectively improving substantially over prior results in the area'],\n",
       " 911: ['in this paper we approach word sense disambiguation and information extraction as a unified tagging problem'],\n",
       " 912: ['this measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech'],\n",
       " 913: ['in this paper we investigate a new problem identifying the which a document is written',\n",
       "  'the results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy'],\n",
       " 914: ['in this paper we describe how treebanks for languages were converted into the same dependency format and how parsing performance was measured',\n",
       "  'we also give an overview of the parsing approaches that participants took and the results that they achieved'],\n",
       " 916: ['the first stage based on the unlabeled dependency parsing models described by mcdonald and pereira augmented with morphological features for a subset of the languages'],\n",
       " 917: ['nonprojective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser'],\n",
       " 918: ['in particular while word level models benefit greatly from reestimation phraselevel models do not the crucial difference is that distinct word alignments cannot all be correct while distinct segmentations can',\n",
       "  'we also show that interpolation of the two methods can result in a modest increase in bleu score'],\n",
       " 919: ['we present discriminative reordering models for phrasebased statistical machine translation',\n",
       "  'we evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a wordaligned corpus'],\n",
       " 921: ['we present translation results on the shared task exploiting parallel texts for statistical machine translation generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories',\n",
       "  'our translation system is available opensource under the gnu general'],\n",
       " 922: ['we also define a direct probability model and use a lineartime dynamic programming algorithm to search for the best derivation'],\n",
       " 924: ['we introduce chinese whispers a randomized graphclustering algorithm which is timelinear in the number of edges',\n",
       "  'after a detailed definition of the algorithm and a discussion of its strengths and weaknesses the performance of chinese whispers is measured on natural language processing nlp problems as diverse as language separation acquisition of syntactic word classes and word sense disambiguation'],\n",
       " 925: ['we demonstrate that the consistency constraints that allow flat phrasal models to scale also help itg algorithms producing an times faster insideoutside algorithm',\n",
       "  'we also show that the phrasal translation tables produced by the itg are superior to those of the flat joint phrasal model producing up to a point improvement in bleu score',\n",
       "  'finally we explore for the first time the utility of a joint phrasal translation model as a word alignment method'],\n",
       " 926: ['we show that this results in an im provement in the quality of translation and that the value of syntactic supertags in flat structured phrasebased models is largely due to better local reorderings'],\n",
       " 927: ['we describe a mixturemodel approach to adapting a statistical machine translation system for new domains using weights that depend on text distances to mixture components',\n",
       "  'we investigate a number of variants on this approach including crossdomain versus dynamic adaptation linear versus loglinear mixtures language and translation model adaptation different methods of assigning weights and granularity of the source unit being adapted to'],\n",
       " 928: [],\n",
       " 930: ['an automatic metric for machine translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality significantly outperforming the more used it is one of several automatic metrics used in this years shared task within the acl wmt workshop',\n",
       "  'this paper recaps the technical details underlying the metric and describes recent improvements in the metric',\n",
       "  'the latest release includes improved metric parameters and extends the metric to support evaluation of mt output in spanish french and'],\n",
       " 932: ['to address this problem we use a maximum entropy classifier combined with rulebased filters to detect preposition errors in a corpus of student essays',\n",
       "  'although our work is preliminary we achieve a precision of with a recall of'],\n",
       " 933: ['the goal of this task is to allow for comparison across senseinduction and discrim ination systems and also to compare thesesystems to other supervised and knowledgebased systems',\n",
       "  'we reused the semeval english lexical sample subtask of task and set up both clusteringstyle unsuper vised evaluation using ontonotes senses as goldstandard and a supervised evaluation using the part of the dataset for mapping'],\n",
       " 934: ['this paper presents the coarsegrained en glish allwords task at semeval',\n",
       "  'we describe our experience in producing acoarse version of the wordnet sense inven tory and preparing the sensetagged corpusfor the task'],\n",
       " 935: ['in the task annotators and systems find an alternative substitute word or phrase for a target word in context',\n",
       "  'the task involves both finding the synonyms and disambiguating the contextparticipating systems are free to use any lexical resource'],\n",
       " 936: ['this task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name'],\n",
       " 937: ['it avoids the pitfalls of evaluating a graph of interrelated labels by defining three sub tasks that allow pairwise eval uation of temporal relations'],\n",
       " 938: ['this paper describes our experience in preparing the data and evaluating the results for three subtasks of semeval task lexical sample semantic role labelingsrl and allwords respectively'],\n",
       " 940: ['in this paper we investigate several nonprojective parsing algorithms for dependency parsing providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others called here the edgefactored model'],\n",
       " 941: ['j schroeder ed ac uk abstract this paper analyzes the translation quality of machine translation systems for language pairs translating between czech english french german hungarian and spanish',\n",
       "  'we use the human judgments of the systems to analyze automatic evaluation metrics for translation quality and we report the strength of the correlation with human judgments at both the systemlevel and at the sentencelevel'],\n",
       " 942: ['based on these findings we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the mt task providing an improvement of bleu',\n",
       "  'we also show that improving segmentation consistency using external lexicon and proper noun features yields a bleu increase'],\n",
       " 943: ['this paper describes two parallel implementations of giza that accelerate this word alignment process',\n",
       "  'results show a nearlinear speedup according to the number of cpus used and alignment quality is preserved'],\n",
       " 944: ['this paper examines the stanford typed dependencies representation which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding'],\n",
       " 945: ['a severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved',\n",
       "  'experiments on the penn wsj treebank show that the model achieves stateoftheart performance for both constituent and dependency accuracy'],\n",
       " 946: ['this shared task not only unifies the shared tasks of the previous four years under a unique dependencybased formalism but also extends them significantly this years syntactic dependencies include more information such as namedentity boundaries the semantic dependencies model roles of both verbal and nominal predicates'],\n",
       " 948: [],\n",
       " 949: ['joshua implements all of the algorithms required for synchronous context grammars scfgs chartparsing gram language model integration beamcubepruning and extraction'],\n",
       " 950: [],\n",
       " 951: ['automatic machine translation mt evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to mt output with human judgments of translation performance',\n",
       "  'terplus was shown to be one of the top metrics in nists metrics matr challenge having the highest average rank in terms of pearson and spearman correlation'],\n",
       " 952: ['in this paper we present a machine learning system that finds the scope of negation in biomedical texts',\n",
       "  'to investigate the robustness of the approach the system is tested on the three subcorpora of the bioscope corpus representing different text types',\n",
       "  'it achieves the best results to date for this task with an error reduction of compared to current state of the art results'],\n",
       " 953: ['in the process of comparing several solutions to these challenges we reach some surprising conclusions as well as develop an system that achieves on the conll ner shared task the best reported result for this dataset'],\n",
       " 954: ['in this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts',\n",
       "  'to investigate the robustness of the approach the system is tested on the three subcorpora of the bioscope corpus that represent different text types'],\n",
       " 955: ['the paper presents the design and implementation of the bionlp shared task and reports the final results with analysis',\n",
       "  'the data was developed based on the genia event corpus'],\n",
       " 956: [],\n",
       " 957: ['in this paper we give an introduction to using amazons mechanical turk crowdsourc ing platform for the purpose of collecting data for human language technologies'],\n",
       " 958: ['this year we also investigated increasing the number of human judgments by hiring nonexpert annotators through amazons'],\n",
       " 959: ['in this paper we explore the computational modelling of compositionality in distributional models of semantics',\n",
       "  'in particular we model the semantic composition of pairs of adjacent english adjecand nouns from the national we build a vectorbased semantic space from a lemmatised version of the bnc where the most frequent an lemma pairs are treated as single tokens'],\n",
       " 960: [],\n",
       " 961: ['the conll shared task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts'],\n",
       " 962: ['the contributions of this paper are we introduce posspecific prior polarity features'],\n",
       " 963: ['five main tasks and three supporting tasks were arranged and their results show advances in the state of the art in finegrained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects'],\n",
       " 964: ['as its second time to be arranged for communitywide focused efforts it aimed to measure the advance of the community since and to evaluate generalization of the technology to full text papers'],\n",
       " 965: ['this paper briefly describes the ontonotes annotation coreference and other layers and then describes the parameters of the shared task including the format preprocessing information and evaluation criteria and presents and discusses the results achieved by the participating systems',\n",
       "  'having a standard test set and evaluation parameters all based on a new resource that provides multiple integrated annotation layers parses semantic roles word senses named entities and coreference that could support joint models should help to energize ongoing research in the task of entity and event coreference'],\n",
       " 966: ['our system was ranked first in both tracks with a score of in the closed track and in the open track'],\n",
       " 967: ['we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for evaluation metrics',\n",
       "  'we also conducted a pilot tunable metrics task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality'],\n",
       " 968: ['we include ranking and adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced tuning version shown to outperform bleu in minimum error rate training for a phrasebased urduenglish system'],\n",
       " 969: [],\n",
       " 970: ['this paper presents the results of the wmt shared tasks which included a translation task a task for machine translation evaluation metrics and a task for runtime estimation of machine translation quality'],\n",
       " 971: ['the combination of wordalign plus charalign reduces the variance average square error by a factor of over more importantly because wordalign and charalign were designed to work robustly on texts that are smaller and more noisy than the hansards it has been possible to successfully deploy the programs at atampt language line services a commercial translation service to help them with difficult terminology'],\n",
       " 972: ['i survey some recent applicationsoriented nl generation systems and claim that despite very different theoretical backgrounds these systems have a remarkably similar architecture in terms of the modules they divide the generation process into the computations these modules perform and the way the modules interact with each other',\n",
       "  'i also compare this consensus architecture among applied nlg systems with psycholinguistic knowledge about how humans speak and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems'],\n",
       " 973: ['in this paper we describe an unsupervised learning algorithm for automatically training a rulebased part of speech tagger without using a manually tagged corpus',\n",
       "  'we compare this algorithm to the baumwelch algorithm used for unsupervised training of stochastic taggers'],\n",
       " 974: ['typically ambiguous verb phrases of the form v p np through a model which considers values of the four head words v nl paper shows that the problem is analogous to ngram language models in speech recognition and that one of the most common methods for language modeling the backedoff estimate is applicable',\n",
       "  'results on wall street journal data of accuracy are obtained using this method'],\n",
       " 975: ['the idea is to pool the evidence provided by the component methods and to then solve a target problem by applying the single strongest piece of evidence whatever type it happens to be'],\n",
       " 976: [],\n",
       " 977: ['for this purpose it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word',\n",
       "  'in automatic tests using treebankderived data this technique achieved recall and precision rates of roughly for basenp chunks and for somewhat more complex chunks that partition the sentence'],\n",
       " 978: [],\n",
       " 979: ['additional advantages specific to a memorybased approach include i the relatively small tagged corpus size sufficient for training ii incremental learning iii explanation capabilities iv flexible integration of information in case representations v its nonparametric nature vi reasonably good results on unknown words without morphological analysis and vii fast learning and tagging',\n",
       "  'in this paper we show that a largescale application of the memorybased approach is feasible we obtain a tagging accuracy that is on a par with that of known statistical approaches and with attractive and time complexity properties when using treebased formalism for indexing and searching huge case bases'],\n",
       " 980: ['this paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context'],\n",
       " 981: ['this paper presents a statistical model which trains from a corpus annotated with partof speech tags and assigns them to previously unseen text with stateoftheart accuracy'],\n",
       " 982: ['we show that bods results are at least partially due to an extremely fortuitous choice of test data and partially due to using cleaner data than other researchers'],\n",
       " 984: ['we present a statistical word feature the word relation matrix which can be used to find translated pairs of words and terms from nonparallel corpora across language groups'],\n",
       " 985: ['selectional preference is traditionally connected with sense ambiguity this paper explores how a statistical model of selectional preference requiring neither manual annotation of selection restrictions nor supervised training can be used in sense disambiguation'],\n",
       " 986: ['abstract this paper presents a statistical parser for natural language that obtains a parsing accuracyroughly precision and recallwhich surpasses the best previously published results on the wall st journal domain',\n",
       "  'furthermore the parser returns several scored parses for a sentence and this paper shows that a scheme to pick the best parse from the highest scoring a dramatically higher accuracy of precision and recall'],\n",
       " 987: ['we present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method at the same performance level',\n",
       "  'we also present a new thresholding technique global thresholding which combined with the new beam thresholding gives an additional factor of two improvement and a novel technique multiple pass parsing that can be combined with the others to yield yet another improvement'],\n",
       " 988: ['the method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages'],\n",
       " 989: [],\n",
       " 990: ['the methods described in this paper mcquittys similarity analysis wards minimumvariance method and the em algorithm assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text',\n",
       "  'these methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs',\n",
       "  'overall the most accurate of these procedures is mcquittys similarity analysis in combination with a high dimensional feature set'],\n",
       " 992: ['we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text we show how these concepts can be implemented and we discuss results that we obtained with a discoursebased summanzation program motivation the evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some consamples of the output in very few cases output of a summarization program with a humanmade summary or evaluated with the help of human subjects usually the results are modest unfortunately evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions the position that we take in this paper is that in order to build highquality summarization programs one needs to evaluate not only a representative set of automatically generated outputs a highly difficult problem by itself but also the adequacy of the assumptions that these programs use that way one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each with few exceptions automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text see paice for an excellent overview determining the salient parts is considered to be achievable because one or more of the following assumptions hold i important sentences in a text contain words that are used frequently lahn edmundson n important sentences contain words that are used in the tide and section headings edmundson in important sentences are located at the beginning or end of paragraphs baxendale iv important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically through training techniques lin and hovy v important sentences use words as greatest and significant or indiphrases as the main aim of this paper and the purpose of this article while nonimportant senuse words as impossible edmundson rush salvador and zamora vi important sentences and concepts highest connected entities in elaborate semantic structures skorochodko lin barzilay and elhadad and vn imponant and nonimportant sentences are derivable from a discourse representation of the text sparck jones ono surmta and mike in determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections computers are accurate tools flowever in determining the concepts that are semantically related or the discourse structure of a text computers are no longer so accurate rather they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement although it is plausible that elaborate cohesionand coherencebased structures can be used effectively in summarization we believe that before building summarization programs we should determine the extent to which these assumptions hold in this paper we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text we show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program from discourse trees to summaries an empirical view'],\n",
       " 993: ['it is compatible with the princeton wordnet but integrates principlebased modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations',\n",
       "  'germanet includes a new treatment of regular polysemy artificial concepts and of particle verbs'],\n",
       " 994: ['it is suggested that the system is resolving a subset of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution'],\n",
       " 995: [],\n",
       " 997: ['recent work has used probabilistic contextfree grammars pcfgs to assign probabilities to constituents and to use these probabilities as the starting point for the fom'],\n",
       " 998: ['by working within the framework of maximum entropy theory and utilizing a flexible objectbased architecture the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions'],\n",
       " 999: ['this paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm',\n",
       "  'we incorporate multiple anaphora resolution factors into a statistical framework specifically the distance between the pronoun and the proposed antecedent gendernumberanimaticity of the proposed antecedent governing head information and noun phrase repetition'],\n",
       " 1000: ['marcu has characterised an important and difficult problem in text planning given a set of facts to convey and a set of rhetorical relations that can be used to link them together how can one arrange this material so as to yield the best possible text'],\n",
       " 1003: ['more importantly the clustering approach outperforms the only muc system to treat coreference resolution as a learning problem'],\n",
       " 1004: ['this paper describes and evaluates a languageindependent bootstrapping algorithm based on iterative learning and reestimation of contextual and morphological patterns captured in hierarchically smoothed trie models',\n",
       "  'the algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required languagespecific information tokenizers or tools'],\n",
       " 1005: ['the first method uses a similar algorithm to that of yarowsky with modifications motivated by blum and mitchell'],\n",
       " 1006: ['three stateoftheart statistical parsers are combined to produce more accurate parses as well as new bounds on achievable treebank parsing accuracy'],\n",
       " 1008: ['in this paper we discuss cascaded memory based grammatical relations assignment',\n",
       "  'we studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder']}"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d94595",
   "metadata": {},
   "source": [
    "#### model_w2v_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "b90682fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:48:48.892590Z",
     "start_time": "2023-05-01T16:48:41.696580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_abstract_teste    = vetor_frases_w2v(abstract_palavras_teste, model_w2v_cbow)\n",
    "v_introducao_teste  = vetor_frases_w2v(introducao_palavras_teste, model_w2v_cbow)\n",
    "v_conclusao_teste   = vetor_frases_w2v(conclusao_palavras_teste, model_w2v_cbow)\n",
    "\n",
    "#Abtract e Introdução\n",
    "sim_ab_intro_teste,indices_ab_intro_teste,lista_idx_ab_intro_teste =aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_introducao_teste.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_teste,indices_ab_conc_teste,lista_idx_ab_c_teste = aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_conclusao_teste.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_teste = frases_interseccao(lista_idx_ab_intro_teste,lista_idx_ab_c_teste)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_teste  = get_frases_objetivo_teste(indices_objetivo_teste, papers_teste.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "760e87f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:48:57.183681Z",
     "start_time": "2023-05-01T16:48:48.892590Z"
    }
   },
   "outputs": [],
   "source": [
    "p_w2v_cbow_teste, r_w2v_cbow_teste, f_w2v_cbow_teste  =calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73bf54f",
   "metadata": {},
   "source": [
    "\n",
    "#w2v\n",
    "rouge_teste = calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))\n",
    "\n",
    "maximo_rouge_por_paper_ft_teste   = max_rouge(rouge_teste)\n",
    "\n",
    "\n",
    "precisao_sg_ft_teste, revocacao_sg_ft_teste = rouge_precisao_revocacao(maximo_rouge_por_paper_ft_teste, lista_padrao_ouro_teste)\n",
    "qtd_precisao_ft_teste, qtd_zero_ft_teste, media_ft_teste = analise_precisao(precisao_sg_ft_teste, lista_padrao_ouro_teste)\n",
    "print('------')\n",
    "qtd_revocacao_ft,  media_revocacao_ft       = analise_revocacao(revocacao_sg_ft_teste, lista_padrao_ouro_teste, media_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8b467",
   "metadata": {},
   "source": [
    "### geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "4bd2a7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:10:50.338759Z",
     "start_time": "2023-05-01T21:10:50.273666Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_sg_teste\n",
      "precisão: 0.886\n",
      "revocação: 0.906\n",
      "f-score: 0.888\n",
      "precisao acima de 80%: 86.0\n",
      "recall acima de 80%: 90.0\n",
      "igual a 0: 7.0\n",
      "-------\n",
      "w2v_cbow_teste\n",
      "precisão: 0.891\n",
      "revocação: 0.909\n",
      "f-score: 0.89\n",
      "precisao acima de 80%: 87.0\n",
      "recall acima de 80%: 90.0\n",
      "igual a 0: 5.0\n",
      "-------\n",
      "ft_sg_teste\n",
      "precisão: 0.889\n",
      "revocação: 0.913\n",
      "f-score: 0.893\n",
      "precisao acima de 80%: 86.0\n",
      "recall acima de 80%: 90.0\n",
      "igual a 0: 6.0\n",
      "-------\n",
      "ft_cbow_teste\n",
      "precisão: 0.892\n",
      "revocação: 0.907\n",
      "f-score: 0.891\n",
      "precisao acima de 80%: 87.0\n",
      "recall acima de 80%: 89.0\n",
      "igual a 0: 6.0\n"
     ]
    }
   ],
   "source": [
    "print('w2v_sg_teste')\n",
    "print('precisão:',media2(media(max_rouge(p_w2v_sg_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_w2v_sg_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_w2v_sg_teste))))\n",
    "funcao_80_e_0(p_w2v_sg_teste,r_w2v_sg_teste)\n",
    "print('-------')\n",
    "\n",
    "print('w2v_cbow_teste')\n",
    "print('precisão:',media2(media(max_rouge(p_w2v_cbow_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_w2v_cbow_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_w2v_cbow_teste))))\n",
    "funcao_80_e_0(p_w2v_cbow_teste,r_w2v_cbow_teste)\n",
    "\n",
    "print('-------')\n",
    "print('ft_sg_teste')\n",
    "print('precisão:',media2(media(max_rouge(p_ft_sg_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_ft_sg_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_ft_sg_teste))))\n",
    "funcao_80_e_0(p_ft_sg_teste,r_ft_sg_teste)\n",
    "\n",
    "print('-------')\n",
    "print('ft_cbow_teste')\n",
    "print('precisão:',media2(media(max_rouge(p_ft_cbow_teste))))\n",
    "print('revocação:',media2(media(max_rouge(r_ft_cbow_teste))))\n",
    "print('f-score:',media2(media(max_rouge(f_ft_cbow_teste))))\n",
    "funcao_80_e_0(p_ft_cbow_teste,r_ft_cbow_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2b853",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "id": "44b6569f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:30:24.665638Z",
     "start_time": "2023-05-01T21:30:18.984806Z"
    }
   },
   "outputs": [],
   "source": [
    "palavras_papers_teste2 = {k:[word_tokenize(f) for f in v] for k,v in frases_papers_teste.items()}\n",
    "palavras_papers_teste_es = funcao_aplicar_es_sentencer(palavras_papers_teste2)\n",
    "palavras_papers_teste = {k:v for k,v in zip(palavras_papers_teste2.keys(),palavras_papers_teste_es)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "69a6d347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:30:46.496770Z",
     "start_time": "2023-05-01T21:30:24.669869Z"
    }
   },
   "outputs": [],
   "source": [
    "bigramas_teste          = gerador_ngramas(token_frases_teste,2)\n",
    "probabilidades_bigramas  = gerador_probabilidades(bigramas_teste, model_bi)\n",
    "\n",
    "bigramas_perp_teste       = funcao_perplexidade(list(frases_papers_teste.values()), 2 , model_bi)\n",
    "media_bigramas_perp_teste = media_perplexidade(bigramas_perp_teste)\n",
    "\n",
    "bigramas_perp_teste_s_inf       = lista_sem_infinito(bigramas_perp_teste)\n",
    "media_bigramas_perp_teste_s_inf = media_lista_sem_infinito(bigramas_perp_teste_s_inf)\n",
    "\n",
    "frase_objetivo_bigramas_teste    = selecao_frases_obj_mle(probabilidades_bigramas, frases_papers_teste2)\n",
    "lista_frase_teste_bigramas,lista_prob_teste_bigramas,lista_geral_teste_bigramas = filtro_selecao_frases_mle(frase_objetivo_bigramas_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "405a9a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:30:54.739991Z",
     "start_time": "2023-05-01T21:30:46.496770Z"
    }
   },
   "outputs": [],
   "source": [
    "#lista_prob_frase - guarda a probabilidade de cada frase, seu tamanho é a qtd de frases\n",
    "#lista_prob_frase_paper - guarda a probabilidade de todas as frases de cada paper, seu tamanho é a qtd de paper\n",
    "\n",
    "lista_prob_frase_paper_teste=[]\n",
    "for paper in lista_bigramas2:\n",
    "    lista_prob_frase=[]\n",
    "    for frase in paper:\n",
    "        prob_frase=1\n",
    "        for bigrama in frase:\n",
    "            prob = (model_bi.score(bigrama[1], context=[bigrama[0]]))\n",
    "            prob_frase = prob_frase * prob\n",
    "            #print(prob)\n",
    "            #print(trigrama, prob)\n",
    "        #print('PROB FRASE',prob_frase)\n",
    "        lista_prob_frase.append(prob_frase)\n",
    "    lista_prob_frase_paper_teste.append(lista_prob_frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cc3cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T16:11:48.087833Z",
     "start_time": "2023-05-01T16:11:48.057745Z"
    }
   },
   "source": [
    "#Analisando o tamanho das frases de teste\n",
    "x=[]\n",
    "for j in list(frase_objetivo_teste.values()):\n",
    "    for i in j:\n",
    "        x.append(len(i.split(' ')))\n",
    "np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "aaf3f029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:30:54.752958Z",
     "start_time": "2023-05-01T21:30:54.743247Z"
    }
   },
   "outputs": [],
   "source": [
    "lista=[]\n",
    "for i,j in zip(range(len(lista_prob_teste_bigramas)),list(frases_papers_teste.keys()) ):\n",
    "    lista.append(sorted(zip(lista_prob_teste_bigramas[i],frases_papers_teste[j]), reverse=True))\n",
    "    \n",
    "todos=[]\n",
    "for i in range(len(lista)):\n",
    "    x=[]\n",
    "    for i in list(lista[i]):\n",
    "        frases = i[1].split(' ')\n",
    "        if (len(frases) >=19) and i[0]!=1:\n",
    "            x.append(i)\n",
    "    todos.append(x)\n",
    "    \n",
    "frases_objetivo_ml_teste=[]\n",
    "for i in range(len(todos)):\n",
    "    frases_objetivo_ml_teste.append(todos[i][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "d9c9e09c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:30:54.768235Z",
     "start_time": "2023-05-01T21:30:54.752958Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_teste_exp2=[]\n",
    "for lista_prob_teste_bigramas in frases_objetivo_ml_teste:\n",
    "    cada_paper=[]\n",
    "    for frase_obj_prob in lista_prob_teste_bigramas:\n",
    "        cada_paper.append(frase_obj_prob[1])\n",
    "    frase_objetivo_teste_exp2.append(cada_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b0fa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:04:32.660167Z",
     "start_time": "2023-05-01T15:04:32.636171Z"
    }
   },
   "source": [
    "dict(zip(list(frases_papers_teste.keys()),frase_objetivo_teste_exp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "f6f68e6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:30:54.828069Z",
     "start_time": "2023-05-01T21:30:54.771266Z"
    }
   },
   "outputs": [],
   "source": [
    "p_bi, r_bi, f_bi = calculo_rouge(frase_objetivo_teste_exp2,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "d908d9e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:30:54.843471Z",
     "start_time": "2023-05-01T21:30:54.830029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrama com rest\n",
      "precisão: 0.009\n",
      "revocação: 0.009\n",
      "f-score: 0.009\n",
      "precisao acima de 80%: 1.0\n",
      "recall acima de 80%: 1.0\n",
      "igual a 0: 99.0\n"
     ]
    }
   ],
   "source": [
    "print('bigrama com rest')\n",
    "print('precisão:',media2(media(max_rouge(p_bi))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi))))\n",
    "funcao_80_e_0(p_bi,r_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e468f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:04:52.654467Z",
     "start_time": "2023-05-01T15:04:46.739483Z"
    }
   },
   "source": [
    "rouge_total=[]\n",
    "for i,j in zip(frase_objetivo_teste_exp2,list(padrao_ouro_teste_frases.values())):\n",
    "    rouge_1={}\n",
    "    for h in range(len(i)):\n",
    "        for z in range(len(j)):\n",
    "            rouge_1[(h,z)] = (scorer.score(j[z],i[h])['rouge1'].precision)\n",
    "    rouge_total.append(rouge_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e2f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:04:54.697747Z",
     "start_time": "2023-05-01T15:04:54.687748Z"
    }
   },
   "source": [
    "maximo_rouge_paper = max_rouge(rouge_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48522264",
   "metadata": {},
   "source": [
    "## trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecao_frases_obj_mle(probabibilidade, frases_papers):\n",
    "    \"\"\"função para selecionar até 3 maiores elementos da lista\"\"\"\n",
    "    #Pega os 3 elementos de maior probabilidade\n",
    "    lista=[]\n",
    "    dicionario={}\n",
    "    x=[]\n",
    "    for i,j in zip(range(len(probabibilidade)),range(len(frases_papers))):\n",
    "        lista.append(sorted(zip(probabibilidade[i],frases_papers[j]), reverse=True)[:3])\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "b0f8b991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:31:02.267340Z",
     "start_time": "2023-05-01T21:30:54.844468Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_tri'):\n",
    "    model_tri = joblib.load('model_tri')\n",
    "else:\n",
    "    n = 3\n",
    "    paddedLine = [list(pad_both_ends(word_tokenize(s), n)) for s in frase_objetivo_ft_formato_ml]\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, paddedLine)\n",
    "\n",
    "    model_tri = MLE(n)\n",
    "    model_tri.fit(train_data, padded_sents)\n",
    "\n",
    "    print(model_tri.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "f3939a5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:31:46.082918Z",
     "start_time": "2023-05-01T21:31:15.089425Z"
    }
   },
   "outputs": [],
   "source": [
    "trigramas_teste          = gerador_ngramas(token_frases_teste,3)\n",
    "probabilidades_trigramas = gerador_probabilidades(trigramas_teste, model_tri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "id": "5173cec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:34:15.349552Z",
     "start_time": "2023-05-01T21:34:15.325553Z"
    }
   },
   "outputs": [],
   "source": [
    "lista=[]\n",
    "for i,j in zip(range(len(probabilidades_trigramas)),list(frases_papers_teste.keys()) ):\n",
    "    lista.append(sorted(zip(probabilidades_trigramas[i],frases_papers_teste[j]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "24072e53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:34:20.814607Z",
     "start_time": "2023-05-01T21:34:20.756050Z"
    }
   },
   "outputs": [],
   "source": [
    "todos=[]\n",
    "for i in range(len(lista)):\n",
    "    x=[]\n",
    "    for i in list(lista[i]):\n",
    "        frases = i[1].split(' ')\n",
    "        if (len(frases) >=19) and i[0]!=1:\n",
    "            x.append(i)\n",
    "    todos.append(x)\n",
    "    \n",
    "frases_objetivo_ml_teste=[]\n",
    "for i in range(len(todos)):\n",
    "    frases_objetivo_ml_teste.append(todos[i][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "93a12510",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:35:16.100004Z",
     "start_time": "2023-05-01T21:35:16.046732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.0,\n",
       "   'words that cooccur with the target word in unusually great frequency especially in certain collocational relationships will tend to be reliable indicators of one of the target words senses eg flock and bulldozer for crane'),\n",
       "  (0.0,\n",
       "   'words not only tend to occur in collocations that reliably indicate their sense they tend to occur in multiple such collocations'),\n",
       "  (0.0,\n",
       "   'words in the entry appearing in the most reliable collocational relationships with the target word are given the most weight based on the criteria given in yarowsky')],\n",
       " [(0.0,\n",
       "   'word lattices are commonly used to model uncertainty in speech recognition waibel and lee and are well adapted for use with ngram models'),\n",
       "  (0.0,\n",
       "   'while true irregular forms eg child i children must be kept in a small exception table the problem of multiple regular patterns usually increases the size of this table dramatically'),\n",
       "  (0.0,\n",
       "   'while this experiment shows that statistical models can help make choices in generation it fails as a computational strategy')],\n",
       " [(0.0,\n",
       "   'where m lt n and k lt ki lt n for all i lt m for example a model pf ihi hh might be interpolated as follows where e ai hi hh for all histories hi hh'),\n",
       "  (0.0,\n",
       "   'usually an ngram model refers to a markov process where the probability of a particular token being generating is dependent on the values of the previous n tokens generated by the same process'),\n",
       "  (0.0,\n",
       "   'traditionally disambiguation problems in parsing have been addressed by enumerating possibilities and explicitly declaring knowledge which might aid the disambiguation process')],\n",
       " [(0.0,\n",
       "   'with criteria like the corpus frequency of a word its specificity for a given domain and the salience of its cooccurrence patterns it should be possible to make a selection of corresponding vocabularies in the two languages'),\n",
       "  (0.0,\n",
       "   'this study suggests that the identification of word translations should also be possible with nonparallel and even unrelated texts'),\n",
       "  (0.0,\n",
       "   'this similarity measure leads to a value of zero for identical matrices and to a value of in the case that a nonzero entry in one of the matrices always corresponds to a zerovalue in the other')],\n",
       " [(0.0,\n",
       "   'when tested on this large data set lexas performs better than the default strategy of picking the most frequent sense'),\n",
       "  (0.0,\n",
       "   'when tested on a large separately collected data set our program performs better than the default strategy of picking the most frequent sense'),\n",
       "  (0.0,\n",
       "   'when tested on a common data set our wsd program gives higher classification accuracy than previous work on wsd')],\n",
       " [(0.0,\n",
       "   'yet for intentionally knock twice this is not the case these adverbs do not commute and the semantics are distinct'),\n",
       "  (0.0,\n",
       "   'yes a different vides flexibility about which parse represents its kind of efficient parser can be built for this case class'),\n",
       "  (0.0,\n",
       "   'wittenburg assumes only one reading semantically so just one of its anala ccg fragment lacking orderchanging or higher yses fg is discovered while parsing')],\n",
       " [(0.0,\n",
       "   'we propose that by creating algorithms that optimize the evaluation criterion rather than some related criterion improved performance can be achieved'),\n",
       "  (0.0,\n",
       "   'we present two new algorithms the labelled recall algorithm which maximizes the expected labelled recall rate and the bracketed recall algorithm which maximizes the bracketed recall rate'),\n",
       "  (0.0,\n",
       "   'we have used the technique outlined in this paper in other work goodman to efficiently parse the dop model in that model the only previously known algorithm which summed over all the possible derivations was a slow monte carlo algorithm bod')],\n",
       " [(0.0,\n",
       "   'with a beam search strategy parsing speed can be improved to over sentences a minute with negligible loss in accuracy'),\n",
       "  (0.0,\n",
       "   'we use the parseval measures black et al to compare performance number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse of constituents which violate constituent boundaries with a constituent in the treebank parse'),\n",
       "  (0.0,\n",
       "   'we use the notation to state that the jth word in the reduced sentence is a modifier to the hith word with relationship r')],\n",
       " [(0.0,\n",
       "   'with few exceptions they rely on intuitive analyses of topic structure operational definitions of discourselevel properties eg interpreting paragraph breaks as discourse segment boundaries or theoryneutral discourse segmentations where subjects are given instructions to simply mark changes in topic'),\n",
       "  (0.0,\n",
       "   'with a view toward automatically segmenting a spoken discourse we would like to directly classify phrases of all three discourse categories'),\n",
       "  (0.0,\n",
       "   'while scont phrases for both speaking styles exhibited significantly shorter preceding and subsequent pauses than other phrases only the spontaneous condition showed a significantly slower rate')],\n",
       " [(0.0,\n",
       "   'written anew it probably would have been about lines characterize the relative performance of two techniques it is necessary to consider multiple training set sizes and to try both bigram and trigram models'),\n",
       "  (0.0,\n",
       "   'while the original paper katz uses a single parameter k we instead use a different k for each n gt kn'),\n",
       "  (0.0,\n",
       "   'while smoothing is a central issue in language modeling the literature lacks a definitive comparison between the many existing techniques')],\n",
       " [(0.0,\n",
       "   'when using sample selection a learning program examines many unlabeled not annotated examples selecting for labeling only those that are most informative for the learner at each stage of training seung opper and sompolinsky freund et al lewis and gale cohn atlas and ladner'),\n",
       "  (0.0,\n",
       "   'when training a bigram model indeed any hmm this is not true as each word is dependent on that before it'),\n",
       "  (0.0,\n",
       "   'when the statistics for a parameter are insufficient the variance of the posterior distribution of the estimates is large and hence there will be large differences in the values of the parameter chosen for different committee members')],\n",
       " [(0.0,\n",
       "   'when parsing the pos tags allowed for each word are limited to those which have been seen in training data for that word'),\n",
       "  (0.0,\n",
       "   'we use the parseval measures black et al to compare performance number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse crossing brackets number of constituents which violate constituent boundaries with a constituent in the treebank parse'),\n",
       "  (0.0,\n",
       "   'we intend to perform experiments to compare the perplexity of the various models and a structurally similar pure pcfg')],\n",
       " [(0.0,\n",
       "   'yarowsky yarowsky proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus thus avoided the need to handannotate any examples'),\n",
       "  (0.0,\n",
       "   'wsd is useful in many natural language tasks such as choosing the correct word in machine translation and coreference resolution'),\n",
       "  (0.0,\n",
       "   'word is the word related to w via the dependency relationship and posit ion can either be head or mod')],\n",
       " [(0.0,\n",
       "   'with its distant orbit percent farther from the sun than earth and slim atmospheric blanket mars experiences frigid weather conditions surface temperatures typically average about degrees celsius degrees fahrenheit at the equator and can dip to degrees c near the poles only the midday sun at tropical latitudes is warm enough to thaw ice on occasion but any liquid water formed in this way would evaporate almost instantly because of the low atmospheric pressure although the atmosphere holds a small amount of water and waterice clouds sometimes develop most martian weather involves blowing dust or carbon dioxide each win terfor example a blizzard of frozen carbon dioxide rages over one pole and a few meters of this dryice snow accumulate as previously frozen carbon dioxide evaporates from the opposite polar cap yet even on the summer pole where the sun remains in the sky all day long temperatures never warm enough to melt frozen waterm since parenthetical information is related only to the elementary unit that it belongs to we do not assign it an elementary textual unit status'),\n",
       "  (0.0,\n",
       "   'when we began this research no empirical data supported the extent to which this ambiguity characterizes natural language texts'),\n",
       "  (0.0,\n",
       "   'we used the valid clause boundaries assigned by judges as indicators of discourse usages of cue phrases and we determined manually the cue phrases that signalled a discourse relation')],\n",
       " [(0.0,\n",
       "   'with this approach the english sound k corresponds to one of t ka ki ku r ke or ko depending on its context'),\n",
       "  (0.0,\n",
       "   'while it is difficult to judge overall accuracysome of the phases are onomatopoetic and others are simply too hard even for good human translatorsit is easier to identify system weaknesses and most of these lie in the pw model'),\n",
       "  (0.0,\n",
       "   'when word separators are removed from the katakana phrases rendering the task exceedingly difficult for people the machines performance is unchanged')],\n",
       " [(0.0,\n",
       "   'while no direct indicators of positive or negative semantic orientation have been proposed we demonstrate that conjunctions between adjectives provide indirect information about orientation'),\n",
       "  (0.0,\n",
       "   'we were unable to reach a unique label out of context for several adjectives which we removed from consideration for example cheap is positive if it is used as a synonym of inexpensive but negative if it implies inferior quality'),\n",
       "  (0.0,\n",
       "   'we want to select the partition pmin that minimizes i subject to the additional constraint that for each adjective x in a cluster c where c is the complement of cluster c ie the other member of the partition')],\n",
       " [(0.0,\n",
       "   'without the ability to calculate performance over subdialogues it would be impossible to test the effect of the different presentation strategies independently of the different confirmation strategies'),\n",
       "  (0.0,\n",
       "   'whenever an attribute value in a dialogue ie data avm matches the value in its scenario key the number in the appropriate diagonal cell of the matrix boldface for clarity is incremented by'),\n",
       "  (0.0,\n",
       "   'when there is total agreement k ic is superior to other measures of success such as transaction success danieli and gerbino concept accuracy simpson and fraser and percent agreement gale church and yarowsky because n takes into account the inherent complexity of the task by correcting for chance expected agreement')],\n",
       " [(0.0,\n",
       "   'yet despite the disparity in initial segmentation scores the transformation sequences effect a significant error reduction in all cases which indicates that the transformation sequences are effectively able to compensate to some extent for weaknesses in the lexicon'),\n",
       "  (0.0,\n",
       "   'wu and fung demonstrate that segmentation accuracy is significantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested'),\n",
       "  (0.0,\n",
       "   'word segmentation can easily be cast as a transformationbased problem which requires an initial model a goal state into which we wish to transform the initial model the gold standard and a series of transformations to effect this improvement')],\n",
       " [(0.0,\n",
       "   'word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences'),\n",
       "  (0.0,\n",
       "   'with the exception of fung b previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts gale amp church kumano amp hirakawa fung a melamed'),\n",
       "  (0.0,\n",
       "   'where the onetoone assumption failed but a link type captured part of a correct translation it was judged incomplete whether incomplete links are correct or incorrect depends on the application')],\n",
       " [(0.0,\n",
       "   'we used the datasets created by rm for np learning their results are shown in table the fo difference is small yet they use a richer feature set which incorporates lexical information as well'),\n",
       "  (0.0,\n",
       "   'we use a simple constraint propagation algorithm that finds the best choice of nonoverlapping candidates in an input sentence ber of patterns and average length in the training data'),\n",
       "  (0.0,\n",
       "   'we thus compute for the set of all covers of a candidate c the each of these items gives an indication regarding the overall strength of the coverbased evidence for the candidate')],\n",
       " [(0.0,\n",
       "   'while the vilain provides intuitive results for coreference scoring it however does not work as well in the context of evaluating cross document coreference'),\n",
       "  (0.0,\n",
       "   'we note that this is simply one fewer than the number of elements in the partition that is ms ipsi'),\n",
       "  (0.0,\n",
       "   'we had mentioned earlier that the error of linking the the two large chains in the second response is more damaging than the error of linking one of the large chains with the smaller chain in the first response')],\n",
       " [(0.0,\n",
       "   'with the exception of the example sentence extraction component all the software modules are highly interactive and have substantial user interface requirements'),\n",
       "  (0.0,\n",
       "   'we are building a constituent type identifier which will semiautomatically assign grammatical function gf and phrase type pt attributes to these femarked constituents eliminating the need for annotators to mark these'),\n",
       "  (0.0,\n",
       "   'used in nlp the framenet database should make it possible for a system which finds a valencebearing lexical item in a text to know for each of its senses where its individual arguments are likely to be found')],\n",
       " [(0.0,\n",
       "   'whereas the transformationbased tagger enforces multiple constraints by having multiple rules fire the maximumentropy tagger can have all of these constraints play a role at setting the probability estimates for the models parameters'),\n",
       "  (0.0,\n",
       "   'when the tag distribution for a context has low entropy it is a very good predictor of the correct tag when the identical environment occurs in unseen data'),\n",
       "  (0.0,\n",
       "   'we used a version of examplebased learning to determine whether these tagger differences could be exploited to determine examplebased learning has also been applied succesfully in building a single part of speech tagger the tag of a word we use the previous word current word next word and the output of each tagger for the previous current and next word')],\n",
       " [(0.0,\n",
       "   'while we are investigating the latter in current work local repair heuristics have the advantage of keeping the training and bracketing algorithms both simple and fast'),\n",
       "  (0.0,\n",
       "   'while previous empirical methods for base np identification have been rather complex this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task'),\n",
       "  (0.0,\n",
       "   'while initial results are promising the full impact of such heuristics on overall performance can be determined only if they are systematically learned and tested using available training data')],\n",
       " [(0.0,\n",
       "   'we then scan all the derivations in the development set and for each occurrence of the elementary event ym xm in derivationwtk we accumulate the value w tk in the cmym xm counter to be used in the next iteration'),\n",
       "  (0.0,\n",
       "   'we achieved a reduction in testdata perplexity bringing an improvement over a deleted interpolation trigram model whose perplexity was on the same trainingtest data the reduction is statistically significant according to a sign test'),\n",
       "  (0.0,\n",
       "   'w t neednt be a constituent but for the parses where it is there is no restriction on which of its words is the headword or what is the nonterminal label that accompanies the headword')],\n",
       " [(0.0,\n",
       "   'z system starts with a basic corpus annotation each word is tagged with its most likely tag and then searches through a space of transformation rules in order to reduce the discrepancy between its current annotation and the correct one in our case rules were learned'),\n",
       "  (0.0,\n",
       "   'when used on test the pairwise voting strategy tagpair clearly outperforms the other voting strategies but does not yet approach the level where all tying majority votes are handled correctly'),\n",
       "  (0.0,\n",
       "   'when abstracting away from individual tags precision and recall are equal and measure how many tokens are tagged correctly in this case we also use the more generic term accuracy')],\n",
       " [(0.0,\n",
       "   'with simwn and simroget we transform wordnet and roget into the same format as the automatically constructed thesauri in the previous section'),\n",
       "  (0.0,\n",
       "   'while previous methods rely on indirect tasks or subjective judgments our method allows direct and objective comparison between automatically and manually constructed thesauri'),\n",
       "  (0.0,\n",
       "   'when w r or w is the wild card the frequency counts of all the dependency triples that matches the rest of the pattern are summed up')],\n",
       " [(0.0,\n",
       "   'while we will be given the candidate attachment sites during testing the training procedure assumes no a priori information about potential attachment sites'),\n",
       "  (0.0,\n",
       "   'while this form of attachment ambiguity is usually easy for people to resolve a computer requires detailed knowledge about words eg washed vs bought in order to successfully resolve such ambiguities and predict the correct interpretation'),\n",
       "  (0.0,\n",
       "   'while the tuple num to num is more frequent than rise to num the conditional probabilities prefer a v which is the choice that maximizes pr v n p a')],\n",
       " [(0.0,\n",
       "   'while we know that the head of the entire compound is carrier in order to properly evaluate the word in question we must determine which of the words following it is its head'),\n",
       "  (0.0,\n",
       "   'whereas the ramps algorithm produced just terms not already present in wordnet for the two categories combined our algorithm produced or over for every valid terms produced'),\n",
       "  (0.0,\n",
       "   'we will also present some experimental results from two corpora and discuss criteria for judging the quality of the output')],\n",
       " [(0.0,\n",
       "   'with respect to any two discourse entities x uttx pas x and y utty posy uttx and utty specifying the current utterance ui or the preceding utterance u i set up the following ordering constraints on elements in the slist table'),\n",
       "  (0.0,\n",
       "   'while tensed clauses are defined as utterances on their own untensed clauses are processed with the main clause so that the cflist of the main clause contains the elements of the untensed embedded clause'),\n",
       "  (0.0,\n",
       "   'when the pronoun her in lc is encountered friedman is the first element of the slist since friedman is unused and in the current utterance')],\n",
       " [(0.0,\n",
       "   'when grouped by average performance they fell into several coherent classes which corresponded to the extent to which the functions focused on the intersection of the supports regions of positive probability of the distributions'),\n",
       "  (0.0,\n",
       "   'we would not be able to tell whether the cause was an inherent deficiency in the l norm or just a poor choice of weight function perhaps li q r would have yielded better estimates'),\n",
       "  (0.0,\n",
       "   'we treat a value of as indicating extreme dissimilarity it is worth noting at this point that there are several wellknown measures from the nlp literature that we have omitted from our experiments')],\n",
       " [(0.0,\n",
       "   'ww states that w appears in the patterns ab as a whole while pp states that p appears as a part'),\n",
       "  (0.0,\n",
       "   'words that appear in only one of the two patterns are suspect but to use this rule we need sufficient counts on the good words to be sure we have a representative sample'),\n",
       "  (0.0,\n",
       "   'while invariance with respect to frequency is generally a good property such invariant metrics can lead to bad results when used with sparse data')],\n",
       " [(0.0,\n",
       "   'when compared to levin s toplevel verb classes we found an agreement of our classification with her class of verbs of changes of state except for the last three verbs in the list in fig'),\n",
       "  (0.0,\n",
       "   'we will sketch an understanding of the lexical representations induced by latentclass labeling in terms of the linguistic theories mentioned above aiming at an interpretation which combines computational learnability linguistic motivation and denotationalsemantic adequacy'),\n",
       "  (0.0,\n",
       "   'we use a statistical subcatinduction system which estimates probability distributions and corpus frequencies for pairs of a head and a subcat frame carroll and rooth')],\n",
       " [(0.0,\n",
       "   'wordnet has been an important research tool but it is insufficient for domainspecific text such as that encountered in the mucs message understanding conferences'),\n",
       "  (0.0,\n",
       "   'within those columns majority lists the opinion of the majority of judges and any indicates the hypernyms that were accepted by even one of the judges'),\n",
       "  (0.0,\n",
       "   'with the addition of some improvements we have identified we believe that these automatic methods can be used to construct truly useful hierarchies')],\n",
       " [(0.0,\n",
       "   'while their kappa results are very good for other tags the opinionstatement tagging was not very successful the distinction was very hard to make by labelers and accounted for a large proportion of our interlabeler error jurafsky et al'),\n",
       "  (0.0,\n",
       "   'when there is such evidence we propose using the latent class model to correct the disagreements this model posits an unobserved latent variable to explain the correlations among the judges observations'),\n",
       "  (0.0,\n",
       "   'when disagreement is symmetric the differences between the actual counts and the counts expected if the judges decisions were not correlated are symmetric that is snii for i j where i is the difference from independence')],\n",
       " [(0.0,\n",
       "   'when the frequency count is reasonably large eg greater than a bin zn we further assume that the estimations of pa pbia and pcia in are accurate'),\n",
       "  (0.0,\n",
       "   'we use the following condition to determine whether or not a collocation is compositional a collocation a is noncompositional if there does not exist another collocation such that a is obtained by substituting the head or the modifier in a with a similar word and b there is an overlap between the confidence interval of the mutual information values of a and'),\n",
       "  (0.0,\n",
       "   'we use is ft i to denote the frequency count of all the collocations that match the pattern h r m where h and m are either words or the wild card and r is either a dependency type or the wild card')],\n",
       " [(0.0,\n",
       "   'with respect to ease of answer key preparation pampr and autsent are clearly superior since they use the publisherprovided answer key'),\n",
       "  (0.0,\n",
       "   'with proper choice of test material it should be possible to challenge systems to successively higher levels of performance'),\n",
       "  (0.0,\n",
       "   'why questions are by far the hardest performance around because they require understanding of rhetorical structure and because answers tend to be whole clauses often occurring as standalone sentences rather than phrases embedded in a context that matches the query closely')],\n",
       " [(0.0,\n",
       "   'while examining the si extractions we found many similar nps for example the salvadoran government the guatemalan government and the us government the similarities indicate that some head nouns when premodified represent existential entities'),\n",
       "  (0.0,\n",
       "   'when we say that a definite np is existential we say this because it completely specifies a cognitive representation of the entity in the readers mind'),\n",
       "  (0.0,\n",
       "   'when we combined all three methods si ehp and do recall increased to without any corresponding loss of precision')],\n",
       " [(0.0,\n",
       "   'within the corpus each word was annotated with all of the pos tags that would be possible given its spelling using the output of a morphological analysis program and also with the single one of those tags that a statistical pos tagging program had predicted to be the correct tag hake and hladka'),\n",
       "  (0.0,\n",
       "   'when tested with the final version of the parser on the full development set those two strategies performed at the same level'),\n",
       "  (0.0,\n",
       "   'we write nonterminals as x x x is the nonterminal label and x is a w t pair where w is the associated headword and t as the pos tag')],\n",
       " [(0.0,\n",
       "   'with the resulting vector we now perform a similarity computation to all vectors in the cooccurrence matrix of the target language'),\n",
       "  (0.0,\n",
       "   'whereas for parallel texts in some studies up to of the word alignments have been shown to be correct the accuracy for nonparallel texts has been around up to now'),\n",
       "  (0.0,\n",
       "   'when comparing an english and a german cooccurrence matrix of corresponding words he found a high correlation between the cooccurrence patterns of the two matrices when the rows and columns of both matrices were in corresponding word order and a low correlation when the rows and columns were in random order')],\n",
       " [(0.0,\n",
       "   'yes by both judges this figure is recall is estimated as the number of pairs that should have been judged good by strand ie that recieved a yes from both judges that strand indeed marked good this figure is'),\n",
       "  (0.0,\n",
       "   'using the cases where the two human judgments agree as ground truth precision of the system is estimated at and recall at'),\n",
       "  (0.0,\n",
       "   'using figures from the babel survey of multilinguality on the web http fwww isoc org it is possible to estimate that as of june there were on the order of primarily nonenglish web servers ranging over languages')],\n",
       " [(0.0,\n",
       "   'with a small number of features the correctparses estimator typically scores better than the pseudolikelihood estimator on the correctparses evaluation metric but the pseudolikelihood estimator always scores better on the pseudolikelihood evaluation metric'),\n",
       "  (0.0,\n",
       "   'while it is possible to construct ubgs for which the number of possible parses is unmanageably high for many grammars it is quite manageable to enumerate the set of possible parses and thereby directly evaluate eo fwiy w yi'),\n",
       "  (0.0,\n",
       "   'we would have liked to have included features concerning specific lexical items to capture headtohead dependencies but we felt that our corpora were so small that the associated parameters could not be accurately estimated')],\n",
       " [(0.0,\n",
       "   'while we used circumstantials to illustrate the issues we also handled revision for a variety of other categories in the same manner'),\n",
       "  (0.0,\n",
       "   'while we have tuned the system to perform with minor errors on the manual set of themes we have created the missing article in the fourth sentence of the summary in figure is an example we need more robust input data from the theme construction component which is still under development to train the generator before beginning large scale testing'),\n",
       "  (0.0,\n",
       "   'while some researchers address this problem by selecting a subset of the repetitions carbonell and goldstein this approach is not always satisfactory')],\n",
       " [(0.0,\n",
       "   'work on the english corpus was supported under the nsfcri grant towards a comprehensive linguistic annotation of language and the nsfint project sustainable interoperability for language technology silt funded by the national science foundation'),\n",
       "  (0.0,\n",
       "   'with the task decomposition allowed by bat it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful'),\n",
       "  (0.0,\n",
       "   'which gets us in a position to present the list of task introduced by tempeval including some motivation as to why we feel that it is a good idea to split up temporal relation classification into sub tasks')],\n",
       " [(0.0,\n",
       "   'word senses are more beneficial than simple word forms for a variety of tasks including information retrieval machine translation and others pantel and lin'),\n",
       "  (0.0,\n",
       "   'when hsk is the solution is perfectly homogeneous because each cluster only contains data points that belong to a single class'),\n",
       "  (0.0,\n",
       "   'when hks is the solution is perfectly complete because all data points of a class belong to the same cluster')],\n",
       " [(0.0,\n",
       "   'wiki pages the task consists of automatically detecting and resolving differences in the information they provide in order to produce aligned mutually enriched versions of the two documents'),\n",
       "  (0.0,\n",
       "   'we report on the training and test data used for evaluation the process of their creation the participating systems teams runs the approaches adopted and the results achieved'),\n",
       "  (0.0,\n",
       "   'towards this objective a crucial requirement is to identify the information in one page that is either equivalent or novel more informative with respect to the content of the other')],\n",
       " [(0.0,\n",
       "   'we would like to thank inderjeet mani wlodek zadrozny rie kubota ando joyce chai and nanda kambhatla for their valuable feedback'),\n",
       "  (0.0,\n",
       "   'we would also like to thank carl sable minyen kan dave evans adam budzikowski and veronika horvath for their help with the evaluation'),\n",
       "  (0.0,\n",
       "   'we would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization')],\n",
       " [(0.0,\n",
       "   'we would like to be able to incorporate semantics for an arbitrarily large number of words and lsa quickly becomes impractical on large sets'),\n",
       "  (0.0,\n",
       "   'we introduce a semanticbased algorithm for learning morphology which only proposes affixes when the stem and stemplusaffix are sufficiently similar semantically'),\n",
       "  (0.0,\n",
       "   'we insert words into a trie figure and extract potential affixes by observing those places in the trie where branching occurs')],\n",
       " [(0.0,\n",
       "   'whereas earlier methods all share the same basic intuition ie that similar words occur in similar contexts i formalise this in a slightly different way each word defines a probability distribution over all contexts namely the probability of the context given the word'),\n",
       "  (0.0,\n",
       "   'we can then measure the similarity of words by the similarity of their context distributions using the kullbackleibler kl divergence as a distance function'),\n",
       "  (0.0,\n",
       "   'we can model the context distribution as being the product of independent distributions for each relative position in this case the kl divergence is the sum of the divergences for each independent distribution')],\n",
       " [(0.0,\n",
       "   'while the bbn model does not perform at the level of model of collins on wall street journal text it is also less languagedependent eschewing the distance metric which relied on specific features of the english treebank in favor of the bigrams on nonterminals model'),\n",
       "  (0.0,\n",
       "   'while results for the two languages are far from equal we believe that further tuning of the head rules and analysis of development test set errors will yield significant performance gains on chinese to close the gap'),\n",
       "  (0.0,\n",
       "   'while more investigation is required we suspect part of the difference may be due to the fact that currently the bbn model uses languagespecific rules to guess part of speech tags for unknown words')],\n",
       " [(0.0,\n",
       "   'within this classification the greatest hopes for tagging improvement appear to come from minimizing errors in the second and third classes of this classification'),\n",
       "  (0.0,\n",
       "   'while progress is slow because each new feature applies only to a limited range of cases nevertheless the improvement in accuracy as compared to previous results is noticeable particularly for the individual decisions on which we focused'),\n",
       "  (0.0,\n",
       "   'we will not discuss in detail the characteristics of the model or the parameter estimation procedure used improved iterative scaling')],\n",
       " [(0.0,\n",
       "   'with the input stimulusfsn the output is stimuli rather than the incorrect stimuluses that would follow from the application of the more general rule in'),\n",
       "  (0.0,\n",
       "   'when the flex rule matches the input addressiv for example the c function npvordform defined elsewhere in the generator is called to determine the word form corresponding to the input the function deletes the inflection type and pos label specifications and the delimiters removes the last character of the lemma and finally attaches the characters es the word form generated is thus addresses'),\n",
       "  (0.0,\n",
       "   'we present such a component a fast and robust morphological generator for english based on finitestate techniques that generates a word form given a specification of the lemma partofspeech and the type of inflection required')],\n",
       " [(0.0,\n",
       "   'while this result is satisfying further investigation reveals that deterioration in the quality of the labeled data accumulated by cotraining hinders further improvement'),\n",
       "  (0.0,\n",
       "   'while previous research summarized in section has investigated the theoretical basis of cotraining this study is motivated by practical concerns'),\n",
       "  (0.0,\n",
       "   'we seek to apply the cotraining paradigm to problems in natural language learning with the goal of reducing the amount of humanannotated data required for developing natural language processing components')],\n",
       " [(0.0,\n",
       "   'when a word maps to a general mesh term like treatment y zeros are appended to the end of the descriptor to stand in place of the missing values so for example treatment in model is y and in model is y etc'),\n",
       "  (0.0,\n",
       "   'we want to support relationships between entities that are shown to be important in cognitive linguistics in particular we intend to support the kinds of inferences that arise from talmys force dynamics talmy'),\n",
       "  (0.0,\n",
       "   'we used a feedforward network trained with conjugate gradient descent number corresponds to the level of the mesh hierarchy used for classification')],\n",
       " [(0.0,\n",
       "   'yet we can filter by pruning ngrams whose beginning or ending word is among the top n most frequent words'),\n",
       "  (0.0,\n",
       "   'yet there is still another maybe or the reciprocal of the words frequencies semantic compositionality is not always bad'),\n",
       "  (0.0,\n",
       "   'with permission and sufficient time one can repeatedly query websites that host large collections of mrds and evaluate each ngram')],\n",
       " [(0.0,\n",
       "   'xu and croft offered a trainable method call local context analysis lca which replaces each query term with frequently cooccurring words'),\n",
       "  (0.0,\n",
       "   'we discovered that lsa is a more accurate measure of similarity than the cosine metric stemming does not always improve segmentation accuracy and ranking is crucial to cosine but not lsa'),\n",
       "  (0.0,\n",
       "   'this procedure is useful in information retrieval hearst and plaunt hearst yaari reynar summarisation reynar text understanding anaphora resolution kozima language modelling morris and hirst beeferman et al and text navigation choi b')],\n",
       " [(0.0,\n",
       "   'while this has allowed for quantitative comparison of parsing techniques it has left open the question of how other types of text might affect parser performance and how portable parsing models are across corpora'),\n",
       "  (0.0,\n",
       "   'while this does not reflect the stateoftheart performance on the wsj task achieved by the more the complex models of charniak and collins we regard it as a reasonable baseline for the investigation of corpus effects on statistical parsing'),\n",
       "  (0.0,\n",
       "   'we examine these questions by comparing results for the brown and wsj corpora and also consider which parts of the parsers probability model are particularly tuned to the corpus on which it was trained')],\n",
       " [(0.0,\n",
       "   'when assigning timestamps we analyze both implicit time references mainly through the tense system and explicit ones temporal adverbials such as on monday in etc'),\n",
       "  (0.0,\n",
       "   'when assigning a time to an event we select the time to be either the most recently assigned date or if the value of the most recently assigned date is undefined to the date of the article'),\n",
       "  (0.0,\n",
       "   'we ran the timestamper program on two types of data list of eventclauses extracted by the event identifier and list of eventclauses created manually')],\n",
       " [(0.0,\n",
       "   'we used nltk as a basis for the assignments and student projects in cis an introductory computational linguistics class taught at the university of pennsylvania'),\n",
       "  (0.0,\n",
       "   'we hope that nltk will allow computational linguistics classes to include more handson experience with using and building nlp components and systems'),\n",
       "  (0.0,\n",
       "   'we describe nltk the natural language toolkit which we have developed in conjunction with a course we have taught at the university of pennsylvania')],\n",
       " [(0.0,\n",
       "   'while polynomial kernels in the svm learning can effectively generate feature conjunctions kernel functions that can effectively generate feature disjunctions are not known'),\n",
       "  (0.0,\n",
       "   'we used a feature set hw pre suf sub posipc and the inner product kernel the training time was measured on a machine with four mhz pentiumiiis and gb ram'),\n",
       "  (0.0,\n",
       "   'we use the maximum entropy tagging method described in kazama et al for the experiments which is a variant of ratnaparkhi modified to use hmm state features')],\n",
       " [(0.0,\n",
       "   'word induction from natural language text without word boundaries is also studied in deligne and bimbot hua where mdlbased model optimization measures are used'),\n",
       "  (0.0,\n",
       "   'whether this is due to the cost function or the splitting strategy cannot be deduced based on these experiments'),\n",
       "  (0.0,\n",
       "   'when the vocabulary is very large say words the basic problems in the estimation of the language model are if words are used as basic representational units in the language model the number of basic units is very high and the estimated word ngrams are poor due to sparse data')],\n",
       " [(0.0,\n",
       "   'with that goal open mind word expert tracks for each contributor the number of items tagged for each topic'),\n",
       "  (0.0,\n",
       "   'with open mind word expert we aim at creating a very large sense tagged corpus by making use of the incredible resource of knowledge constituted by the millions of web users combined with techniques for active learning'),\n",
       "  (0.0,\n",
       "   'while the first two sources are well known to the nlp community the open mind common sense constitutes a fairly new textual corpus')],\n",
       " [(0.0,\n",
       "   'with this computational means at hand we can now measure the spelling similarity between every german and english word and sort possible word pairs accordingly'),\n",
       "  (0.0,\n",
       "   'when words are adopted into another language their spelling might change slightly in a manner that can not be simply generalized in a rule'),\n",
       "  (0.0,\n",
       "   'we examined the german words in our lexicon and tried to find english words that have the exact same spelling')],\n",
       " [(0.0,\n",
       "   'with this gold standard in place it is possible to use precision and recall measures to evaluate the quality of the extracted thesaurus'),\n",
       "  (0.0,\n",
       "   'when these are used with weighted attributes if the weight is greater than zero then it is considered in the set'),\n",
       "  (0.0,\n",
       "   'we would like to thank stephen clark caroline sporleder tara murphy and the anonymous reviewers for their comments on drafts of this paper')],\n",
       " [(0.0,\n",
       "   'yet the results shown in line of figure are relatively poor the adjectives provide less useful information than unigram presence'),\n",
       "  (0.0,\n",
       "   'while the tie rates suggest that the brevity of the humanproduced lists is a factor in the relatively poor performance results it is not the case that size alone necessarily limits accuracy'),\n",
       "  (0.0,\n",
       "   'what accounts for these two differences difficulty and types of information proving useful between topic and sentiment classification and how might we improve the latter')],\n",
       " [(0.0,\n",
       "   'while metabootstrapping trusts individual extraction patterns to make unilateral decisions basilisk gathers collective evidence from a large set of extraction patterns'),\n",
       "  (0.0,\n",
       "   'we will use the abbreviation cat to indicate that only one semantic category was bootstrapped and mcat to indicate that multiple semantic categories were simultaneously bootstrapped'),\n",
       "  (0.0,\n",
       "   'we were surprised that the improvement for metabootstrapping was much we also measured the recall of basilisks lexicons after words had been learned based on the gold standard data shown in table')],\n",
       " [(0.0,\n",
       "   'yarowsky et al states that during their work on noun phrase bracketing they found a strong cohesion among noun phrases even when comparing english to czech a relatively free word order language'),\n",
       "  (0.0,\n",
       "   'while reflecting genuine ambiguity an smt system would likely pursue only one of the alternatives and only a portion of the crossings would come into play'),\n",
       "  (0.0,\n",
       "   'when two annotators disagree the union of the p alignments produced by each annotator is recorded as the p alignment in the corpus')],\n",
       " [(0.0,\n",
       "   'while grammar development is carried out in the lkb copestake processing both in the application domain and for the purposes of running test suites is done with the highly efficient pet parser callmeier'),\n",
       "  (0.0,\n",
       "   'when an argument is realized its sat value on the mother node is specified as sat and its synsem is unified with its val value on the subcategorizing head'),\n",
       "  (0.0,\n",
       "   'we model this within the sign by positing a feature empathy within context and linking it to the relevant arguments indices')],\n",
       " [(0.0,\n",
       "   'work is also progressing on establishing a standard relational database using postgresql for storing information for the lexical entries themselves improving both scalability and clarity compared to the current simple text file representation'),\n",
       "  (0.0,\n",
       "   'while the development of the matrix will be built largely on the lkb platform support will also be needed for using the emerging grammars on other processing platforms and for linking to other packages for preprocessing the linguistic input'),\n",
       "  (0.0,\n",
       "   'while the details of modifier placement which parts of speech can modify which kinds of phrases etc differ across languages we believe that all languages display a distinction between scopal and intersective modification')],\n",
       " [(0.0,\n",
       "   'within lfg fstructures are meant to encode a language universal level of analysis allowing for crosslinguistic parallelism at this level of abstraction'),\n",
       "  (0.0,\n",
       "   'when this is noticed via the featuretable comparison it is determined why one grammar needs the feature and the other does not and thus it may be possible to eliminate the feature in one grammar or to add it to another'),\n",
       "  (0.0,\n",
       "   'we report on the parallel grammar pargram project which uses the xle parser and grammar development platform for six languages english')],\n",
       " [(0.0,\n",
       "   'we used sentences from the articles on january st to january th as training examples and sentences from the articles on january th as the test data'),\n",
       "  (0.0,\n",
       "   'we propose a new method that is simple and efficient since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side'),\n",
       "  (0.0,\n",
       "   'we presented a new japanese dependency parser using a cascaded chunking model which achieves accuracy using the kyoto university corpus')],\n",
       " [(0.0,\n",
       "   'while the loglikelihood function for me models in is twice differentiable for large scale problems the evaluation of the hessian matrix is computationally impractical and newtons method is not competitive with iterative scaling or first order methods'),\n",
       "  (0.0,\n",
       "   'while parameter estimation for me models is conceptually straightforward in practice me models for typical natural language tasks are very large and may well contain many thousands of free parameters'),\n",
       "  (0.0,\n",
       "   'while parameter estimation for me models is conceptually straightforward in practice me models for typical natural language tasks are usually quite large and frequently contain hundreds of thousands of free parameters')],\n",
       " [(0.0,\n",
       "   'weaksubjective was used for three situations words that have weak subjective connotations such as aberration which implies something out of the ordinary but does not evoke a strong sense of judgement words that have multiple senses or uses where one is subjective but the other is not'),\n",
       "  (0.0,\n",
       "   'we used the nouns as feature sets rather than define a separate feature for each word so the classifier could generalize over the set to minimize sparse data problems'),\n",
       "  (0.0,\n",
       "   'we used a separate annotated tuning corpus of documents with a total of sentences to establish some experimental parameters')],\n",
       " [(0.0,\n",
       "   'with this type of agglomerative clustering the most similar pages are clustered first and outliers are assigned as stragglers at the top levels of the cluster tree'),\n",
       "  (0.0,\n",
       "   'while word senses and translation ambiguities may typically have alternative meanings that must be resolved through context a personal name such as jim clark may potentially refer to hundreds or thousands of distinct individuals'),\n",
       "  (0.0,\n",
       "   'while these names could be used in a undifferentiated vectorbased bagofwords model further accuracy can be gained by extracting specific types of association such as familial relationships eg son wife employment relationships eg manager of and nationality as distinct from simple term cooccurrence in a window')],\n",
       " [(0.0,\n",
       "   'we would like to thank the anonymous reviewers for their helpful comments and also iain rae for computer support'),\n",
       "  (0.0,\n",
       "   'we trained one of the taggers on much more labelled seed data than the other to see how this affects the cotraining process'),\n",
       "  (0.0,\n",
       "   'we leave these experiments to future work but note that there is a large computational cost associated with such experiments')],\n",
       " [(0.0,\n",
       "   'zhou and su used a wide variety of features which suggests that the relatively poor performance of the taggers used in conll was largely due to the feature sets used rather than the machine learning method'),\n",
       "  (0.0,\n",
       "   'we used three data sets the english and german data for the conll shared task tjong kim sang and de meulder and the dutch data for the conll shared task tjong kim sang'),\n",
       "  (0.0,\n",
       "   'we report reasonable precision and recall fscore for the conll english test data and an fscore of for the conll german test data')],\n",
       " [(0.0,\n",
       "   'within each of the regions a statistical bigram language model is used to compute the likelihood of words occurring within that region named entity type'),\n",
       "  (0.0,\n",
       "   'when no gazetteer or other additional training resources are used the combined system attains a performance of f on the english development data integrating name location and person gazetteers and named entity systems trained on additional more general data reduces the fmeasure error by a factor of to on the english data'),\n",
       "  (0.0,\n",
       "   'we note that the numbers are roughly twice as large for the development data in german as they are for english')],\n",
       " [(0.0,\n",
       "   'when using characterlevel models for wordevaluated tasks one would not want multiple characters inside a single word to receive different labels'),\n",
       "  (0.0,\n",
       "   'we present two models in which the basic units are characters and character grams instead of words and word phrases'),\n",
       "  (0.0,\n",
       "   'we found knowing that the previous word was an other wasnt particularly useful without also knowing its partofspeech eg a preceding preposition might indicate a location')],\n",
       " [(0.0,\n",
       "   'we used two variants of hmm hedge one which selects headline words from the first words of the story and one which selects words from the first sentence of the story'),\n",
       "  (0.0,\n",
       "   'we treat summarization as a type of translation from a verbose language to a concise one and compare automatically generated headlines to human generated headlines'),\n",
       "  (0.0,\n",
       "   'we should be able to quickly produce a comparable system for other languages especially in light of current multilingual initiatives that include automatic parser induction for new languages eg the tides initiative')],\n",
       " [(0.0,\n",
       "   'we will discuss tags and an important principle guiding their formation the extraction procedure from the ptb that is described in chen including extensions to extract a tag from the propbank and finally the extraction of deeper linguistic features from the resulting tag'),\n",
       "  (0.0,\n",
       "   'we use deep linguistic features to predict semantic roles on syntactic arguments and show that these perform considerably better than surfaceoriented features'),\n",
       "  (0.0,\n",
       "   'we suggest that the syntactic representation chosen in the ptb is less well suited for semantic processing than the other deeper syntactic representations')],\n",
       " [(0.0,\n",
       "   'we would expect a higher performance for the ccgbased system if the analyses in ccgbank resembled more closely those in propbank'),\n",
       "  (0.0,\n",
       "   'we speculate that much of the performance improvement we show could be obtained with traditional ie nonccgbased parsers if they were designed to recover more of the information present in the penn treebank in particular the trace coindexation'),\n",
       "  (0.0,\n",
       "   'we present a system for automatically identifying propbankstyle semantic roles based on the output of a statistical parser for combinatory categorial grammar')],\n",
       " [(0.0,\n",
       "   'we would like to thank john carroll for the use of his parser adam kilgarriff and bill keller for valuable discussions and the uk epsrc for its studentship to the first author'),\n",
       "  (0.0,\n",
       "   'we will refer to these verbs as the features of n f n where dnv is the degree of association between noun n and verb v possible association functions will be defined in the context of each model described below'),\n",
       "  (0.0,\n",
       "   'we will now consider some different possibilities for measuring the degree of association between a noun n and a verb v in the combinatorial model we simply consider whether a verb has ever been seen to cooccur with the noun')],\n",
       " [(0.0,\n",
       "   'when those sentences are removed the average pairwise percentage agreement increases to and the average pairwise r value increases to'),\n",
       "  (0.0,\n",
       "   'western countries were left frustrated and impotent after robert mugabe formally declared that he had overwhelmingly won zimbabwes presidential election annotators are also asked to judge the strength of each private state'),\n",
       "  (0.0,\n",
       "   'we have developed a bootstrapping process for subjectivity classification that explores three ideas highprecision classifiers can be used to automatically identify subjective and objective sentences from unannotated texts this data can be used as a training set to automatically learn extraction patterns associated with subjectivity and the learned patterns can be used to grow the training set allowing this entire process to be bootstrapped')],\n",
       " [(0.0,\n",
       "   'wiebe et al report that this expectation is borne out of the time for opinion documents and of the time for factual documents'),\n",
       "  (0.0,\n",
       "   'while words and ngrams had little performance effect for the opinion class they increased the recall for the fact class around five fold compared to the approach by wiebe et al'),\n",
       "  (0.0,\n",
       "   'while we have found presenting information organized in separate opinion and fact classes useful our goal is to introduce further analysis of each sentence so that opinion sentences can be linked to particular perspectives on a specific subject')],\n",
       " [(0.0,\n",
       "   'when pos patterns are used to extract potential terms the problem lies in how to restrict the number of terms and only keep the ones that are relevant'),\n",
       "  (0.0,\n",
       "   'when inspecting manually assigned keywords the vast majority turn out to be nouns or noun phrases with adjectives and as discussed in section the research on term extraction focuses on noun patterns'),\n",
       "  (0.0,\n",
       "   'when extracting the terms from the test set according to the ngram approach the data consisted of negative examples and positive examples thus in total examples were classified by the trained model')],\n",
       " [(0.0,\n",
       "   'while we are familiar with the approaches taken in several of the tested systems we leave it up to the individual participants to describe their approaches and hopefully elucidate which aspects of their approaches are most responsible for their successes and failures the participants papers all appear in this volume'),\n",
       "  (0.0,\n",
       "   'while the test sets turned out to be large enough to measure significant differences between systems in most cases a larger test set would allow even better statistics'),\n",
       "  (0.0,\n",
       "   'while it would be fairly straightforward in many cases to automatically score both alternatives we felt we could provide a more objective measure if we went strictly by the particular segmentation standard being tested on and simply did not get into the business of deciding upon allowable alternatives')],\n",
       " [(0.0,\n",
       "   'we have taken six tracks academia sinica closed asc u penn chinese tree bank open and closedctboc hong kong cityu closed hkc peking university open and closedpkoc'),\n",
       "  (0.0,\n",
       "   'we apply to word segmentation classbased hmm which is a generalized approach covering both common words and unknown words wi iff wi is listed in the segmentation lexicon per iff wi is unlisted personal name loc iff wi is unlisted location name org iff wi is unlisted organization name time iff wi is unlisted time expression num iff wi is unlisted numeric expression str iffwi is unlisted symbol string beg iff beginning of a sentence end iff ending of a sentence other otherwise'),\n",
       "  (0.0,\n",
       "   'we also thank richard sproat qing ma fei xia and other sighan colleagues for their elaborate organization and enthusiastic help in the first international chinese word segmentation bakeoff')],\n",
       " [(0.0,\n",
       "   'while these results may appear at first glance to be less than conclusive we must bear in mind that we are working with limited amounts of data and relatively simplistic models of a cognitively intensive task'),\n",
       "  (0.0,\n",
       "   'while the results are far from stable such variation is perhaps to be expected on a test like this since the nature of context space models means that rogue items sometimes get extremely high similarity scores and we are performing the regression over only vpcs vpccomponent pairs'),\n",
       "  (0.0,\n",
       "   'while method softens the reliance upon productivity as a test for compositionality it still confuses institutionalisation with noncompositionality somewhat in its reliance upon substitution')],\n",
       " [(0.0,\n",
       "   'whilst statistics are useful indicators of noncompositionality there are compositional multiwords which have low values for these statistics yet are highly noncompositional'),\n",
       "  (0.0,\n",
       "   'whilst it is possible to put every single occurrence of a verb and particle combination into a lexicon this is not desirable'),\n",
       "  (0.0,\n",
       "   'whether such an unexpectedly high cooccurrence frequency warrants an entry in the lexicon depends on the type of lexicon being built')],\n",
       " [(0.0,\n",
       "   'with simple decomposable mwes we can expect the constituents and particularly the head to be hypernyms ancestor nodes or synonyms of the mwe'),\n",
       "  (0.0,\n",
       "   'with nondecomposable mwes eg kick the bucket shoot the breeze hot dog no decompositional analysis is possible and the mwe is semantically impenetrable'),\n",
       "  (0.0,\n",
       "   'while taking note of the low correlation with wordnet similarities therefore we move straight on to look at the hyponymy test')],\n",
       " [(0.0,\n",
       "   'we will try to make this more precise in a minute but first we want to discuss the relation between incrementality and determinism'),\n",
       "  (0.0,\n",
       "   'we will represent parser configurations by triples i a where is the stack represented as a list i is the list of remaining input tokens and a is the current arc relation for the dependency graph'),\n",
       "  (0.0,\n",
       "   'we will formalize deterministic dependency parsing in a way which is inspired by traditional shiftreduce parsing for contextfree grammars using a buffer of input tokens and a stack for storing previously processed input')],\n",
       " [(0.0,\n",
       "   'with an average of five senses per word the average value for the agreement by chance is measured at resulting in a micro statistic of'),\n",
       "  (0.0,\n",
       "   'we measure two figures microaverage where number of senses agreement by chance and are determined as an average for all words in the set and macroaverage where intertagger agreement agreement by chance and are individually determined for each of the words in the set and then combined in an overall average'),\n",
       "  (0.0,\n",
       "   'we describe in this paper the task definition resources participating systems and comparative results for the english lexical sample task which was organized as part of the senseval evaluation exercise')],\n",
       " [(0.0,\n",
       "   'with the duc data we computed pearsons product moment correlation coefficients spearmans rank order correlation coefficients and kendalls correlation coefficients between systems average rouge scores and their human assigned average coverage scores using single reference and multiple references'),\n",
       "  (0.0,\n",
       "   'when multiple references are used we compute pairwise summarylevel rougen between a candidate summary s and every reference ri in the reference set'),\n",
       "  (0.0,\n",
       "   'when applying to summarylevel we take the union lcs matches between a reference summary sentence ri and every candidate summary sentence cj')],\n",
       " [(0.0,\n",
       "   'we thank chuck wooters don baron chris oei and andreas stolcke for software assistance ashley krupski for contributions to the annotation scheme andrei popescubelis for analysis and comments on a release of the meetings and barbara peskin and jane edwards for general advice and feedback'),\n",
       "  (0.0,\n",
       "   'we suggest various ways to group the large set of labels into a smaller set of classes depending on the research focus'),\n",
       "  (0.0,\n",
       "   'we provide a brief summary of the annotation system and labeling procedure interannotator reliability statistics overall distributional statistics a description of auxiliary files distributed with the corpus and information on how to obtain the data')],\n",
       " [(0.0,\n",
       "   'while snow can be used as a classifier and predicts using a winnertakeall mechanism over the activation value of the target classes we can also rely directly on the raw activation value it outputs which is the weighted linear sum of the active features to estimate the posteriors'),\n",
       "  (0.0,\n",
       "   'while many statistical methods make stupid mistakes ie inconsistency among predictions that no human ever makes as we show our approach improves also the quality of the inference significantly'),\n",
       "  (0.0,\n",
       "   'when the relation classifier is trained using the true entity labels the performance is much worse than using the predicted entity labels')],\n",
       " [(0.0,\n",
       "   'with smaller quantities of data there is less possibility of finding instances that use exactly the same set of words'),\n",
       "  (0.0,\n",
       "   'while there has been some previous work in sense discrimination eg schutze pedersen and bruce pedersen and bruce schutze fukumoto and suzuki by comparison it is much less than that devoted to word sense disambiguation which is the process of assigning a meaning to a word from a predefined set of possibilities'),\n",
       "  (0.0,\n",
       "   'when we cluster test instances we specify an upper limit on the number of clusters that can be discovered')],\n",
       " [(0.0,\n",
       "   'wn is a labeled directed graph d wa where a w is the set of nodes ie word tokens in the input string b a is a set of labeled arcs wi r wj where wi wj w and r r'),\n",
       "  (0.0,\n",
       "   'wj to say that there is an arc from wi to wj labeled r and wi wj to say that there is an arc from wi to wj regardless of the label we use'),\n",
       "  (0.0,\n",
       "   'while the sen tence data for validation consists of sentences the corresponding transition data contains instancesfor training only transition data is relevant and the train ing data set contains instances')],\n",
       " [(0.0,\n",
       "   'while previous programs with similar goals gildea and jurafsky were statisticsbased this tool will be based completely on handcoded rules and lexical resources'),\n",
       "  (0.0,\n",
       "   'whether or not one adapts an a la carte approach nombank and propbank projects provide users with data to recognize regularizations of lexically and syntactically related sentence structures'),\n",
       "  (0.0,\n",
       "   'when complete nombank will provide argument structure for instances of about common nouns in the penn treebank ii corpus')],\n",
       " [(0.0,\n",
       "   'we wanted to explore how well the annotators agreed on relatively abstract classifications such as requires extrapolation from actual findings and thus we refrained from writing instructions such as if the sentence contains a form of suggest then mark it as speculative into the guidelines'),\n",
       "  (0.0,\n",
       "   'we report here the isolation ofhuman zinc finger hzf a putative zincfinger transcription factor by motifdirected differential display of mrna extracted from histaminestimulated human vein endothelial cells'),\n",
       "  (0.0,\n",
       "   'we may use it to test a kr systems ability to predict b as the connecting aspect between a and c and to do this using data prior to the publication')],\n",
       " [(0.0,\n",
       "   'work over the last few years in literature data mining for biology has progressed from linguistically unsophisticated models to the adaptation of natural language processing nlp techniques that use full parsers park et al yakushiji et al and coreference to extract relations that span multiple sentences pustejovsky et al hahn et al for an overview see hirschman et al'),\n",
       "  (0.0,\n",
       "   'while this also has conceptual benefits for the annotation guidelines it has the fortunate effect of making such otherwise syntaxunfriendly malignancies as colorectal adenomas containing early cancer and acute myelomonocytic leukemia in remission amenable for mapping the component parts to syntactic nodes'),\n",
       "  (0.0,\n",
       "   'while sufficiently complex patterns can deal with these issues it requires a good amount of time and effort to build such handcrafted rules particularly since such rules are developed for each specific problem')],\n",
       " [(0.0,\n",
       "   'while like most discriminative models it is computeintensive to train it allows fast parsing remaining cubic despite the incorporation of lexical features'),\n",
       "  (0.0,\n",
       "   'we used two such auxiliary classifiers giving a prediction feature for each span these classifiers predicted only the presence or absence of a bracket over that span not bracket labels'),\n",
       "  (0.0,\n",
       "   'we used the constituent loss in our experiments marginals as qm where is the vector with components iri and qm is defined as where lir lxi yi r ir xi r and iir ixi yi r')],\n",
       " [(0.0,\n",
       "   'y or at least x yed or at least xed not only xed but yed not just xed but yed the probabilities in the denominator are difficult to calculate directly from search engine results'),\n",
       "  (0.0,\n",
       "   'wordnets cause relation between a causative and a resultative verb as in buy own would be tagged as instances of happensbefore by our system'),\n",
       "  (0.0,\n",
       "   'wordnet does not include the relation x buys y happensbefore x sells y since it is possible to sell something without having bought it eg having manufactured or stolen it')],\n",
       " [(0.0,\n",
       "   'yet this method also suffers from certain limitations a it identifies only templates with prespecified structures b accuracy seems more limited due to the weaker notion of similarity and c coverage is limited to the scope of an available corpus'),\n",
       "  (0.0,\n",
       "   'while in this paper we considered anchor sets to have equal weights we are also carrying out experiments with weights based on crosscorrelation between anchor sets'),\n",
       "  (0.0,\n",
       "   'web searching is then used to find occurrences of the input anchor set resulting in new templates that are supposed to specify the same relation as the original one born in')],\n",
       " [(0.0,\n",
       "   'yamada and knight introduced treetostring alignment on japanese data and gildea performed treetotree alignment on the korean treebank allowing for nonisomorphic structures he applied this to wordtoword alignment'),\n",
       "  (0.0,\n",
       "   'xia et al compare the rule templates of lexicalized tree adjoining grammars extracted from treebanks in english chinese and korean'),\n",
       "  (0.0,\n",
       "   'wu and alshawi et al used unsupervised learning on parallel text to induce syntactic analysis that was useful for their respective applications in phrasal translation extraction and speech translation though not necessarily similar to what a human annotator would select')],\n",
       " [(0.0,\n",
       "   'we would like to thank scott cotton for providing the propbank api which greatly simplifies the implementation of our system'),\n",
       "  (0.0,\n",
       "   'we take a critical look at the previously used features against each subtask and propose a new set of features in section'),\n",
       "  (0.0,\n",
       "   'we propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed')],\n",
       " [(0.0,\n",
       "   'while framenet uses semantic roles specific to a particular situation such as speaker message admire adore appreciate cherish enjoy addressee and propbank uses roles specific to a verb such as arg arg arg verbnet uses an intermediate level of thematic roles such as agent theme recipient'),\n",
       "  (0.0,\n",
       "   'when using general thematic roles with a small set of verb classes the probability used for the baseline works very well for subjects and objects which are primarily agents and themes respectively for our verbs'),\n",
       "  (0.0,\n",
       "   'when backing off from this probability we use statistics over more general classes of information such as conditioning over the semantic class of the verb instead of the verb itselffor this example psychological state verbs')],\n",
       " [(0.0,\n",
       "   'while translationbased approaches to obtaining data do address the problem of how to identify two strings as meaning the same thing they are limited in scalability owing to the difficulty and expense of obtaining large quantities of multiplytranslated source documents'),\n",
       "  (0.0,\n",
       "   'while this means certain common structural alternations eg activepassive cannot be generated we are still able to express a broad range of phenomena pings to be both unwieldy in practice and very often indicative of poor a word alignment'),\n",
       "  (0.0,\n",
       "   'while the alternations our system produces are currently limited in character the field of smt offers a host of possible enhancementsincluding reordering modelsaffording a natural path for future improvements')],\n",
       " [(0.0,\n",
       "   'wordlevel templates are employed when the words are lexicalized ie those that belong to particle auxiliary verb or suffix'),\n",
       "  (0.0,\n",
       "   'while the relative rates of lerror and serror are almost the same in hmms and crfs the number of lerrors with memms amounts to which is of total errors and is even larger than that of naive hmms'),\n",
       "  (0.0,\n",
       "   'we would like to thank kiyotaka uchimoto and masayuki asahara who explained the details of their japanese morphological analyzers')],\n",
       " [(0.0,\n",
       "   'zhou and su investigated an approach to build a chinese analyzer that integrated word segmentation pos tagging and parsing based on a hidden markov model'),\n",
       "  (0.0,\n",
       "   'with an allatonce characterbased approach an average word segmentation fmeasure of and an average pos tagging accuracy of was achieved'),\n",
       "  (0.0,\n",
       "   'with a oneatatime characterbased pos tagger the average pos tagging accuracy improved to higher than that achieved by the oneatatime wordbased pos tagger')],\n",
       " [(0.0,\n",
       "   'when evaluating on the mismatched outofdomain test data the gram baseline is outperformed by the improvement brought by the adaptation technique using a very small amount of matched bn data kwds is about relative'),\n",
       "  (0.0,\n",
       "   'we used a simple count cutoff feature selection algorithm which counts the number of occurrences of all features in a predefined set after which it discards the features whose count is less than a prespecified threshold'),\n",
       "  (0.0,\n",
       "   'we study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to datadriven approaches since vast amounts of training data are easily obtainable by simply wiping the case information in text')],\n",
       " [(0.0,\n",
       "   'when we explicitly enumerate the subtrees used in tree kernel the number of active nonzero features might amount to ten thousand or more'),\n",
       "  (0.0,\n",
       "   'when a convolution kernel is applied to sparse data kernel dot products between almost the same instances become much larger than those between different instances'),\n",
       "  (0.0,\n",
       "   'we would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications eg parse reranking collins and duffy and information extraction')],\n",
       " [(0.0,\n",
       "   'when computing degree centrality we have treated each edge as a vote to determine the overall prestige value of each node'),\n",
       "  (0.0,\n",
       "   'we use pagerank to weight each vote so that a vote that comes from a more prestigious sentence has a greater value in the centrality of a sentence'),\n",
       "  (0.0,\n",
       "   'we show three of the rouge metrics in our experiment results rouge unigrambased rouge bigrambased and rougew based on longest common subsequence weighted by the length')],\n",
       " [(0.0,\n",
       "   'while the statistical significance level is the most commonly used for historical reasons we want to validate as well the accuracy of the bootstrap resampling method at different statistical significance levels'),\n",
       "  (0.0,\n",
       "   'while bleu has become the most popular metric for machine translation evaluation some of its shortcomings have become apparent it does not work on single sentences since gram precision is often'),\n",
       "  (0.0,\n",
       "   'when we collect only sentences in sequence certain therefore better test sets of sentences may be constructed by sampling these sentences from different parts of the corpus')],\n",
       " [(0.0,\n",
       "   'while the size of the abstracts ranges from to words with an average size of words we have deliberately selected a very small abstract for the purpose of illustration'),\n",
       "  (0.0,\n",
       "   'while the final vertex scores and therefore rankings differ significantly as compared to their unweighted alternatives the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs'),\n",
       "  (0.0,\n",
       "   'we investigate and evaluate the application of textrank to two language processing tasks consisting of unsupervised keyword and sentence extraction and show that the results obtained with textrank are competitive with stateoftheart systems developed in these areas')],\n",
       " [(0.0,\n",
       "   'with these entities tagged a number of classes of features may be extracted representing various relationships between topic entities and value phrases similar to those described in section'),\n",
       "  (0.0,\n",
       "   'we describe the methods used to assign values to selected words and phrases and we introduce a method of bringing them together to create a model for the classification of texts'),\n",
       "  (0.0,\n",
       "   'topicsentence pot the average pot value of all adjectives which share a sentence with the topic of the text')],\n",
       " [(0.0,\n",
       "   'with the progress of mt research in recent years we are not satisfied with the getting correct words in the translations we also expect them to be wellformed and more readable'),\n",
       "  (0.0,\n",
       "   'while the subtree overlap metric defined above considers only subtrees of a fixed depth subtrees of other configurations may be important for discriminating good hypotheses'),\n",
       "  (0.0,\n",
       "   'while the machine learning approach improves correlation with human judgments all the metrics discussed are based on the same type of information ngram subsequences of the hypothesis translations')],\n",
       " [(0.0,\n",
       "   'while word frequency does not always constitute a good measure of word importance the distribution of words across an entire collection can be a good indicator of the specificity of the words'),\n",
       "  (0.0,\n",
       "   'while there is a large body of previous work focused on finding the semantic similarity of concepts and words the application of these wordoriented methods to text similarity has not been yet explored'),\n",
       "  (0.0,\n",
       "   'while there are several methods previously proposed for finding the semantic similarity of words to our knowledge the application of these wordoriented methods to text similarity has not been yet explored')],\n",
       " [(0.0,\n",
       "   'without this line algorithm could be considered as a generalization of the jimenez andmarzal algorithm to the case of acyclic monotonic hy pergraphs'),\n",
       "  (0.0,\n",
       "   'with the derivations thus ranked we can introduce anonrecursive representation for derivations that is analogous to the use of backpointers in parser implementa tion'),\n",
       "  (0.0,\n",
       "   'while charniak and johnson propose using an algorithm similar to our algorithm but with multiple passes to improve efficiency')],\n",
       " [(0.0,\n",
       "   'yamada and matsumoto experienced a reduction of slightly more than in dependency accuracy due to training set splitting and we expect that a similar loss is incurred here'),\n",
       "  (0.0,\n",
       "   'wong and wus parser is further differentiated from the other parsers mentioned here in that it does not use lexical items working only from partofspeech tags'),\n",
       "  (0.0,\n",
       "   'while yamada and matsumoto use a quadratic runtime algorithm with multiple passes over the input string nivre and scholz use a simplified version of the algorithm described here which handles only labeled or unlabeled dependency structures')],\n",
       " [(0.0,\n",
       "   'words in the positive class carry positive valence whereas these were randomly selected from english verbs and english adjectives those in negative class carry negative valence'),\n",
       "  (0.0,\n",
       "   'when we mark all sentences as opinionbearing it achieved and of accuracy for the annotation result of human and human respectively'),\n",
       "  (0.0,\n",
       "   'we used the charniak parser to get a phrase type feature of a frame element and the parse tree path feature')],\n",
       " [(0.0,\n",
       "   'with this scaling procedure into place we produce nonunique nbest lists for all sentences in our development corpus using all spmt submodels'),\n",
       "  (0.0,\n",
       "   'when using a phrasebased translation model one can easily extract the phrase pair the mutual the mutual and use it during the phrasebased model estimation phrase and in decoding'),\n",
       "  (0.0,\n",
       "   'when one analyzes the lexicalized xrs rules in this manner it is easy to associate with them any of the submodel probability distributions that have been proven useful in statistical phrasebased mt')],\n",
       " [(0.0,\n",
       "   'zens and ney describe a noisyor combination where sj is the probability that sj is not in the translation of t and psjti is a lexical probability'),\n",
       "  (0.0,\n",
       "   'we tested different phrasetable smoothing techniques in two different translation settings european language pairs with relatively small corpora and chinese to english translation with large corpora'),\n",
       "  (0.0,\n",
       "   'we show that any type of smoothing is a better idea than the relativefrequency estimates that are often used')],\n",
       " [(0.0,\n",
       "   'with source domain sentences and target domain sentences using supervised tagger features gives no improvement over using no source features'),\n",
       "  (0.0,\n",
       "   'with source domain sentences and target domain sentences using scl tagger features gives a relative reduction in error over using supervised tagger features and a relative reduction in error over using no source features'),\n",
       "  (0.0,\n",
       "   'with one hundred sentences of training data structural correspondence learning gives a relative reduction in error over the supervised baseline and it consistently outperforms both baseline models')],\n",
       " [(0.0,\n",
       "   'while we have presented significant improvements using additional constraints one may woneven when caching feature extraction during training mcdonald et al a still takes approximately minutes to train der whether the improvements are large enough to justify further research in this direction especially since mcdonald and pereira present an approximate algorithm which also makes more global decisions'),\n",
       "  (0.0,\n",
       "   'while we expect a longer runtime than using the chuliuedmonds as in previous work mcdonald et al b we are interested in how large the increase is'),\n",
       "  (0.0,\n",
       "   'while this is not crucial during decoding it does make discriminative online training difficult as training requires several iterations of parsing the whole corpus time st for the cross dataset using varying q values and the chuliuedmonds algorithm cle thus we investigate if it is possible to speed up our inference using a simple approximation')],\n",
       " [(0.0,\n",
       "   'while the developmentset results would induce us to utilize the standard threshold value of which is suboptimal on the test set the bagr agreementlink policy still achieves noticeable improvement over not using agreement links test set vs'),\n",
       "  (0.0,\n",
       "   'while such functionality is well beyond the scope of our current study we are optimistic that we can develop methods to exploit additional types of relationships in future work'),\n",
       "  (0.0,\n",
       "   'when we impose the constraint that all speech segments uttered by the same speaker receive the same label via samespeaker links both testset and tion accuracy in percent')],\n",
       " [(0.0,\n",
       "   'wilson et al proposed supervised learning dividing the resources into prior polarity and context polarity which are similar to polar atoms and syntactic patterns in this paper respectively'),\n",
       "  (0.0,\n",
       "   'wilson et al prepared prior polarities from existing resources and learned the context polarities by using prior polarities and annotated corpora'),\n",
       "  (0.0,\n",
       "   'when the window size is oo implying anywhere within a discourse the ratio is larger than the baseline by only and thus these types of coherency are not reliable even though the number of clues is relatively large')],\n",
       " [(0.0,\n",
       "   'withoutprogress on the joint extraction of opinion entities and their relations the capabilities of opinion based applications will remain limited'),\n",
       "  (0.0,\n",
       "   'while good performance in entity or relation extraction can contribute to better performance ofthe final system this is not always the case'),\n",
       "  (0.0,\n",
       "   'when a syntactic frame is matched to a sen tence the bracketed items should be instantiatedwith particular values corresponding to the sen tence')],\n",
       " [(0.0,\n",
       "   'wordnet fellbaum is a broadcoverage machinereadable dictionary which includes verbs mapped to word senses called synsets and common and proper nouns mapped to synsets'),\n",
       "  (0.0,\n",
       "   'word sense disambiguation at this level of granularity is a complex task which resisted all attempts of robust broadcoverage solutions'),\n",
       "  (0.0,\n",
       "   'with respect to ner wsd lies at the other end of the semantic tagging spectrum since the dictionary defines tens of thousand of very specific word senses including ner categories')],\n",
       " [(0.0,\n",
       "   'words that are very general in nature and that appear all over the place add noise to the vectors'),\n",
       "  (0.0,\n",
       "   'word vector gloss vector banerjee and pedersen encounter a similar issue when measuring semantic relatedness by counting the number of matching words between the glosses of two different concepts'),\n",
       "  (0.0,\n",
       "   'word sense disambiguation is the task of determining the meaning from multiple possibilities of a word in its given context')],\n",
       " [(0.0,\n",
       "   'words in the document are then sampled from a multinomial distribution where ln is the length of the document'),\n",
       "  (0.0,\n",
       "   'without the assistance of computers analysts have no choice but to read each document in order to identify those from a perspective of interest which is extremely timeconsuming'),\n",
       "  (0.0,\n",
       "   'while one might solicit these contrasting word pairs from domain experts our results show that statistical models such as svm and naive bayes can automatically acquire them')],\n",
       " [(0.0,\n",
       "   'within the third approach head and deprel can be predicted simultaneously or in two separate steps potentially using two different learners'),\n",
       "  (0.0,\n",
       "   'within the first approach each dependency can be labeled independently corstonoliver and aue or a news dialogue novel type of annotation d dependency c constituents dc discontinuous constituents f with functions t with types'),\n",
       "  (0.0,\n",
       "   'while the training data contained all columns although sometimes only with dummy values ie underscores the test data given to participants contained only the first')],\n",
       " [(0.0,\n",
       "   'with the availability of resources such as the penn wsj treebank much of the focus in the parsing community had been on producing syntactic representations based on phrasestructure'),\n",
       "  (0.0,\n",
       "   'we use the mira online learner to set the weights crammer and singer mcdonald et al a since we found it trained quickly and provide good performance'),\n",
       "  (0.0,\n",
       "   'we then add to the representation of the edge mi as head features mj as dependent features and also each conjunction of a feature from both sets')],\n",
       " [(0.0,\n",
       "   'we use svm classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion'),\n",
       "  (0.0,\n",
       "   'we projectivize training data by a minimal transformation lifting nonprojective arcs one step at a time and extending the arc label of lifted arcs using the encoding scheme called head by nivre and nilsson which means that a lifted arc is assigned the label rth where r is the original label and h is the label of the original head in the nonprojective dependency graph'),\n",
       "  (0.0,\n",
       "   'we have the best reported score for japanese swedish and turkish and the score for arabic danish dutch portuguese spanish and overall does not differ significantly from the best one')],\n",
       " [(0.0,\n",
       "   'while this determinism of em may be desirable in some circumstances we found that the ambiguity in h is often preferable at decoding time'),\n",
       "  (0.0,\n",
       "   'while the model and training regimen for oem differ from the model frommarcu and wong we achieved tion maximization algorithm for training oem was initialized with the heuristic parameters oh so the heuristic curve can be equivalently labeled as iteration'),\n",
       "  (0.0,\n",
       "   'while such examples of improvement are encouraging the trend of spurious determinism overwhelms this benefit by introducing errors in four related ways each of which will be explored in turn')],\n",
       " [(0.0,\n",
       "   'we use the source position j which is aligned to the last word of the target phrase in target position i'),\n",
       "  (0.0,\n",
       "   'we use a stateoftheart phrasebased translation system zens and ney zens et al including the following models an ngram language model a phrase translation model and a wordbased lexicon model'),\n",
       "  (0.0,\n",
       "   'we use a stateoftheart baseline system which would have obtained a good rank in the last nist evaluation nist')],\n",
       " [(0.0,\n",
       "   'while yamada and knight represent syntactical information in the decoding process through a series of transformation operations we operate directly at the phrase level'),\n",
       "  (0.0,\n",
       "   'while our submission time system syn using lm for rescoring only shows no improvement over the baseline we clearly see the impact of integrating the language model into the kbest list extraction process'),\n",
       "  (0.0,\n",
       "   'while no improvements were available at submission time our subsequent performance highlights the importance of tight integration of ngram language modeling within the syntax driven parsing environment')],\n",
       " [(0.0,\n",
       "   'would be while section will define the model formally we first proceed with an example translation from english to chinese note in particular that the inverted phrases between source and target throughout this paper we will use lhs and sourceside interchangeably so are rhs and targetside'),\n",
       "  (0.0,\n",
       "   'we then collect the resulting targetlanguage strings and plug them into the chineseside sr of rule r getting a translation for the subtree t'),\n",
       "  (0.0,\n",
       "   'we require each variable xi e x occurs exactly once in t and exactly once in s linear and nondeleting')],\n",
       " [(0.0,\n",
       "   'while the games goal is to arrive at some funny derivative of the original message by passing it through several noisy channels the cw algorithm aims at finding groups of nodes that broadcast the same message to their neighbors'),\n",
       "  (0.0,\n",
       "   'when generating swgraphs with the steyverstenenbaum model we fixed m to and varied n and the merge rate r which is the fraction of nodes of the smaller graph that is merged with nodes of the larger graph'),\n",
       "  (0.0,\n",
       "   'we use the following notation throughout this paper let g ve be a weighted graph with nodes viev and weighted edges vi vj wij ee with weight wij')],\n",
       " [(0.0,\n",
       "   'wu used a binary bracketing itg to segment a sentence while simultaneously wordaligning it to its translation but the model was trained heuristically with a fixed segmentation'),\n",
       "  (0.0,\n",
       "   'with this constraint in place the use of hillclimbing and sampling during em training becomes one of the largest remaining weaknesses of the cjptm'),\n",
       "  (0.0,\n",
       "   'with terminal productions producing cepts and inversions measuring distortion our phrasal itg is essentially a variation on the jptm with an alternate distortion model')],\n",
       " [(0.0,\n",
       "   'where not otherwise specified the postag and supertag sequence models are gram mod els and the language model is a gram model'),\n",
       "  (0.0,\n",
       "   'we use a model which combines more specificdependencies on source words and source ccg su pertags with a more general model which only has dependancies on the source word see equation we explore two different ways of balancing the sta tistical evidence from these multiple sources'),\n",
       "  (0.0,\n",
       "   'we tried setting the distortion limit to to see if allowing longer distance reorderings with ccg supertag sequence models could further improve performance however it resulted in a decrease in performance to a bleu score of')],\n",
       " [(0.0,\n",
       "   'zhao et al apply a slightly different sentencelevel strategy to language model adaptation first generating an nbest list with a baseline system then finding similar sentences in a monolingual targetlanguage corpus'),\n",
       "  (0.0,\n",
       "   'when using a loglinear combining framework as described in section mixture weights are set in the same way as the other loglinear parameters when performing crossdomain adaptation'),\n",
       "  (0.0,\n",
       "   'when translating however distances to the current source text are used in or instead of distances to the indomain development corpus')],\n",
       " [(0.0,\n",
       "   'word alignments are created between the source sentence and the reference translation shown and the source sentence and each of the system translations not shown'),\n",
       "  (0.0,\n",
       "   'while we consider the human evaluation to be primary it is also interesting to see how the entries were ranked by the various automatic evaluation metrics'),\n",
       "  (0.0,\n",
       "   'while these are based on a relatively few number of items and while we have not performed any tests to determine whether the differences in p are statistically significant the results are nevertheless interesting since three metrics have higher correlation than bleu tables and report p for the six metrics which were used to evaluate translations into the other languages')],\n",
       " [(0.0,\n",
       "   'when optimizing correlation with the sum of adequacy and fluency optimal values fall in between the values found for adequacy and fluency'),\n",
       "  (0.0,\n",
       "   'when n systems were available for a particular language we train the parameters n times leaving one system out in each training and pooling the segments from all other systems'),\n",
       "  (0.0,\n",
       "   'when evaluating a set of parameters on test data we compute segmentlevel correlation with human judgments for each of the systems in the test set and then report the mean over all systems')],\n",
       " [(0.0,\n",
       "   'while the training for the me classifier was done on a separate corpus and it was this classifier that contributed the most to the high precision it should be noted that some of the filters were tuned on the evaluation corpus'),\n",
       "  (0.0,\n",
       "   'while most nlp systems are a balancing act between precision and recall the domain of designing grammatical error detection systems is distinguished in its emphasis on high precision over high recall'),\n",
       "  (0.0,\n",
       "   'when we examined the errors we discovered that frequently the classifiers most probable preposition the one it assigned differed from the second most probable by just a few percentage points')],\n",
       " [(0.0,\n",
       "   'wsd is performed by assigning to each in duced cluster a score equal to the sum of weights of hyperedges found in the local context of the target word'),\n",
       "  (0.0,\n",
       "   'with this goal onmind we gave all the participants an unlabeled cor pus and asked them to induce the senses and create a clustering solution on it'),\n",
       "  (0.0,\n",
       "   'we were also surprised to see that no system could system supervised evaluation random random ramdom random table supervised evaluation of several random baselinesbeat the one cluster one word')],\n",
       " [(0.0,\n",
       "   'when this mapping was not straightforward we just adopted the wordnet sense inventory for that wordwe released the entire sense groupings those in duced from the manual mapping for words in the test set plus those automatically derived on the other words and made them available to the participants'),\n",
       "  (0.0,\n",
       "   'we report about the use of semantic resources as well as semantically annotated corpora sc semcor dso defence science organ isation corpus se senseval corpora omwe open mind word expert xwn extended word net wn wordnet glosses andor relations wnd wordnet domains as well as information about the use of unannotated corpora uc training tr mfs based on the semcor sense frequencies and the coarse senses provided by the organizers cs'),\n",
       "  (0.0,\n",
       "   'we observe that on a technical domain suchas computer science most supervised systems per formed worse due to the nature of their training set')],\n",
       " [(0.0,\n",
       "   'whilstresearchers believe that it will ultimately prove useful for applications which need some degree of se mantic interpretation the jury is still out on this point'),\n",
       "  (0.0,\n",
       "   'we tookthe word with the largest similarity or smallest dis tance for sd and l for best and the top for oot'),\n",
       "  (0.0,\n",
       "   'we thank serge sharoff for the use of his internet corpus julie weeds for the software we used for producing the distributional similarity baselines and suzanne stevenson for suggesting the oot task')],\n",
       " [(0.0,\n",
       "   'while we expected a raise in the case of the us census names the other two cases just show that there is a high and unpredictable variability which would require much larger data sets to have reliable population samples'),\n",
       "  (0.0,\n",
       "   'we reused the web corpus mann which contains names randomly picked from the us census and was well suited for the task'),\n",
       "  (0.0,\n",
       "   'we preferred how ever to leave the corpus as is and concentrate our efforts in producing clean training and test datasets rather than investing time in improving trial data')],\n",
       " [(0.0,\n",
       "   'wvalis hybrid approach outperforms the other systems in task b and using relaxed scoring in task c as well'),\n",
       "  (0.0,\n",
       "   'with this more complete temporal annotation it is no longer possible to simply evaluate the entire graph by scoring pairwise comparisons'),\n",
       "  (0.0,\n",
       "   'when applied to the test data the task b system was run first in order to supplythe necessary features to the task a and task c sys temslccte automatically identifies temporal refer ring expressions events and temporal relations in text using a hybrid approach leveraging variousnlp tools and linguistic resources at lcc')],\n",
       " [(0.0,\n",
       "   'whether or not the more coarsegrained senses are effective in improving natural language processing applications remains to be seen'),\n",
       "  (0.0,\n",
       "   'we selecteda total of lemmas verbs and nouns con sidering the degree of polysemy and total instances that were annotated'),\n",
       "  (0.0,\n",
       "   'we proposed two levels of participation in thistask i closed the systems could use only the an notated data provided and nothing else')],\n",
       " [(0.0,\n",
       "   'zx will now be equal to the sum of the weights of all trees that contain i jk a naive implementation to compute the expectation of all ln edges takes oln ln since calculating zx takes on ln for a single edge'),\n",
       "  (0.0,\n",
       "   'within this setting every edge in an induced graph gx for a sentence x will have an associated weight wk ij that maps the kth directed edge from node i to node j to a real valued numerical weight'),\n",
       "  (0.0,\n",
       "   'when this analysis is coupled with the projective parsing algorithms of eisner and paskin we begin to get a clear picture of the complexity for datadriven dependency parsing within an edgefactored framework')],\n",
       " [(0.0,\n",
       "   'with respect to mt quality we noticed that the introduction of test sets from a different domain did have an impact on the ranking of systems'),\n",
       "  (0.0,\n",
       "   'with respect to measuring the correlation between automated evaluation metrics and human judgments we found that using meteor and ulch which utilizes a variety of metrics including meteor resulted in the highest spearman correlation scores on average when translating into english'),\n",
       "  (0.0,\n",
       "   'while the judgments that we collected provide a wealth of information for developing automatic evaluation metrics we cannot not reuse them to evaluate our translation systems after we update their parameters or change their behavior in anyway')],\n",
       " [(0.0,\n",
       "   'word segmentation is considered an important first step for chinese natural language processing tasks because chinese words can be composed of multiple characters but with no space appearing between words'),\n",
       "  (0.0,\n",
       "   'without a standardized notion of a word traditionally the task of chinese word segmentation starts from designing a segmentation standard based on linguistic and task intuitions and then aiming to building segmenters that output words that conform to the standard'),\n",
       "  (0.0,\n",
       "   'with current statistical phrasebased mt systems one might hypothesize that segmenting into small chunks including perhaps even working with individual characters would be optimal')],\n",
       " [(0.0,\n",
       "   'with parallel computing processing time wall time can often be cut down by one or two orders of magnitude'),\n",
       "  (0.0,\n",
       "   'with compatible interface mgiza is suitable for a dropin replacement for giza while pgiza can utilize huge computation resources which is suitable for building large scale systems that cannot be built using a single machine'),\n",
       "  (0.0,\n",
       "   'while working on the required modification to giza to run the alignment step in parallel we identified a bug which needed to be fixed')],\n",
       " [(0.0,\n",
       "   'while a limited amount of gold standard annotated data was prepared for the parser evaluation shared task this is the main source of goldstandard sd data which is currently available'),\n",
       "  (0.0,\n",
       "   'when the relation between a head and its dependent can be identified more precisely relations further down in the hierarchy are used but when it is unclear more generic dependencies are possible dp dp'),\n",
       "  (0.0,\n",
       "   'when seeking a goldstandard dependency scheme for parser evaluation the ultimate goal of such an evaluation is an important question')],\n",
       " [(0.0,\n",
       "   'with the beam setting used in our experiments only of possible dependencies are considered by the tagbased model but of all correct dependencies are included'),\n",
       "  (0.0,\n",
       "   'we would argue that the collins method is considerably more complex than ours requiring a firststage generative model together with a reranking approach'),\n",
       "  (0.0,\n",
       "   'we will extend our model to include higherorder features in particular features based on sibling dependencies mcdonald and pereira and grandparent dependencies as in carreras')],\n",
       " [(0.0,\n",
       "   'zhang et al achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus'),\n",
       "  (0.0,\n",
       "   'wsem is the weight assigned to the semantic task the macro labeled fi score which was used for the ranking of the participating systems is computed as the harmonic mean of lmp and lmr'),\n",
       "  (0.0,\n",
       "   'who within an ensemblebased architecture implemented a joint syntacticsemantic model using maltparser with labels enriched with semantic information lluis and marquez who used a modified version of the eisner algorithm to jointly predict syntactic and semantic dependencies and finally sun et al who integrated dependency label classification and argument identification using a maximumentropy markov model')],\n",
       " [(0.0,\n",
       "   'wpf and wpbleu popovic and ney these metrics are based on words and part of speech sequences wpf is an ngram based fmeasure which takes into account both word ngrams and part of speech ngrams wpbleu is a combnination of the normal blue score and a part of speechbased bleu score'),\n",
       "  (0.0,\n",
       "   'word parallel corpus to create the large frenchenglish parallel corpus we conducted a targeted web crawl of bilingual web sites'),\n",
       "  (0.0,\n",
       "   'when there are no ties p can be calculated using the simplified equation where di is the difference between the rank for systemi and n is the number of systems')],\n",
       " [(0.0,\n",
       "   'where there is a clear point of departure for research a basic implementation of each interface is provided as an abstract class to minimize the work necessary for new extensions'),\n",
       "  (0.0,\n",
       "   'when designing our toolkit we applied general principles of software engineering to achieve three major goals extensibility endtoend coherence and scalability'),\n",
       "  (0.0,\n",
       "   'when a sentence is selected the count of every ngram in w that is found in the source sentence is incremented by the number of its occurrences in the source sentence')],\n",
       " [(0.0,\n",
       "   'with respect to the standard procedure the total training time was reduced by almost phrase extraction produced more phrase pairs and the final translation system showed a loss in translation performance bleu score below relative'),\n",
       "  (0.0,\n",
       "   'while data sparseness corroborates the need of large language samples in smt linguistic variability would indeed suggest to consider many alternative data sources as well'),\n",
       "  (0.0,\n",
       "   'when spanish indomain data is provided bleu score increases from to tm and rm contribute by about relative by covering the gap from to')],\n",
       " [(0.0,\n",
       "   'with the exception of the phrase substitutions the cost for all other edit operations is the same regardless of what the words in question are'),\n",
       "  (0.0,\n",
       "   'while this can make no judgement as to the preference of one type of human judgment over another it indicates differences between these human judgment types and in particular the difference between hter and adequacy and fluency'),\n",
       "  (0.0,\n",
       "   'while this analysis is not comprehensive it does give a general idea of the performance of all metrics by synthesizing the results into a single table')],\n",
       " [(0.0,\n",
       "   'with pcs on the abstracts corpus the system achieves of error reduction over current state of the art results'),\n",
       "  (0.0,\n",
       "   'with a more comprehensive list of negation signals it would be possible to identify all of them in a text'),\n",
       "  (0.0,\n",
       "   'we take the scope to the right for the baseline because it is much more frequent than the scope to the left as is shown by the statistics contained in table of section')],\n",
       " [(0.0,\n",
       "   'word class models discussed in section are computed offline are available online and provide an alternative to traditional semisupervised learning'),\n",
       "  (0.0,\n",
       "   'within the binary tree produced by the algorithm each word can be uniquely identified by its path from the root and this path can be compactly represented with a bit string'),\n",
       "  (0.0,\n",
       "   'while these two needs have motivated some of the research in ner in the last decade several other fundamental decisions must be made')],\n",
       " [(0.0,\n",
       "   'work on hedging in the machine learning field has as a goal to classify sentences into speculative or definite non speculative'),\n",
       "  (0.0,\n",
       "   'we take the scope to the right for the baseline because it is much more frequent than the scope to the left as is shown by the statistics contained in table of section'),\n",
       "  (0.0,\n",
       "   'we show that the system performs well for this task and that the same scope finding approach can be applied to both negation and hedging')],\n",
       " [(0.0,\n",
       "   'without constraints on the type of theme arguments the following two annotations are both legitimate the two can be seen as specifying the same event at different levels of specificity'),\n",
       "  (0.0,\n",
       "   'with the emergence of ner systems with performance capable of supporting practical applications the recent interest of the biotm community is shifting toward ie'),\n",
       "  (0.0,\n",
       "   'while using the final scores for weighting uses data that would not be available in practice similar weighting could likely be obtained eg using performance on the development data')],\n",
       " [(0.0,\n",
       "   'work in emotion detection can be roughly classified into that which looks for specific emotion denoting words elliott that which determines tendency of terms to cooccur with seed words whose emotions are known read that which uses handcoded rules neviarouskaya et al and that which uses machine learning and a number of emotion features including emotion denoting words alm et al'),\n",
       "  (0.0,\n",
       "   'words may evoke different emotions in different contexts and the emotion evoked by a phrase or a sentence is not simply the sum of emotions conveyed by the words in it but the emotion lexicon will be a useful component for any sophisticated emotion detecting algorithm'),\n",
       "  (0.0,\n",
       "   'wordnet affect lexicon strapparava and valitutti has a few hundred words annotated with the emotions they evoke it was created by manually identifying the emotions of a few seed words and then marking all their wordnet synonyms as having the same emotion')],\n",
       " [(0.0,\n",
       "   'you can do this by measuring the httpcastingwordscom httpgroupscsailmiteduuidturkit interannotator agreement of the turkers against ex perts on small amounts of gold standard data or by stating what controls you used and what criteria youused to block bad turkers'),\n",
       "  (0.0,\n",
       "   'workshop par ticipants struggled with how to attract turkers howto price hits hit design instructions cheating de tection etc no doubt that as work progresses so will a communal knowledge and experience of how to use mturk'),\n",
       "  (0.0,\n",
       "   'when publishing papers that use mechanical turk as a source of training data or to evaluate the outputof an nlp system report how you ensured the qual ity of your data')],\n",
       " [(0.0,\n",
       "   'your edited translations the machine translations the shortage of snow in mountain the shortage of snow in mountain worries the hoteliers worries the hoteliers correct reset edited no corrections needed unable to the deserted tracks are not the deserted tracks are not putting down problem only at the exploitants of skilift putting down problem only at the exploitants of skilift correct reset edited no corrections needed unable to the lack of snow deters the people the lack of snow deters the people to reserving their stays at the ski in the hotels and pension to reserving their stays at the ski in the hotels and pension correct reset edited no corrections needed unable to thereby is always possible to thereby is always possible to track free bedrooms for all the dates in winter including christmas and nouvel an track free bedrooms for all the dates in winter including christmas and nouvel an'),\n",
       "  (0.0,\n",
       "   'within each batch the source segments for nine of the screens were chosen from a small pool of source segments instead of being sampled from the larger pool of source segments designated for the ranking task the larger pool was used to choose source segments for nine other screens also'),\n",
       "  (0.0,\n",
       "   'with the exception of frenchenglish and englishfrench one can observe that topperforming constrained systems did as well as the unconstrained system onlineb')],\n",
       " [(0.0,\n",
       "   'wordspace vector models or distributional models of semantics henceforth dsms are computational models that build contextual semantic representations for lexical items from corpus data'),\n",
       "  (0.0,\n",
       "   'with these settings the compared spaces become less asymmetrical gold standard neighbours from a pool of just different items plus predictions prediction space neighbours from a pool of items'),\n",
       "  (0.0,\n",
       "   'widdows obtain results indicating that both the tensor product and the convolution product perform better than the simple additive model')],\n",
       " [(0.0,\n",
       "   'when positive feedback is received a new training instance for a structured learner is created from the input sentence and prediction line this training instance replaces any previous instance for the input sentence'),\n",
       "  (0.0,\n",
       "   'when a structure is found with positive feedback it is added to the training pool for a structured learner'),\n",
       "  (0.0,\n",
       "   'we use a custom implementation to optimize the objective function using the cuttingplane method this allows us to parrallelize the learning process by solving the inference problem for multiple training examples simultaneously')],\n",
       " [(0.0,\n",
       "   'zhao et al extended the biological cue word dictionary of their system using it as a feature for classification by the frequent cues of the wikipedia dataset while ji et al'),\n",
       "  (0.0,\n",
       "   'weasel words do not give a neutral account of facts rather they offer an opinion without any backup or source'),\n",
       "  (0.0,\n",
       "   'we think that this is due to the practical importance of the task for principally biomedical applications and because it addresses several open research questions')],\n",
       " [(0.0,\n",
       "   'we use the following procedure to convert a tweet into a tree representation initialize the main tree to be root'),\n",
       "  (0.0,\n",
       "   'we use stratified sampling to get a balanced dataset of tweets tweets each from classes positive negative and neutral'),\n",
       "  (0.0,\n",
       "   'we use previously proposed stateoftheart unigram model as our baseline and report an overall gain of over for two classification tasks a binary positive versus negative and a way positive versus negative versus neutral')],\n",
       " [(0.0,\n",
       "   'while the task is related to synonymy relation extraction yu and agichtein it has a novel definition of renaming one name permanently replacing the other'),\n",
       "  (0.0,\n",
       "   'while the basic task setup and entity definitions follow those of the ge task epi extends on the extraction targets by defining new event types relevant to task topics including major protein modification types and their reverse reactions'),\n",
       "  (0.0,\n",
       "   'while finding connections between event triggers and protein references is a major part of event extraction it becomes much harder if one is replaced with a coreferencing expression')],\n",
       " [(0.0,\n",
       "   'when only primary arguments are considered the first five event types in table are classified as simple event types requiring only unary arguments'),\n",
       "  (0.0,\n",
       "   'uturku was the winning system of task in bjorne et al and miwa was the best system reported after bionlpst miwa et al b'),\n",
       "  (0.0,\n",
       "   'to our disappointment however an effective use of supporting task results was not observed which thus remains as future work for further improvement')],\n",
       " [(0.0,\n",
       "   'word sense we trained a word sense tagger using a svm classifier and contextual word and part of speech features on all the training portion of the ontonotes data'),\n",
       "  (0.0,\n",
       "   'while trained morels seem able to better balance precision and recall and thus to achieve a higher fscore on the mention task itself their recall tends to be quite a bit lower than that achievable by rulebased systems designed to favor recall'),\n",
       "  (0.0,\n",
       "   'while this is lower than the figures cited in previous coreference evaluations that is as expected given that the task here includes predicting the underlying mentions and mention boundaries the insistence on exact match and given that the relatively easier appositive coreference cases are not included in this measure')],\n",
       " [(0.0,\n",
       "   'while the corpora used in haghighi and klein are different from the one in this shared task our result of b suggests that our systems performance is competitive'),\n",
       "  (0.0,\n",
       "   'we use all synsets for each mention but restrict it to mentions that are at most three sentences apart and lexical chains of length at most four'),\n",
       "  (0.0,\n",
       "   'we select for resolution only the first mentions in each cluster for two reasons a the first mention tends to be better defined fox which provides a richer environment for feature extraction and b it has fewer antecedent candidates which means fewer opportunities to make a mistake')],\n",
       " [(0.0,\n",
       "   'zmert is designed to be modular with respect to the objective function and allows bleu to be easily replaced with other automatic evaluation metrics'),\n",
       "  (0.0,\n",
       "   'yo dim minustah ap bay djob mason ki kote pou mw ta pase si mw ta vle jwenn nan djob sa yo'),\n",
       "  (0.0,\n",
       "   'working this back into pes definition we have pa gt b pa lt b and therefore pe rather than and below for a detailed breakdown by language pair')],\n",
       " [(0.0,\n",
       "   'while versions tuned to various types of human judgments do not perform as well as the widely used bleu metric papineni et al a balanced tuning version of meteor consistently outperforms bleu over multiple endtoend tunetest runs on this data set'),\n",
       "  (0.0,\n",
       "   'whereas previous versions of meteor simply strip punctuation characters prior to scoring version includes a new text normalizer intended specifically for translation evaluation'),\n",
       "  (0.0,\n",
       "   'we use the nist open machine translation evaluation urduenglish parallel data przybocki plus m words of monolingual data from the english gigaword corpus parker et al to build a standard moses system hoang et al as follows')],\n",
       " [(0.0,\n",
       "   'with some minor api changes namely returning the length of the ngram matched it could also be fasterthough this would be at the expense of an optimization we explain in section'),\n",
       "  (0.0,\n",
       "   'with a good hash function collisions of the full bit hash are exceedingly rare one in billion queries for our baseline model will falsely find a key not present'),\n",
       "  (0.0,\n",
       "   'while sorted arrays could be used to implement the same data structure as probing effectively making m we abandoned this implementation because it is slower and larger than a trie implementation')],\n",
       " [(0.0,\n",
       "   'wlvshef r s the systems integrates novel linguistic features from the source and target texts in an attempt to overcome the limitations of existing shallow features for quality estimation'),\n",
       "  (0.0,\n",
       "   'while some systems did not achieve improvements over the baseline while exploiting such features others have the uu submissions for instance exploiting both constituency and dependency trees'),\n",
       "  (0.0,\n",
       "   'when there are no ties can be calculated using the simplified equation systemlevel correlation for translations into english where di is the difference between the rank for systemi and n is the number of systems')],\n",
       " [(0.0,\n",
       "   'wordalign thus provides an example how a model such as brown et al s model that was originally designed for research in statistical machine translation can be modified to achieve practical though less ambitious goals in the near term'),\n",
       "  (0.0,\n",
       "   'wordalign starts with an initial rough alignment i which maps french positions to english positions if the mapping is partial we use linear extrapolation to make it complete'),\n",
       "  (0.0,\n",
       "   'we used this stratified sampling because we wanted to make more accurate statements about our error rate by tokens than we would have obtained from random sampling or even from equal weighting of the quintiles')],\n",
       " [(0.0,\n",
       "   'whether the human language engine is organized as a pipeline plus a few feedback loops or an every module talks to every other module architecture is unknown at this point hopefully new psycholinguistic experiments will shed more light on this issue'),\n",
       "  (0.0,\n",
       "   'we have insufficient engineering data at present to make any wellsubstantiated claims about whether the oneway pipeline has the optimal costbenefit tradeoff or not and in any case this will probably depend somewhat on the circumstances of each application reiter and mellish but the circumstantial evidence on this question is striking despite the fact that so many theoretical papers have argued against pipelines and very few if any have argued for pipelines every one of the applicationsoriented systems examined in this survey chose to use the oneway pipeline architecture'),\n",
       "  (0.0,\n",
       "   'unfortunately while all of the systems possessed a module which converted semantic representations into deep syntactic ones each system used a different name for this module')],\n",
       " [(0.0,\n",
       "   'with unsupervised learning the learner does not have a gold standard training corpus with which accuracy can be measured'),\n",
       "  (0.0,\n",
       "   'with markovmodel based taggers there have been two different methods proposed for adding knowledge to a tagger trained using the baumwelch algorithm'),\n",
       "  (0.0,\n",
       "   'when training a stochastic tagger using the baumwelch algorithm overtraining often does occur merialdo elworthy requiring an additional heldout training corpus for determining an appropriate number of training iterations')],\n",
       " [(0.0,\n",
       "   'wordclasses of semantically similar words may be used to help the sparse data problem both rrr and br report significant improvements through the use of wordclasses'),\n",
       "  (0.0,\n",
       "   'when evaluating an algorithm it is useful to have an idea of the lower and upper bounds on its performance'),\n",
       "  (0.0,\n",
       "   'we will use the symbol f to denote the number of times a particular tuple is seen in training data')],\n",
       " [(0.0,\n",
       "   'yarowsky used the following metric to calculate the strength of a feature f this is for the case of a confusion set of two words w and w'),\n",
       "  (0.0,\n",
       "   'yarowsky pointed out the complementarity between context words and collocations context words pick up those generalities that are best expressed in an orderindependent way while collocations capture orderdependent generalities'),\n",
       "  (0.0,\n",
       "   'yarowsky describes further refinements such as detecting and pruning features that make a zero or negative contribution to overall performance')],\n",
       " [(0.0,\n",
       "   'yet a computational system has no choice but to consider other more awkward possibilities for example this cluster might be capturing a distributional relationship between advice as one sense of counsel and royalty as one sense of court'),\n",
       "  (0.0,\n",
       "   'yarowskys algorithm for sense disambiguation can be thought of as a way of determining how rogets thesaurus categories behave with respect to contextual features'),\n",
       "  (0.0,\n",
       "   'word word similarity most informative subsumer doctor nurse health professional doctor lawyer professional person doctor man person individual doctor medicine entity doctor hospital entity doctor health virtual root doctor sickness virtual root doctors are minimally similar to medicine and hospitals since these things are all instances of something having concrete existence living or nonliving wordnet class entity but they are much more similar to lawyers since both are kinds of professional people and even more similar to nurses since both are professional people working specifically within the health professions')],\n",
       " [(0.0,\n",
       "   'while this automatic derivation process introduced a small percentage of errors of its own it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing'),\n",
       "  (0.0,\n",
       "   'while brackets must be correctly paired in order to derive a chunk structure it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags the few hard cases that arise can be handled completely locally'),\n",
       "  (0.0,\n",
       "   'when this approach is applied to partofspeech tagging the possible sources of evidence for templates involve the identities of words within a neighborhood of some appropriate size and their current partofspeech tag assignments')],\n",
       " [(0.0,\n",
       "   'words that the most precise lexicon didnt know about which were found in the second most precise lexicon were translated next'),\n",
       "  (0.0,\n",
       "   'while the true size of the source vocabulary is usually unknown recall can be estimated using a representative text sample by computing the fraction of words in the text that also appear in the lexicon'),\n",
       "  (0.0,\n",
       "   'whether a pair of words is considered a cognate pair depends on the ratio of the length of their longest not necessarily contiguous common subsequence to the length of the longer word')],\n",
       " [(0.0,\n",
       "   'without optimisation it has an asymptotic retrieval complexity of nf where n is the number of items in memory and f the number of features'),\n",
       "  (0.0,\n",
       "   'when a word is not found in the lexicon its lexical representation is computed on the basis of its form its context is determined and the resulting pattern is looked up in the unknown words case base'),\n",
       "  (0.0,\n",
       "   'weiss amp kulikowski independent training and test sets were selected from the original corpus the system was trained on the training set and the generalization accuracy percentage of correct category assignments was computed on the independent test set')],\n",
       " [(0.0,\n",
       "   'without this optimization testing would have been several orders of magnitude slower than making a decision by testing only a small subset of highly predictive features'),\n",
       "  (0.0,\n",
       "   'with respect to training time the symbolic methods are significantly slower since they are searching for a simple declarative representation of the concept'),\n",
       "  (0.0,\n",
       "   'with respect to testing time the symbolic methods perform the best since they only need to test a small number of features before making a decision')],\n",
       " [(0.0,\n",
       "   'weischedel et al provide the results from a battery of tritag markov model experiments in which the probability pwt of observing a word sequence w wi w wn together with a tag sequence t is given by furthermore pwiiti for unknown words is computed by the following heuristic which uses a set of predetermined endings this approximation works as well as the maxent model giving unknown word accuracyweischedel et al on the wall st journal but cannot be generalized to handle more diverse information sources'),\n",
       "  (0.0,\n",
       "   'using the set of difficult words the model performs at accuracy on the development set an insignificant improvement from the baseline accuracy of'),\n",
       "  (0.0,\n",
       "   'unlike sdt the maxent training procedure does not recursively split the data and hence does not suffer from unreliable counts due to data fragmentation')],\n",
       " [(0.0,\n",
       "   'with this criterion and the example grammar of figure the best parse tree would be the probability that the s constituent is correct is while the probability that the a constituent is correct is and the probability that the b constituent is correct is thus this tree has on average constituents correct'),\n",
       "  (0.0,\n",
       "   'when using a chart parser as bod did three problematic cases must be handled productions unary productions and nary ngt productions'),\n",
       "  (0.0,\n",
       "   'were previous results due only to the choice of test data or are the differences in implementation partly responsible')],\n",
       " [(0.0,\n",
       "   'wwz w is the weighted mutual information in our algorithm since it is most suitable for lexicon compilation of midfrequency technical words or terms as an initial step all prw are precomputed for the seed words in both languages'),\n",
       "  (0.0,\n",
       "   'word correlations ww wt are computed from general likelihood scores based on the cooccurrence of words in common segments'),\n",
       "  (0.0,\n",
       "   'word correlations are important statistical information which has been successfully employed to find bilingual word pairs from parallel corpora')],\n",
       " [(0.0,\n",
       "   'what is most interesting here is the way in which strongly selecting word w is typically the head of a noun phrase which could lead the model astray for example toy soldiers behave differently from soldiers mccawley'),\n",
       "  (0.0,\n",
       "   'thus despite the absence of class annotation in the training text it is still possible to arrive at a usable estimate of classbased probabilities'),\n",
       "  (0.0,\n",
       "   'this means that the observed countverbobj drink coffee will be distributed by adding a to the joint frequency with drink for each of the classes containing coffee')],\n",
       " [(0.0,\n",
       "   'typically the procedures postulate many different values for a which cause the parser to explore many different derivations when parsing an input sentence'),\n",
       "  (0.0,\n",
       "   'thus there are three parameters for the search heuristic namely k m and q and all experiments reported in this paper use k m and q table describes the top k bfs and the semantics of the supporting functions'),\n",
       "  (0.0,\n",
       "   'this paper takes a historybased approach black et al where each treebuilding procedure uses a probability model palb derived from pa b to weight any action a based on the available context or history b')],\n",
       " [(0.0,\n",
       "   'while we will almost always wish to parse using thresholds it is nice to know that multiplepass parsing can be seen as an approximation to an admissible technique where the degree of approximation is controlled by the thresholding parameter'),\n",
       "  (0.0,\n",
       "   'while we had not spent a great deal of time hand optimizing these parameters we are very encouraged by the optimization algorithms practical utility'),\n",
       "  (0.0,\n",
       "   'while we dont know of other systems that have used exactly our techniques our techniques are certainly similar to those of others')],\n",
       " [(0.0,\n",
       "   'you can skip ahead to table for a random sample of the nccs that the method validated for use in a machine translation task'),\n",
       "  (0.0,\n",
       "   'wu showed how to use an existing translation lexicon to populate a database of phrasal correspondences for use in examplebased mt'),\n",
       "  (0.0,\n",
       "   'wrongful conviction erreur judiciaire weak sister parent pauvre of both the users and providers of transportation des utilisateurs et des transporteurs understand the motivation saisir le motif swimming pool piscine ship unprocessed uranium expedier de luranium non raffine by reason of insanity pour cause dalienation mentale lagence de presse libre du qubec lagence de presse libre du qubec do cold weather research etudier leffet du froid the bread basket of the nation le grenier du canada turn back the boatload of european jews renvoyer tout ces juifs europeens generic pharmaceutical industry association generic pharmaceutical industry association')],\n",
       " [(0.0,\n",
       "   'within the context of a restricted domain many polysemous words have a strong preference for one word sense so knowing the most probable word sense in a domain can strongly constrain the ambiguity'),\n",
       "  (0.0,\n",
       "   'with the exception of the energy category we were able to find words that were judged as s or s for each category'),\n",
       "  (0.0,\n",
       "   'while these efforts may be useful for some applications we believe that they will never fully satisfy the need for semantic knowledge')],\n",
       " [(0.0,\n",
       "   'zipf pedersen kayaalp and bruce there is evidence that our data when represented by a dissimilarity matrix can be adequately characterized by a normal distribution'),\n",
       "  (0.0,\n",
       "   'yarowsky compares his method to schiitze and shows that for four words the former performs significantly better in distinguishing between two senses'),\n",
       "  (0.0,\n",
       "   'with the exception of line each ambiguous word is tagged with a single sense defined in the longman dictionary of contemporary english ldoce procter')],\n",
       " [(0.0,\n",
       "   'we measured the ability of judges to agree with one another using the notion of percent agreement that was defined by gale and used extensively in discourse segmentation studies passonneau and litman hearst percent agreement reflects the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion the percent agreements computed for each of the five texts and each level of importance are given in table the agreements among judges for our experiment seem to follow the same pattern as those described by other researchers in summarization johnson that is the judges are quite consistent with respect to what they perceive as being very important and unimportant but less consistent with respect to what they perceive as being less important in contrast with the agreement observed among judges the percentage agreements computed for importance assignments that were randomly generated for the same texts followed a normal distribution with p o these results suggest that the agreement among judges is significant agreement among judges with respect to the importance of each textual unit'),\n",
       "  (0.0,\n",
       "   'we described the first experiment that shows that the concepts of rhetoncal analysis and nucleanty can be used effectively for summarizing text the experiment suggests that discoursebased methods can account for detemmimg the most important units in a text with a recall and precision as high as we showed how the concepts of rhetorical analysis and nucleanty can be treated algorithmically and we compared recall and precision figures of a summarization program that implements these concepts with recall and precision figures that pertain to a baseline algorithm and to a commercial system the microsoft office summarizer the discoursebased summanzation program that we propose outperforms both the baseline and the commercial summarizer see table however since its results do not match yet the recall and precision figures that pertain to the manual discourse analyses it is likely that improvements of the rhetorical parser algorithm will result in better performance of subsequent implemetations'),\n",
       "  (0.0,\n",
       "   'we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text we show how these concepts can be implemented and we discuss results that we obtained with a discoursebased summanzation program motivation the evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some consamples of the output in very few cases output of a summarization program with a humanmade summary or evaluated with the help of human subjects usually the results are modest unfortunately evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions the position that we take in this paper is that in order to build highquality summarization programs one needs to evaluate not only a representative set of automatically generated outputs a highly difficult problem by itself but also the adequacy of the assumptions that these programs use that way one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each with few exceptions automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text see paice for an excellent overview determining the salient parts is considered to be achievable because one or more of the following assumptions hold i important sentences in a text contain words that are used frequently lahn edmundson n important sentences contain words that are used in the tide and section headings edmundson in important sentences are located at the beginning or end of paragraphs baxendale iv important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically through training techniques lin and hovy v important sentences use words as greatest and significant or indiphrases as the main aim of this paper and the purpose of this article while nonimportant senuse words as impossible edmundson rush salvador and zamora vi important sentences and concepts highest connected entities in elaborate semantic structures skorochodko lin barzilay and elhadad and vn imponant and nonimportant sentences are derivable from a discourse representation of the text sparck jones ono surmta and mike in determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections computers are accurate tools flowever in determining the concepts that are semantically related or the discourse structure of a text computers are no longer so accurate rather they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement although it is plausible that elaborate cohesionand coherencebased structures can be used effectively in summarization we believe that before building summarization programs we should determine the extent to which these assumptions hold in this paper we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text we show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program from discourse trees to summaries an empirical view')],\n",
       " [(0.0,\n",
       "   'we use the princeton wordnet technology for the database format database compilation as well as the princeton wordnet interface applying extensions only where necessary'),\n",
       "  (0.0,\n",
       "   'we therefore propose a mixed approach treating irregular particle verbs by enumeration and regular particle verbs in a compositional manner'),\n",
       "  (0.0,\n",
       "   'we present the lexicalsemantic net for german germanet which integrates conceptual ontological information with lexical semantics within and across word classes')],\n",
       " [(0.0,\n",
       "   'while the winograd sentences are too difficult for current robust lexical semantic systems simpler generalizations about what can fill an argument are possible consider the price of aluminum rose today due to large purchases by alcoa inc'),\n",
       "  (0.0,\n",
       "   'whether this holds in general remains an open question but it is a central design assumption behind the system'),\n",
       "  (0.0,\n",
       "   'what distinguishes cogniac from algorithms that use similar sorts of information is that it will not resolve a pronoun in circumstances of ambiguity')],\n",
       " [(0.0,\n",
       "   'wordnet does not include crosspartofspeech semantic relations so this relation cannot be used with word senses while term indexing simply and successfully does not distinguish them'),\n",
       "  (0.0,\n",
       "   'with this collection we can see if plain disambiguation is helpful for retrieval because word senses are distinguished but synonymous word senses are not identified'),\n",
       "  (0.0,\n",
       "   'while his results are very interesting it remains unclear in our opinion whether they would be corroborated with real occurrences of ambiguous words')],\n",
       " [(0.0,\n",
       "   'zn in the bottomup variant of the earley algorithm where a gt is a production of the original grammar'),\n",
       "  (0.0,\n",
       "   'when one has a completed parse or perhaps several possible parses one simply stops parsing leaving items remaining on the agenda'),\n",
       "  (0.0,\n",
       "   'we tested the parser on section section of the wsj text with various normalization constants working on each sentence only until we reached the first full parse')],\n",
       " [(0.0,\n",
       "   'while was still the fourth highest score out of the twelve participants in the evaluation we feel that it is necessary to view this number as a crossdomain portability result rather than as an indicator of how the system can do on unseen data within its training domain'),\n",
       "  (0.0,\n",
       "   'while all of menes features have binaryvalued output the binary features are features whose associated historyview can be considered to be either on or off for a given token'),\n",
       "  (0.0,\n",
       "   'when the features attempt to determine whether or not they fire on a given history they request an appropriate history view object from the history object and then query the history view object to determine whether their firing conditions are satisfied')],\n",
       " [(0.0,\n",
       "   'when viewed in this way a can be regarded as an index into these vectors that specifies which value is relevant to the particular choice of antecedent'),\n",
       "  (0.0,\n",
       "   'we would like to know therefore whether the pattern of pronoun references that we observe for a given referent is the result of our supposed hypothesis about pronoun reference that is the pronoun reference strategy we have provisionally adopted in order to gather statistics or whether the result of some other unidentified process'),\n",
       "  (0.0,\n",
       "   'we will say that the gender class for which this relative frequency is the highest is the gender class to which the referent most probably belongs')],\n",
       " [(0.0,\n",
       "   'yet if there are enough relations to link any pair of facts which given the existence of elaboration may often be nearly the case the number of trees whose top nucleus are a specified fact grows from to to as the number of facts grows from to to'),\n",
       "  (0.0,\n",
       "   'with each example problem we have specified the number of facts the number of elaboration relations and the number of nonelaboration relations'),\n",
       "  (0.0,\n",
       "   'we score as follows for each fact that will come textually between a satellite and its nucleus constraints on information ordering our relations have preconditions which are facts that should be conveyed before them')],\n",
       " [(0.0,\n",
       "   'without this mechanism it is possible that the decision tree classifies np i and np i as coreferent and np i and npk as coreferent but np i and npk as not coreferent'),\n",
       "  (0.0,\n",
       "   'while human audiences have little trouble mapping a collection of noun phrases onto the same entity this task of noun phrase np coreference resolution can present a formidable challenge to an nlp system'),\n",
       "  (0.0,\n",
       "   'when computing a sum that involves both oo and oo we choose the more conservative route and the oo distance takes precedence ie the two noun phrases are not considered coreferent')],\n",
       " [(0.0,\n",
       "   'with only training words per context this is difficult and in the face of such strong odds against any of the named entity classes the conservative nature of the learning algorithm only braves an entity label correctly for more words than the baseline model'),\n",
       "  (0.0,\n",
       "   'when using dominants the order does not affect the process because of the fact that once a dominant state is reached it cannot change to another dominant state in the future probability mass is moved only from questionable'),\n",
       "  (0.0,\n",
       "   'when tokenization is used each token is inserted in the two morphological tries one that keeps the letters of the tokens in the normal prefix order another that keeps the letter in the reverse suffix order')],\n",
       " [(0.0,\n",
       "   'zt can be written as follows following the derivation of schapire and singer providing that w gt w equ'),\n",
       "  (0.0,\n",
       "   'yarowskycautious does not separate the spelling and contextual features but does have a limit on the number of rules added at each stage'),\n",
       "  (0.0,\n",
       "   'with each iteration more examples are assigned labels by both classifiers while a high level of agreement gt is maintained between them')],\n",
       " [(0.0,\n",
       "   'while we cannot prove there are no such useful features on which one should condition trust we can give some insight into why the features we explored offered no gain'),\n",
       "  (0.0,\n",
       "   'when this metric is less than we expect to incur more errors than we will remove by adding those constituents to the parse'),\n",
       "  (0.0,\n",
       "   'we would like to thank eugene charniak michael collins and adwait ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments')],\n",
       " [(0.0,\n",
       "   'without the chunker the relations finder would have to decide for every word whether it is the head of a constituent that bears a relation to the verb'),\n",
       "  (0.0,\n",
       "   'with the extended job tag set at hand we can tag the sentence after having found prep np and other chunks we collapse preps and nps to pps in a second step'),\n",
       "  (0.0,\n",
       "   'with the churlker the relations finder has to make this decision for fewer words namely only for those which are the last word in a chunk resp the preposition of a pp chunk')]]"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_objetivo_ml_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "ddbcd844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:31:46.227457Z",
     "start_time": "2023-05-01T21:31:46.215370Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_teste_exp2=[]\n",
    "for lista_frase_obj_prob in frases_objetivo_ml_teste:\n",
    "    cada_paper=[]\n",
    "    for frase_obj_prob in lista_frase_obj_prob:\n",
    "        cada_paper.append(frase_obj_prob[1])\n",
    "    frase_objetivo_teste_exp2.append(cada_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "503dee47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:32:07.004478Z",
     "start_time": "2023-05-01T21:31:46.229555Z"
    }
   },
   "outputs": [],
   "source": [
    "p_r_tri, r_r_tri, f1_r_tri = calculo_rouge(frase_objetivo_teste_exp2,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9ce5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:06:30.224894Z",
     "start_time": "2023-05-01T15:06:23.255859Z"
    }
   },
   "source": [
    "rouge_total=[]\n",
    "for i,j in zip(frase_objetivo_teste_exp2,list(padrao_ouro_teste_frases.values())):\n",
    "    #print (len(i))\n",
    "    rouge_1={}\n",
    "    for h in range(len(i)):\n",
    "        for z in range(len(j)):\n",
    "            rouge_1[(h,z)] = (scorer.score(j[z],i[h])['rouge1'].precision)\n",
    "    rouge_total.append(rouge_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff1742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:06:33.673452Z",
     "start_time": "2023-05-01T15:06:33.655357Z"
    }
   },
   "source": [
    "maximo_rouge_paper = max_rouge(rouge_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab6db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:06:34.486615Z",
     "start_time": "2023-05-01T15:06:34.475614Z"
    }
   },
   "source": [
    "precisao_exp3_pt2, revocacao_exp3_pt2 = rouge_precisao_revocacao(maximo_rouge_paper, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0efece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:06:34.974199Z",
     "start_time": "2023-05-01T15:06:34.960887Z"
    }
   },
   "source": [
    "print('------ revocação ---------')\n",
    "print('média recall',np.mean(revocacao_exp3_pt2))\n",
    "qtd_revocacao_exp3_pt2, media_revocacao_exp3_pt2  = analise_revocacao(revocacao_exp3_pt2, lista_padrao_ouro_teste, media_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a3206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:06:35.508383Z",
     "start_time": "2023-05-01T15:06:35.503339Z"
    }
   },
   "source": [
    "print('------ precisão ---------')\n",
    "qtd_precisao_exp3_pt2, qtd_zero_exp3_pt2, media_exp3_pt2 = analise_precisao(precisao_exp3_pt2, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c739da",
   "metadata": {},
   "source": [
    "## UNIGRAMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "37640207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:35:45.730556Z",
     "start_time": "2023-05-01T21:35:42.459587Z"
    }
   },
   "outputs": [],
   "source": [
    "unigramas_teste          = gerador_ngramas(token_frases_teste,1)\n",
    "probabilidades_unigramas  = gerador_probabilidades(unigramas_teste, model_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "309b98c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:35:47.784491Z",
     "start_time": "2023-05-01T21:35:47.752397Z"
    }
   },
   "outputs": [],
   "source": [
    "lista=[]\n",
    "for i,j in zip(range(len(probabilidades_unigramas)),list(frases_papers_teste.keys()) ):\n",
    "    lista.append(sorted(zip(probabilidades_unigramas[i],frases_papers_teste[j]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "95ee0f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:35:48.306919Z",
     "start_time": "2023-05-01T21:35:48.228290Z"
    }
   },
   "outputs": [],
   "source": [
    "todos=[]\n",
    "for i in range(len(lista)):\n",
    "    x=[]\n",
    "    for i in list(lista[i]):\n",
    "        frases = i[1].split(' ')\n",
    "        if (len(frases) >=19) and i[0]!=1:\n",
    "            x.append(i)\n",
    "    todos.append(x)\n",
    "    \n",
    "frases_objetivo_ml_teste=[]\n",
    "for i in range(len(todos)):\n",
    "    frases_objetivo_ml_teste.append(todos[i][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "760837d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:35:51.161618Z",
     "start_time": "2023-05-01T21:35:51.150525Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_teste_exp2=[]\n",
    "for lista_frase_obj_prob in frases_objetivo_ml_teste:\n",
    "    cada_paper=[]\n",
    "    for frase_obj_prob in lista_frase_obj_prob:\n",
    "        cada_paper.append(frase_obj_prob[1])\n",
    "    frase_objetivo_teste_exp2.append(cada_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "c28b20b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:35:52.575828Z",
     "start_time": "2023-05-01T21:35:52.525376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1.1093008973424372e-49,\n",
       "   'for each possible sense of the word identify a relatively small number of training examples representative of that sense'),\n",
       "  (8.277624632415869e-61,\n",
       "   'this would indicate that the cost of a large sensetagged training corpus may not be necessary to achieve accurate wordsense disambiguation'),\n",
       "  (2.644463252716067e-64,\n",
       "   'words not only tend to occur in collocations that reliably indicate their sense they tend to occur in multiple such collocations')],\n",
       " [(3.2209589774547606e-57,\n",
       "   'while this experiment shows that statistical models can help make choices in generation it fails as a computational strategy'),\n",
       "  (0.0,\n",
       "   'word lattices are commonly used to model uncertainty in speech recognition waibel and lee and are well adapted for use with ngram models'),\n",
       "  (0.0,\n",
       "   'while true irregular forms eg child i children must be kept in a small exception table the problem of multiple regular patterns usually increases the size of this table dramatically')],\n",
       " [(6.970010849754863e-51,\n",
       "   'it remains to be shown that an accurate broadcoverage parser can improve the performance of a text processing application'),\n",
       "  (1.8691229868780135e-52,\n",
       "   'the word feature value of the internal nodes is intended to contain the lexical head of the nodes constituent'),\n",
       "  (1.6255110728008127e-66,\n",
       "   'a search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses')],\n",
       " [(9.371023847233717e-49,\n",
       "   'it is assumed that there is a correlation between the cooccurrences of words which are translations of each other'),\n",
       "  (6.167434884415482e-50,\n",
       "   'the word cooccurrences were computed on the basis of an english corpus of and a german corpus of million words'),\n",
       "  (2.7566364022867134e-51,\n",
       "   'in a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences')],\n",
       " [(1.1660919812674524e-55,\n",
       "   'to our knowledge very few of the existing work on wsd has been tested and compared on a common data set'),\n",
       "  (3.6977830462439706e-56,\n",
       "   'when tested on a common data set our wsd program gives higher classification accuracy than previous work on wsd'),\n",
       "  (2.0424155483081886e-86,\n",
       "   'we draw our sentences containing the occurrences of the words listed above from the combined corpus of the million word brown corpus and the million word wall street journal wsj corpus')],\n",
       " [(0.0,\n",
       "   'yet for intentionally knock twice this is not the case these adverbs do not commute and the semantics are distinct'),\n",
       "  (0.0,\n",
       "   'yes a different vides flexibility about which parse represents its kind of efficient parser can be built for this case class'),\n",
       "  (0.0,\n",
       "   'wittenburg assumes only one reading semantically so just one of its anala ccg fragment lacking orderchanging or higher yses fg is discovered while parsing')],\n",
       " [(1.1491304230569287e-55,\n",
       "   'thus these algorithms improve performance not only on the measures that they were designed for but also on related criteria'),\n",
       "  (1.5603566156239453e-61,\n",
       "   'we propose that by creating algorithms that optimize the evaluation criterion rather than some related criterion improved performance can be achieved'),\n",
       "  (0.0,\n",
       "   'we present two new algorithms the labelled recall algorithm which maximizes the expected labelled recall rate and the bracketed recall algorithm which maximizes the bracketed recall rate')],\n",
       " [(3.409307959317717e-52,\n",
       "   'the distance measure could be extended to capture more context such as other words or tags in the sentence'),\n",
       "  (8.551592281276764e-64,\n",
       "   'we have shown that a simple statistical model based on dependencies between words can parse wall street journal news text with high accuracy'),\n",
       "  (0.0,\n",
       "   'with a beam search strategy parsing speed can be improved to over sentences a minute with negligible loss in accuracy')],\n",
       " [(4.766667313986976e-65,\n",
       "   'typically values of or higher for this measure provide evidence of good reliability while values of or greater indicate high reliability'),\n",
       "  (0.0,\n",
       "   'with few exceptions they rely on intuitive analyses of topic structure operational definitions of discourselevel properties eg interpreting paragraph breaks as discourse segment boundaries or theoryneutral discourse segmentations where subjects are given instructions to simply mark changes in topic'),\n",
       "  (0.0,\n",
       "   'with a view toward automatically segmenting a spoken discourse we would like to directly classify phrases of all three discourse categories')],\n",
       " [(0.0,\n",
       "   'written anew it probably would have been about lines characterize the relative performance of two techniques it is necessary to consider multiple training set sizes and to try both bigram and trigram models'),\n",
       "  (0.0,\n",
       "   'while the original paper katz uses a single parameter k we instead use a different k for each n gt kn'),\n",
       "  (0.0,\n",
       "   'while smoothing is a central issue in language modeling the literature lacks a definitive comparison between the many existing techniques')],\n",
       " [(7.0406882911790175e-56,\n",
       "   'figure a shows that accuracy increases with batch size only up to a point and then starts to decrease'),\n",
       "  (5.35546543388415e-57,\n",
       "   'many corpusbased methods for natural language processing nlp are based on supervised training acquiring information from a manually annotated corpus'),\n",
       "  (6.215830875652065e-59,\n",
       "   'when training a bigram model indeed any hmm this is not true as each word is dependent on that before it')],\n",
       " [(3.1338265231803053e-52,\n",
       "   'eisner proposes dependency models and gives results that show that a generative model similar to model performs best of the three'),\n",
       "  (2.270800758032744e-58,\n",
       "   'results on wall street journal text show that the parser performs at constituent precisionrecall an average improvement of over collins'),\n",
       "  (0.0,\n",
       "   'when parsing the pos tags allowed for each word are limited to those which have been seen in training data for that word')],\n",
       " [(5.934842017425446e-54,\n",
       "   'wsd is useful in many natural language tasks such as choosing the correct word in machine translation and coreference resolution'),\n",
       "  (5.921931355737183e-57,\n",
       "   'we also showed as a baseline the performance of the simple strategy of always choosing the first sense of a word in the wordnet'),\n",
       "  (3.321603758701119e-62,\n",
       "   'in our approach a local context of a word is defined in terms of the syntactic dependencies between the word and other words in the same sentence')],\n",
       " [(1.2862586011586015e-73,\n",
       "   'we first discuss the key concepts on which our approach relies section and the corpus analysis section that provides the empirical data for our rhetorical parsing algorithm'),\n",
       "  (0.0,\n",
       "   'with its distant orbit percent farther from the sun than earth and slim atmospheric blanket mars experiences frigid weather conditions surface temperatures typically average about degrees celsius degrees fahrenheit at the equator and can dip to degrees c near the poles only the midday sun at tropical latitudes is warm enough to thaw ice on occasion but any liquid water formed in this way would evaporate almost instantly because of the low atmospheric pressure although the atmosphere holds a small amount of water and waterice clouds sometimes develop most martian weather involves blowing dust or carbon dioxide each win terfor example a blizzard of frozen carbon dioxide rages over one pole and a few meters of this dryice snow accumulate as previously frozen carbon dioxide evaporates from the opposite polar cap yet even on the summer pole where the sun remains in the sky all day long temperatures never warm enough to melt frozen waterm since parenthetical information is related only to the elementary unit that it belongs to we do not assign it an elementary textual unit status'),\n",
       "  (0.0,\n",
       "   'when we began this research no empirical data supported the extent to which this ambiguity characterizes natural language texts')],\n",
       " [(0.0,\n",
       "   'with this approach the english sound k corresponds to one of t ka ki ku r ke or ko depending on its context'),\n",
       "  (0.0,\n",
       "   'while it is difficult to judge overall accuracysome of the phases are onomatopoetic and others are simply too hard even for good human translatorsit is easier to identify system weaknesses and most of these lie in the pw model'),\n",
       "  (0.0,\n",
       "   'when word separators are removed from the katakana phrases rendering the task exceedingly difficult for people the machines performance is unchanged')],\n",
       " [(5.611271989567632e-73,\n",
       "   'our method achieves high precision more than and while our focus to date has been on adjectives it can be directly applied to other word classes'),\n",
       "  (0.0,\n",
       "   'while no direct indicators of positive or negative semantic orientation have been proposed we demonstrate that conjunctions between adjectives provide indirect information about orientation'),\n",
       "  (0.0,\n",
       "   'we were unable to reach a unique label out of context for several adjectives which we removed from consideration for example cheap is positive if it is used as a synonym of inexpensive but negative if it implies inferior quality')],\n",
       " [(1.2438696081257058e-57,\n",
       "   'finally to our knowledge we are the first to propose using user satisfaction to determine weights on factors related to performance'),\n",
       "  (4.446243648852192e-58,\n",
       "   'a single user satisfaction measure can be calculated from a single question or as the mean of a set of ratings'),\n",
       "  (2.0888079413812774e-71,\n",
       "   'section describes the use of linear regression and user satisfaction to estimate the relative contribution of the success and cost measures in a single performance function')],\n",
       " [(4.2991795682946976e-48,\n",
       "   'the transformations applied to the test set improved the score from f to a reduction in the error rate'),\n",
       "  (7.87194295012006e-52,\n",
       "   'this technique provides a simple algorithm for learning a sequence of rules that can be applied to various nlp tasks'),\n",
       "  (4.843380305701514e-53,\n",
       "   'a variety of methods have recently been developed to perform word segmentation and the results have been published widely')],\n",
       " [(1.0148409384204343e-49,\n",
       "   'we define the recall of a wordtoword translation model as the fraction of the bitext vocabulary represented in the model'),\n",
       "  (1.7019633672603327e-64,\n",
       "   'in addition to the quantitative differences between the wordtoword model and the ibm model there is an important qualitative difference illustrated in figure'),\n",
       "  (3.1340585097582043e-69,\n",
       "   'for these applications we have designed a fast algorithm for estimating a partial translation model which accounts for translational equivalence only at the word level')],\n",
       " [(6.621389073517745e-55,\n",
       "   'one approach for detecting syntactic patterns is to obtain a full parse of a sentence and then extract the required patterns'),\n",
       "  (5.8859718291567986e-58,\n",
       "   'syntactic patterns are useful also for many basic computational linguistic tasks such as statistical word similarity and various disambiguation problems'),\n",
       "  (1.7755338411367205e-63,\n",
       "   'recognizing shallow linguistic patterns such as basic syntactic relationships between words is a common task in applied natural language and text processing')],\n",
       " [(5.737969484788221e-52,\n",
       "   'our scoring algorithm takes this into account and computes a final precision of and for the two responses respectively'),\n",
       "  (2.399286249146163e-70,\n",
       "   'the numbers computed with respect to each entity in the document are then combined to produce final precision and recall numbers for the entire output'),\n",
       "  (1.4769633063547314e-92,\n",
       "   'in addition we also describe a scoring algorithm for evaluating the crossdocument coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the muc within document coreference task')],\n",
       " [(8.957855842258512e-67,\n",
       "   'these three components form a highly relational and tightly integrated whole elements in each may point to elements in the other two'),\n",
       "  (0.0,\n",
       "   'with the exception of the example sentence extraction component all the software modules are highly interactive and have substantial user interface requirements'),\n",
       "  (0.0,\n",
       "   'we are building a constituent type identifier which will semiautomatically assign grammatical function gf and phrase type pt attributes to these femarked constituents eliminating the need for annotators to mark these')],\n",
       " [(4.6428473148084e-53,\n",
       "   'the test set was not used in any way in training so the test set does contain unknown words'),\n",
       "  (1.2597565516192988e-62,\n",
       "   'in this paper we showed that the error distributions for three popular state of the art part of speech taggers are highly complementary'),\n",
       "  (1.5165096031538154e-63,\n",
       "   'in this paper we first show that the errors made from three different state of the art part of speech taggers are strongly complementary')],\n",
       " [(1.0060618304289763e-51,\n",
       "   'using this simple algorithm with a naive heuristic for matching rules we achieve surprising accuracy in an evaluation on the'),\n",
       "  (3.415539067169042e-59,\n",
       "   'by evaluating on both corpora we can measure the effect of noun phrase complexity on the treebank approach to base np identification'),\n",
       "  (2.474123729474297e-67,\n",
       "   'for all experiments we derived the training pruning and testing sets from the sections of wall street journal distributed with the penn treebank ii')],\n",
       " [(0.0,\n",
       "   'we then scan all the derivations in the development set and for each occurrence of the elementary event ym xm in derivationwtk we accumulate the value w tk in the cmym xm counter to be used in the next iteration'),\n",
       "  (0.0,\n",
       "   'we achieved a reduction in testdata perplexity bringing an improvement over a deleted interpolation trigram model whose perplexity was on the same trainingtest data the reduction is statistically significant according to a sign test'),\n",
       "  (0.0,\n",
       "   'w t neednt be a constituent but for the parses where it is there is no restriction on which of its words is the headword or what is the nonterminal label that accompanies the headword')],\n",
       " [(1.5522007219208154e-52,\n",
       "   'when a data driven method is used a model is automatically learned from the implicit structure of an annotated training corpus'),\n",
       "  (1.1166673167483967e-60,\n",
       "   'the second stage can be provided with the first level outputs and with additional information eg about the original input pattern'),\n",
       "  (5.406008278336162e-64,\n",
       "   'all combination taggers outperform their best component with the best combination showing a lower error rate than the best individual tagger')],\n",
       " [(3.5444037388154593e-66,\n",
       "   'the merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs'),\n",
       "  (0.0,\n",
       "   'with simwn and simroget we transform wordnet and roget into the same format as the automatically constructed thesauri in the previous section'),\n",
       "  (0.0,\n",
       "   'while previous methods rely on indirect tasks or subjective judgments our method allows direct and objective comparison between automatically and manually constructed thesauri')],\n",
       " [(8.527766612812791e-55,\n",
       "   'several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task'),\n",
       "  (1.6573552859990503e-59,\n",
       "   'the accuracy of our technique approaches the accuracy of the best supervised methods and does so with only a tiny fraction of the supervision'),\n",
       "  (7.709771353707201e-85,\n",
       "   'the unsupervised algorithm for prepositional phrase attachment presented here is the only algorithm in the published literature that can significantly outperform the baseline without using data derived from a treebank or parser')],\n",
       " [(3.8008439990425223e-50,\n",
       "   'the input to the algorithm is a parsed corpus and a set of initial seed words for the desired category'),\n",
       "  (3.1381064305064446e-53,\n",
       "   'we will also present some experimental results from two corpora and discuss criteria for judging the quality of the output'),\n",
       "  (3.7960175043120007e-54,\n",
       "   'the relationship between the nouns in a compound noun is very different from that in the other constructions we are considering')],\n",
       " [(2.146524574868491e-58,\n",
       "   'the centering model describes the relation between the focus of attention the choices of referring expressions and the perceived coherence of discourse'),\n",
       "  (0.0,\n",
       "   'with respect to any two discourse entities x uttx pas x and y utty posy uttx and utty specifying the current utterance ui or the preceding utterance u i set up the following ordering constraints on elements in the slist table'),\n",
       "  (0.0,\n",
       "   'while tensed clauses are defined as utterances on their own untensed clauses are processed with the main clause so that the cflist of the main clause contains the elements of the untensed embedded clause')],\n",
       " [(0.0,\n",
       "   'when grouped by average performance they fell into several coherent classes which corresponded to the extent to which the functions focused on the intersection of the supports regions of positive probability of the distributions'),\n",
       "  (0.0,\n",
       "   'we would not be able to tell whether the cause was an inherent deficiency in the l norm or just a poor choice of weight function perhaps li q r would have yielded better estimates'),\n",
       "  (0.0,\n",
       "   'we treat a value of as indicating extreme dissimilarity it is worth noting at this point that there are several wellknown measures from the nlp literature that we have omitted from our experiments')],\n",
       " [(3.6693728424264455e-52,\n",
       "   'overall about of the top words for each seed are parts and about of the top for each seed'),\n",
       "  (1.4167401209424744e-54,\n",
       "   'finally we order the possible parts by the likelihood that they are true parts according to some appropriate metric'),\n",
       "  (6.4683982495458295e-56,\n",
       "   'given a very large corpus our method finds part words with accuracy for the top words as ranked by the system')],\n",
       " [(9.358087675252955e-87,\n",
       "   'the method is applicable to any natural language where text samples of sufficient size computational morphology and a robust parser capable of extracting subcategorization frames with their fillers are available'),\n",
       "  (0.0,\n",
       "   'when compared to levin s toplevel verb classes we found an agreement of our classification with her class of verbs of changes of state except for the last three verbs in the list in fig'),\n",
       "  (0.0,\n",
       "   'we will sketch an understanding of the lexical representations induced by latentclass labeling in terms of the linguistic theories mentioned above aiming at an interpretation which combines computational learnability linguistic motivation and denotationalsemantic adequacy')],\n",
       " [(4.791302241280378e-66,\n",
       "   'recall from the previous discussion that we do not assume any kind of semantic relationship among the hypernyms listed for a particular cluster'),\n",
       "  (0.0,\n",
       "   'wordnet has been an important research tool but it is insufficient for domainspecific text such as that encountered in the mucs message understanding conferences'),\n",
       "  (0.0,\n",
       "   'within those columns majority lists the opinion of the majority of judges and any indicates the hypernyms that were accepted by even one of the judges')],\n",
       " [(0.0,\n",
       "   'while their kappa results are very good for other tags the opinionstatement tagging was not very successful the distinction was very hard to make by labelers and accounted for a large proportion of our interlabeler error jurafsky et al'),\n",
       "  (0.0,\n",
       "   'when there is such evidence we propose using the latent class model to correct the disagreements this model posits an unobserved latent variable to explain the correlations among the judges observations'),\n",
       "  (0.0,\n",
       "   'when disagreement is symmetric the differences between the actual counts and the counts expected if the judges decisions were not correlated are symmetric that is snii for i j where i is the difference from independence')],\n",
       " [(4.1066502040493345e-62,\n",
       "   'in fact the difference of mutual information values appear to be more important to the phrasal similarity than the similarity of individual words'),\n",
       "  (1.0433680097252879e-94,\n",
       "   'to find out the benefit of using the dependency relationships identified by a parser instead of simple cooccurrence relationships between words we also created a database of the cooccurrence relationship between partofspeech tagged words'),\n",
       "  (0.0,\n",
       "   'when the frequency count is reasonably large eg greater than a bin zn we further assume that the estimations of pa pbia and pcia in are accurate')],\n",
       " [(4.0695563550777615e-47,\n",
       "   'in all cases the score for a set of questions was the average of the scores for each question'),\n",
       "  (2.5920507704674946e-52,\n",
       "   'our system represents the information content of a sentence both question and text sentences as the set of words in the sentence'),\n",
       "  (0.0,\n",
       "   'with respect to ease of answer key preparation pampr and autsent are clearly superior since they use the publisherprovided answer key')],\n",
       " [(4.0655474837254165e-53,\n",
       "   'given the distribution of definite np types in our test set this would result in recall of and precision of'),\n",
       "  (0.0,\n",
       "   'while examining the si extractions we found many similar nps for example the salvadoran government the guatemalan government and the us government the similarities indicate that some head nouns when premodified represent existential entities'),\n",
       "  (0.0,\n",
       "   'when we say that a definite np is existential we say this because it completely specifies a cognitive representation of the entity in the readers mind')],\n",
       " [(1.922071952257273e-56,\n",
       "   'our final results dependency accuracy represent good progress towards the accuracy of the parser on english wall street journal text'),\n",
       "  (1.545897659271359e-56,\n",
       "   'in test data the correct answer was not available but the pos tagger output could be used if desired'),\n",
       "  (1.1496544552477556e-59,\n",
       "   'collins describes results of accuracy in recovering dependencies on section of the penn wall street journal treebank using model of collins')],\n",
       " [(2.7566364022867134e-51,\n",
       "   'in a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences'),\n",
       "  (7.1266257693769e-55,\n",
       "   'this was done on the basis of a list of approximately german and another list of about english function words'),\n",
       "  (1.6008431896065537e-56,\n",
       "   'our method is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different languages')],\n",
       " [(5.304913989529069e-60,\n",
       "   'precision is estimated as the proportion of pages judged good by strand that were also judged to be good ie'),\n",
       "  (0.0,\n",
       "   'yes by both judges this figure is recall is estimated as the number of pairs that should have been judged good by strand ie that recieved a yes from both judges that strand indeed marked good this figure is'),\n",
       "  (0.0,\n",
       "   'using the cases where the two human judgments agree as ground truth precision of the system is estimated at and recall at')],\n",
       " [(1.887651027114435e-53,\n",
       "   'the estimated models are able to identify the correct parse from the set of all possible parses approximately of the time'),\n",
       "  (9.527209599636277e-67,\n",
       "   'one of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses'),\n",
       "  (0.0,\n",
       "   'with a small number of features the correctparses estimator typically scores better than the pseudolikelihood estimator on the correctparses evaluation metric but the pseudolikelihood estimator always scores better on the pseudolikelihood evaluation metric')],\n",
       " [(9.532835613539866e-58,\n",
       "   'the evaluation task for the content selection stage is to measure how well we identify common phrases throughout multiple sentences'),\n",
       "  (5.132131724293808e-59,\n",
       "   'we will be investigating what factors influence the combination process and how they can be computed from input articles'),\n",
       "  (1.3268415358363891e-59,\n",
       "   'given the theme shown in figure how can we determine which phrases should be selected to form the summary content')],\n",
       " [(1.0582379010295352e-82,\n",
       "   'task participants could choose to either do all tasks focus on the time expression task focus on the event task or focus on the four temporal relation tasks'),\n",
       "  (0.0,\n",
       "   'work on the english corpus was supported under the nsfcri grant towards a comprehensive linguistic annotation of language and the nsfint project sustainable interoperability for language technology silt funded by the national science foundation'),\n",
       "  (0.0,\n",
       "   'with the task decomposition allowed by bat it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful')],\n",
       " [(4.5747540754672646e-51,\n",
       "   'for each target word participants were provided with a training set in order to learn the senses of that word'),\n",
       "  (1.443300027644856e-60,\n",
       "   'systems answers were evaluated in an unsupervised manner by using two clustering evaluation measures and a supervised manner in a wsd task'),\n",
       "  (0.0,\n",
       "   'word senses are more beneficial than simple word forms for a variety of tasks including information retrieval machine translation and others pantel and lin')],\n",
       " [(3.4708145358546815e-56,\n",
       "   'overall the results suggest that the bidirectional and no entailment categories are more problematic than forward and backward judgments'),\n",
       "  (1.021445290469941e-56,\n",
       "   'for the other language pairs the results are lower with only out of participants above the two baselines in all datasets'),\n",
       "  (3.698162456478639e-57,\n",
       "   'in one run run they are used to train a classifier that assigns separate entailment judgments for each direction')],\n",
       " [(0.0,\n",
       "   'we would like to thank inderjeet mani wlodek zadrozny rie kubota ando joyce chai and nanda kambhatla for their valuable feedback'),\n",
       "  (0.0,\n",
       "   'we would also like to thank carl sable minyen kan dave evans adam budzikowski and veronika horvath for their help with the evaluation'),\n",
       "  (0.0,\n",
       "   'we would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization')],\n",
       " [(0.0,\n",
       "   'we would like to be able to incorporate semantics for an arbitrarily large number of words and lsa quickly becomes impractical on large sets'),\n",
       "  (0.0,\n",
       "   'we introduce a semanticbased algorithm for learning morphology which only proposes affixes when the stem and stemplusaffix are sufficiently similar semantically'),\n",
       "  (0.0,\n",
       "   'we insert words into a trie figure and extract potential affixes by observing those places in the trie where branching occurs')],\n",
       " [(0.0,\n",
       "   'whereas earlier methods all share the same basic intuition ie that similar words occur in similar contexts i formalise this in a slightly different way each word defines a probability distribution over all contexts namely the probability of the context given the word'),\n",
       "  (0.0,\n",
       "   'we can then measure the similarity of words by the similarity of their context distributions using the kullbackleibler kl divergence as a distance function'),\n",
       "  (0.0,\n",
       "   'we can model the context distribution as being the product of independent distributions for each relative position in this case the kl divergence is the sum of the divergences for each independent distribution')],\n",
       " [(0.0,\n",
       "   'while the bbn model does not perform at the level of model of collins on wall street journal text it is also less languagedependent eschewing the distance metric which relied on specific features of the english treebank in favor of the bigrams on nonterminals model'),\n",
       "  (0.0,\n",
       "   'while results for the two languages are far from equal we believe that further tuning of the head rules and analysis of development test set errors will yield significant performance gains on chinese to close the gap'),\n",
       "  (0.0,\n",
       "   'while more investigation is required we suspect part of the difference may be due to the fact that currently the bbn model uses languagespecific rules to guess part of speech tags for unknown words')],\n",
       " [(1.2548337801960378e-43,\n",
       "   'in this way the information that a word is the first word in a sentence is available to the tagger'),\n",
       "  (9.391827847065355e-51,\n",
       "   'a large number of words including many of the most common words can have more than one syntactic category'),\n",
       "  (4.8363553312187755e-68,\n",
       "   'rare words are defined to be words that appear less than a certain number of times in the training data here the value was used')],\n",
       " [(0.0,\n",
       "   'with the input stimulusfsn the output is stimuli rather than the incorrect stimuluses that would follow from the application of the more general rule in'),\n",
       "  (0.0,\n",
       "   'when the flex rule matches the input addressiv for example the c function npvordform defined elsewhere in the generator is called to determine the word form corresponding to the input the function deletes the inflection type and pos label specifications and the delimiters removes the last character of the lemma and finally attaches the characters es the word form generated is thus addresses'),\n",
       "  (0.0,\n",
       "   'we present such a component a fast and robust morphological generator for english based on finitestate techniques that generates a word form given a specification of the lemma partofspeech and the type of inflection required')],\n",
       " [(0.0,\n",
       "   'while this result is satisfying further investigation reveals that deterioration in the quality of the labeled data accumulated by cotraining hinders further improvement'),\n",
       "  (0.0,\n",
       "   'while previous research summarized in section has investigated the theoretical basis of cotraining this study is motivated by practical concerns'),\n",
       "  (0.0,\n",
       "   'we seek to apply the cotraining paradigm to problems in natural language learning with the goal of reducing the amount of humanannotated data required for developing natural language processing components')],\n",
       " [(1.5092177784582e-52,\n",
       "   'we also compared to a method in which the lexical indicator variables were used as input to a neural network'),\n",
       "  (6.1413443304791914e-55,\n",
       "   'our goals are more general than those of information extraction and so this work should be helpful for that task'),\n",
       "  (6.850050851635718e-56,\n",
       "   'our results also indicate that the second noun the head is more important in determining the relationships than the first one')],\n",
       " [(0.0,\n",
       "   'yet we can filter by pruning ngrams whose beginning or ending word is among the top n most frequent words'),\n",
       "  (0.0,\n",
       "   'yet there is still another maybe or the reciprocal of the words frequencies semantic compositionality is not always bad'),\n",
       "  (0.0,\n",
       "   'with permission and sufficient time one can repeatedly query websites that host large collections of mrds and evaluate each ngram')],\n",
       " [(0.0,\n",
       "   'xu and croft offered a trainable method call local context analysis lca which replaces each query term with frequently cooccurring words'),\n",
       "  (0.0,\n",
       "   'we discovered that lsa is a more accurate measure of similarity than the cosine metric stemming does not always improve segmentation accuracy and ranking is crucial to cosine but not lsa'),\n",
       "  (0.0,\n",
       "   'this procedure is useful in information retrieval hearst and plaunt hearst yaari reynar summarisation reynar text understanding anaphora resolution kozima language modelling morris and hirst beeferman et al and text navigation choi b')],\n",
       " [(3.7695466666584135e-52,\n",
       "   'the parsing models of charniak and collins add more complex features to the parsing model that we use as our baseline'),\n",
       "  (6.085654079385655e-69,\n",
       "   'our implementation of collins model performs at precision and recall of labeled parse constituents on the standard wall street journal training and test sets'),\n",
       "  (0.0,\n",
       "   'while this has allowed for quantitative comparison of parsing techniques it has left open the question of how other types of text might affect parser performance and how portable parsing models are across corpora')],\n",
       " [(0.0,\n",
       "   'when assigning timestamps we analyze both implicit time references mainly through the tense system and explicit ones temporal adverbials such as on monday in etc'),\n",
       "  (0.0,\n",
       "   'when assigning a time to an event we select the time to be either the most recently assigned date or if the value of the most recently assigned date is undefined to the date of the article'),\n",
       "  (0.0,\n",
       "   'we ran the timestamper program on two types of data list of eventclauses extracted by the event identifier and list of eventclauses created manually')],\n",
       " [(1.7688634850536789e-56,\n",
       "   'the probability module implements classes that encode frequency distributions and probability distributions including a variety of statistical smoothing techniques'),\n",
       "  (8.59283578887578e-57,\n",
       "   'finally it can be used to show how these individual rules combine to find a complete parse for a given sentence'),\n",
       "  (0.0,\n",
       "   'we used nltk as a basis for the assignments and student projects in cis an introductory computational linguistics class taught at the university of pennsylvania')],\n",
       " [(1.1094618379606031e-52,\n",
       "   'in the experiments we show this class splitting technique not only enables the feasible training but also improves the accuracy'),\n",
       "  (2.705216983307746e-55,\n",
       "   'varying the size of training data we compared the change in the training time and the accuracy with and without the class splitting'),\n",
       "  (8.45554545325963e-56,\n",
       "   'in addition it is difficult to compare the techniques used in each study because they used a closed and different corpus')],\n",
       " [(2.025869383635651e-45,\n",
       "   'in the second method the cost function is defined as the maximum likelihood of the data given the model'),\n",
       "  (7.657611592073511e-52,\n",
       "   'the first word tokens were used as training data and the following word tokens were used as test data'),\n",
       "  (1.1408408012869341e-54,\n",
       "   'figure shows the development of the average cost per word as a function of the increasing amount of source text')],\n",
       " [(7.440497276028804e-55,\n",
       "   'if other training data is not available a number of these sentences are presented to the users for tagging in stage'),\n",
       "  (1.092979449479386e-70,\n",
       "   'these initial set of tagged examples will then be used to train the two classifiers described in section and annotate an additional set of examples'),\n",
       "  (2.773542770329654e-72,\n",
       "   'next this tagged collection is used as training data and active learning is used to identify in the remaining corpus the examples that are hard to tag')],\n",
       " [(1.0900830925227136e-54,\n",
       "   'the statistical models are trained on parallel corpora large amounts of text in one language along with their translation in another'),\n",
       "  (6.755740370121273e-55,\n",
       "   'this local context has to be translated into the other language and we can search the word with the most similar context'),\n",
       "  (2.5354397733179472e-55,\n",
       "   'the baseline for this task choosing words at random results on average in only correct mapping in the entire lexicon')],\n",
       " [(7.163807691484351e-55,\n",
       "   'we evaluate existing and new similarity metrics for thesaurus extraction and experiment with the tradeoff between extraction performance and efficiency'),\n",
       "  (2.9458130901801946e-55,\n",
       "   'in this paper we evaluate some existing similarity metrics and propose and motivate a new metric which outperforms the existing metrics'),\n",
       "  (2.2650512304305267e-62,\n",
       "   'in these experiments we have proposed new measure and weight functions that as our evaluation has shown significantly outperform existing similarity functions')],\n",
       " [(6.424963102619032e-57,\n",
       "   'initial unigram results the classification accuracies resulting from using only unigrams as features are shown in line of figure'),\n",
       "  (9.815970889328245e-66,\n",
       "   'hence if context is in fact important as our intuitions suggest bigrams are not effective at capturing it in our setting'),\n",
       "  (0.0,\n",
       "   'yet the results shown in line of figure are relatively poor the adjectives provide less useful information than unigram presence')],\n",
       " [(8.902006871948325e-54,\n",
       "   'the nouns extracted by these patterns become candidates for the lexicon and are placed in a candidate word pool'),\n",
       "  (0.0,\n",
       "   'while metabootstrapping trusts individual extraction patterns to make unilateral decisions basilisk gathers collective evidence from a large set of extraction patterns'),\n",
       "  (0.0,\n",
       "   'we will use the abbreviation cat to indicate that only one semantic category was bootstrapped and mcat to indicate that multiple semantic categories were simultaneously bootstrapped')],\n",
       " [(3.2868973722490585e-47,\n",
       "   'the translation model captures the translation of source language words into the target language and the reordering of those words'),\n",
       "  (3.0646055878286045e-57,\n",
       "   'to accomplish this we defined a simple heuristic to detect phrasal translations so we can filter them if desired'),\n",
       "  (9.218205846312561e-61,\n",
       "   'an alignment is a mapping between the words in a string in one language and the translations of those words in a string in another language')],\n",
       " [(1.3995194289573018e-54,\n",
       "   'the evaluation shows that the grammar is at a stage where domain adaptation is possible in a reasonable amount of time'),\n",
       "  (2.0524582898644572e-57,\n",
       "   'the grammar is being developed in a multilingual context where much value is placed on parallel and consistent semantic representations'),\n",
       "  (1.7885875738347887e-58,\n",
       "   'the grammar is created for use in real world applications such that robustness and performance issues play an important role')],\n",
       " [(0.0,\n",
       "   'work is also progressing on establishing a standard relational database using postgresql for storing information for the lexical entries themselves improving both scalability and clarity compared to the current simple text file representation'),\n",
       "  (0.0,\n",
       "   'while the development of the matrix will be built largely on the lkb platform support will also be needed for using the emerging grammars on other processing platforms and for linking to other packages for preprocessing the linguistic input'),\n",
       "  (0.0,\n",
       "   'while the details of modifier placement which parts of speech can modify which kinds of phrases etc differ across languages we believe that all languages display a distinction between scopal and intersective modification')],\n",
       " [(0.0,\n",
       "   'within lfg fstructures are meant to encode a language universal level of analysis allowing for crosslinguistic parallelism at this level of abstraction'),\n",
       "  (0.0,\n",
       "   'when this is noticed via the featuretable comparison it is determined why one grammar needs the feature and the other does not and thus it may be possible to eliminate the feature in one grammar or to add it to another'),\n",
       "  (0.0,\n",
       "   'we report on the parallel grammar pargram project which uses the xle parser and grammar development platform for six languages english')],\n",
       " [(2.703061372431397e-56,\n",
       "   'this section describes the general formulation of the probabilistic model for parsing which has been applied to japanese statistical dependency analysis'),\n",
       "  (0.0,\n",
       "   'we used sentences from the articles on january st to january th as training examples and sentences from the articles on january th as the test data'),\n",
       "  (0.0,\n",
       "   'we propose a new method that is simple and efficient since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side')],\n",
       " [(1.5204123094453547e-56,\n",
       "   'the gradient of a function is a vector which points in the direction in which the functions value increases most rapidly'),\n",
       "  (0.0,\n",
       "   'while the loglikelihood function for me models in is twice differentiable for large scale problems the evaluation of the hessian matrix is computationally impractical and newtons method is not competitive with iterative scaling or first order methods'),\n",
       "  (0.0,\n",
       "   'while parameter estimation for me models is conceptually straightforward in practice me models for typical natural language tasks are very large and may well contain many thousands of free parameters')],\n",
       " [(4.827299187578132e-53,\n",
       "   'we used a separate annotated tuning corpus of documents with a total of sentences to establish some experimental parameters'),\n",
       "  (2.881697641449846e-55,\n",
       "   'we explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms'),\n",
       "  (8.12632295543071e-58,\n",
       "   'the annotated corpus used to train and test our subjectivity classifiers the experiment corpus consists of documents with a total of sentences')],\n",
       " [(0.0,\n",
       "   'with this type of agglomerative clustering the most similar pages are clustered first and outliers are assigned as stragglers at the top levels of the cluster tree'),\n",
       "  (0.0,\n",
       "   'while word senses and translation ambiguities may typically have alternative meanings that must be resolved through context a personal name such as jim clark may potentially refer to hundreds or thousands of distinct individuals'),\n",
       "  (0.0,\n",
       "   'while these names could be used in a undifferentiated vectorbased bagofwords model further accuracy can be gained by extracting specific types of association such as familial relationships eg son wife employment relationships eg manager of and nationality as distinct from simple term cooccurrence in a window')],\n",
       " [(0.0,\n",
       "   'we would like to thank the anonymous reviewers for their helpful comments and also iain rae for computer support'),\n",
       "  (0.0,\n",
       "   'we trained one of the taggers on much more labelled seed data than the other to see how this affects the cotraining process'),\n",
       "  (0.0,\n",
       "   'we leave these experiments to future work but note that there is a large computational cost associated with such experiments')],\n",
       " [(2.441906373254539e-53,\n",
       "   'the first set of features apply to rare words ie those which appear less than times in the training data'),\n",
       "  (8.421081162324005e-57,\n",
       "   'collins also describes a mapping from words to word types which groups words with similar orthographic forms into classes'),\n",
       "  (2.5072865340098395e-61,\n",
       "   'this paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy')],\n",
       " [(0.0,\n",
       "   'within each of the regions a statistical bigram language model is used to compute the likelihood of words occurring within that region named entity type'),\n",
       "  (0.0,\n",
       "   'when no gazetteer or other additional training resources are used the combined system attains a performance of f on the english development data integrating name location and person gazetteers and named entity systems trained on additional more general data reduces the fmeasure error by a factor of to on the english data'),\n",
       "  (0.0,\n",
       "   'we note that the numbers are roughly twice as large for the development data in german as they are for english')],\n",
       " [(0.0,\n",
       "   'when using characterlevel models for wordevaluated tasks one would not want multiple characters inside a single word to receive different labels'),\n",
       "  (0.0,\n",
       "   'we present two models in which the basic units are characters and character grams instead of words and word phrases'),\n",
       "  (0.0,\n",
       "   'we found knowing that the previous word was an other wasnt particularly useful without also knowing its partofspeech eg a preceding preposition might indicate a location')],\n",
       " [(2.016762272827537e-52,\n",
       "   'we have compared two systems that use this basic technique one taking a statistical approach and the other a linguistic approach'),\n",
       "  (3.324774017776133e-74,\n",
       "   'the results of the linguistically motivated approach show that we can build a working system with minimal linguistic knowledge and circumvent the need for large amounts of training data'),\n",
       "  (0.0,\n",
       "   'we used two variants of hmm hedge one which selects headline words from the first words of the story and one which selects words from the first sentence of the story')],\n",
       " [(8.379084322387703e-52,\n",
       "   'in this case we use the highest performing model from section in order to label arguments with semantic roles'),\n",
       "  (6.576254043383475e-59,\n",
       "   'for the latter we use the best performing model from section in order to find semantic roles given syntactic features from the parse'),\n",
       "  (2.7619290296464614e-60,\n",
       "   'the goal of parsing syntactic analysis is ultimately to provide the first step towards giving a semantic interpretation of a string of words')],\n",
       " [(3.419957747789703e-49,\n",
       "   'using the gold standard parses provides an upper bound on the performance of the system based on automatic parses'),\n",
       "  (1.0771150170681396e-51,\n",
       "   'in this paper we examine how the syntactic representations used by different statistical parsers affect the performance of such a system'),\n",
       "  (2.929775752723458e-61,\n",
       "   'head words of nodes in the parse tree are determined using the same deterministic set of head word rules used by collins')],\n",
       " [(4.814054445951015e-58,\n",
       "   'from those nouns also occurring in wordnet we selected the most frequent nouns and a set of low frequency nouns'),\n",
       "  (1.965776749384583e-73,\n",
       "   'we will first explain the wordnetbased distance measure lin and then explain how we determine the similarity between neighbour sets generated using different measures'),\n",
       "  (4.203589572390288e-98,\n",
       "   'in the syntactic domain language models which can be used to evaluate alternative interpretations of text and speech require probabilistic information about words and their cooccurrences which is often not available due to the sparse data problem')],\n",
       " [(1.3527889819457943e-48,\n",
       "   'section describes the data that we use presents our experimental results and shows examples of patterns that are learned'),\n",
       "  (5.546881841700924e-57,\n",
       "   'in our second experiment we used the learned extraction patterns to classify previously unlabeled sentences from the unannotated text collection'),\n",
       "  (1.2917302758486815e-58,\n",
       "   'these patterns match sentences such as the fact is and is a fact which apparently are often used in subjective contexts')],\n",
       " [(5.191115419175569e-45,\n",
       "   'finally we used as one of the features the average semantic orientation score of the words in the sentence'),\n",
       "  (1.472499074573824e-61,\n",
       "   'these labels are used only to provide the correct classification labels during training and evaluation and are not included in the feature space'),\n",
       "  (4.4195564769783303e-67,\n",
       "   'we first discuss how such words are automatically found by our system and then describe the method by which we aggregate this information across the sentence')],\n",
       " [(4.810675081892836e-68,\n",
       "   'the system used allows for different ensemble techniques to be applied meaning that a number of classifiers are generated and then combined to predict the class'),\n",
       "  (3.3401646082699515e-69,\n",
       "   'the machine learning approach used for the experiments is that of rule induction ie the model that is constructed from the given examples consists of a set of rules'),\n",
       "  (0.0,\n",
       "   'when pos patterns are used to extract potential terms the problem lies in how to restrict the number of terms and only keep the ones that are relevant')],\n",
       " [(4.383374855530359e-57,\n",
       "   'in the closed test participants could only use training material from the training data for the particular corpus being testing on'),\n",
       "  (1.1205443131280146e-71,\n",
       "   'the problem with this literature has always been that it is very hard to compare systems due to the lack of any common standard test set'),\n",
       "  (0.0,\n",
       "   'while we are familiar with the approaches taken in several of the tested systems we leave it up to the individual participants to describe their approaches and hopefully elucidate which aspects of their approaches are most responsible for their successes and failures the participants papers all appear in this volume')],\n",
       " [(4.339055152884751e-53,\n",
       "   'as for each closed track we first extracted all the common words and tokens that appear in the training corpus'),\n",
       "  (0.0,\n",
       "   'we have taken six tracks academia sinica closed asc u penn chinese tree bank open and closedctboc hong kong cityu closed hkc peking university open and closedpkoc'),\n",
       "  (0.0,\n",
       "   'we apply to word segmentation classbased hmm which is a generalized approach covering both common words and unknown words wi iff wi is listed in the segmentation lexicon per iff wi is unlisted personal name loc iff wi is unlisted location name org iff wi is unlisted organization name time iff wi is unlisted time expression num iff wi is unlisted numeric expression str iffwi is unlisted symbol string beg iff beginning of a sentence end iff ending of a sentence other otherwise')],\n",
       " [(8.362675792933413e-67,\n",
       "   'in order to get a reliable sense for how good these scores are we compare them with the level of agreement across human judges'),\n",
       "  (1.4470620973492275e-73,\n",
       "   'in terms of relative performance the semantic similarity based approach of methods and outperform the distribution based approach of methods and in terms of fscore on of the sets of results reported'),\n",
       "  (3.1120065666412708e-83,\n",
       "   'these agreement scores give us an upper bound for classification accuracy on each task from which it is possible to benchmark the classification accuracy of the classifiers on that same task')],\n",
       " [(9.173883272685116e-53,\n",
       "   'additionally in lexical acquisition and for word sense disambiguation it is important that related senses of words are identified'),\n",
       "  (7.800400594481033e-57,\n",
       "   'we also give results for comparison obtained on the same data for another wide coverage parser minipar lin b'),\n",
       "  (0.0,\n",
       "   'whilst statistics are useful indicators of noncompositionality there are compositional multiwords which have low values for these statistics yet are highly noncompositional')],\n",
       " [(1.1175619814047252e-61,\n",
       "   'for both tests values closer to indicate random distribution of the data whereas values closer to indicate a strong correlation'),\n",
       "  (0.0,\n",
       "   'with simple decomposable mwes we can expect the constituents and particularly the head to be hypernyms ancestor nodes or synonyms of the mwe'),\n",
       "  (0.0,\n",
       "   'with nondecomposable mwes eg kick the bucket shoot the breeze hot dog no decompositional analysis is possible and the mwe is semantically impenetrable')],\n",
       " [(2.2549832912847112e-52,\n",
       "   'moreover we have shown that in practical parsing the algorithm performs incremental processing for the majority of input structures'),\n",
       "  (0.0,\n",
       "   'we will try to make this more precise in a minute but first we want to discuss the relation between incrementality and determinism'),\n",
       "  (0.0,\n",
       "   'we will represent parser configurations by triples i a where is the stack represented as a list i is the list of remaining input tokens and a is the current arc relation for the dependency graph')],\n",
       " [(0.0,\n",
       "   'with an average of five senses per word the average value for the agreement by chance is measured at resulting in a micro statistic of'),\n",
       "  (0.0,\n",
       "   'we measure two figures microaverage where number of senses agreement by chance and are determined as an average for all words in the set and macroaverage where intertagger agreement agreement by chance and are individually determined for each of the words in the set and then combined in an overall average'),\n",
       "  (0.0,\n",
       "   'we describe in this paper the task definition resources participating systems and comparative results for the english lexical sample task which was organized as part of the senseval evaluation exercise')],\n",
       " [(4.576692953277174e-61,\n",
       "   'therefore how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years'),\n",
       "  (0.0,\n",
       "   'with the duc data we computed pearsons product moment correlation coefficients spearmans rank order correlation coefficients and kendalls correlation coefficients between systems average rouge scores and their human assigned average coverage scores using single reference and multiple references'),\n",
       "  (0.0,\n",
       "   'when multiple references are used we compute pairwise summarylevel rougen between a candidate summary s and every reference ri in the reference set')],\n",
       " [(0.0,\n",
       "   'we thank chuck wooters don baron chris oei and andreas stolcke for software assistance ashley krupski for contributions to the annotation scheme andrei popescubelis for analysis and comments on a release of the meetings and barbara peskin and jane edwards for general advice and feedback'),\n",
       "  (0.0,\n",
       "   'we suggest various ways to group the large set of labels into a smaller set of classes depending on the research focus'),\n",
       "  (0.0,\n",
       "   'we provide a brief summary of the annotation system and labeling procedure interannotator reliability statistics overall distributional statistics a description of auxiliary files distributed with the corpus and information on how to obtain the data')],\n",
       " [(1.0227277732770225e-48,\n",
       "   'we seek an inference algorithm that can produce a coherent labeling of entities and relations in a given sentence'),\n",
       "  (6.273415836577385e-51,\n",
       "   'similarly we assume a learning mechanism that can recognize the semantic relation between two given phrases in a sentence'),\n",
       "  (1.5243138053829327e-53,\n",
       "   'following this work we model inference as an optimization problem and show how to cast it as a linear program')],\n",
       " [(1.1621664047446257e-50,\n",
       "   'these feature vectors are in fact the first order context vectors of the feature words and not target word'),\n",
       "  (4.078132652168343e-61,\n",
       "   'first order context vectors represent the context of each instance of a target word as a vector of features that occur in that context'),\n",
       "  (2.2055529168338e-61,\n",
       "   'most words in natural language have multiple possible meanings that can only be determined by considering the context in which they occur')],\n",
       " [(3.825376479198641e-49,\n",
       "   'section describes the data used in the experiments the evaluation metrics and the models and algorithms used in the learning process'),\n",
       "  (7.746197653281615e-50,\n",
       "   'for this purpose we define a number of features that can be used to define different models of parser state'),\n",
       "  (1.1948621014575202e-54,\n",
       "   'results from the experiments are given in section while conclusions and suggestions for further research are presented in section')],\n",
       " [(0.0,\n",
       "   'while previous programs with similar goals gildea and jurafsky were statisticsbased this tool will be based completely on handcoded rules and lexical resources'),\n",
       "  (0.0,\n",
       "   'whether or not one adapts an a la carte approach nombank and propbank projects provide users with data to recognize regularizations of lexically and syntactically related sentence structures'),\n",
       "  (0.0,\n",
       "   'when complete nombank will provide argument structure for instances of about common nouns in the penn treebank ii corpus')],\n",
       " [(0.0,\n",
       "   'we wanted to explore how well the annotators agreed on relatively abstract classifications such as requires extrapolation from actual findings and thus we refrained from writing instructions such as if the sentence contains a form of suggest then mark it as speculative into the guidelines'),\n",
       "  (0.0,\n",
       "   'we report here the isolation ofhuman zinc finger hzf a putative zincfinger transcription factor by motifdirected differential display of mrna extracted from histaminestimulated human vein endothelial cells'),\n",
       "  (0.0,\n",
       "   'we may use it to test a kr systems ability to predict b as the connecting aspect between a and c and to do this using data prior to the publication')],\n",
       " [(3.101614192860336e-55,\n",
       "   'we are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process'),\n",
       "  (3.962991420344817e-65,\n",
       "   'in this project the syntactic and semantic annotation is being done on a corpus which is also being annotated for entities as described in section'),\n",
       "  (0.0,\n",
       "   'work over the last few years in literature data mining for biology has progressed from linguistically unsophisticated models to the adaptation of natural language processing nlp techniques that use full parsers park et al yakushiji et al and coreference to extract relations that span multiple sentences pustejovsky et al hahn et al for an overview see hirschman et al')],\n",
       " [(1.1928740660150024e-50,\n",
       "   'in this paper we describe a dynamic programming approach to discriminative parsing that is an alternative to maximum entropy estimation'),\n",
       "  (6.050563541373254e-51,\n",
       "   'for example in the work of collins of the correct parses were not in the candidate pool of best parses'),\n",
       "  (2.909257069828884e-72,\n",
       "   'we provide an efficient algorithm for learning such models and show experimental evidence of the models improved performance over a natural baseline model and a lexicalized probabilistic contextfree grammar')],\n",
       " [(6.338183390044118e-58,\n",
       "   'there are several ways to improve the accuracy of the current algorithm and to detect relations between low frequency verb pairs'),\n",
       "  (1.1839004828562065e-63,\n",
       "   'hence verb semantics could help in many natural language processing nlp tasks that deal with events or relations between entities'),\n",
       "  (0.0,\n",
       "   'y or at least x yed or at least xed not only xed but yed not just xed but yed the probabilities in the denominator are difficult to calculate directly from search engine results')],\n",
       " [(3.3417849155845825e-71,\n",
       "   'our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the web for related syntactic entailment templates'),\n",
       "  (9.684039794599352e-77,\n",
       "   'broad coverage lexicons are widely available or may be constructed using known term acquisition techniques making it a feasible and scalable input requirement'),\n",
       "  (0.0,\n",
       "   'yet this method also suffers from certain limitations a it identifies only templates with prespecified structures b accuracy seems more limited due to the weaker notion of similarity and c coverage is limited to the scope of an available corpus')],\n",
       " [(1.5878408675933205e-57,\n",
       "   'the bilingual parser without the english head span filter gives a small recall improvement on average at similar precision'),\n",
       "  (0.0,\n",
       "   'yamada and knight introduced treetostring alignment on japanese data and gildea performed treetotree alignment on the korean treebank allowing for nonisomorphic structures he applied this to wordtoword alignment'),\n",
       "  (0.0,\n",
       "   'xia et al compare the rule templates of lexicalized tree adjoining grammars extracted from treebanks in english chinese and korean')],\n",
       " [(3.31115826597666e-53,\n",
       "   'one is that for a given verb the majority of the constituents in a syntactic tree are not its semantic arguments'),\n",
       "  (8.965330020222945e-75,\n",
       "   'most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels'),\n",
       "  (6.911993794331646e-92,\n",
       "   'this paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input generally a syntactic parse tree has yet to be fully exploited')],\n",
       " [(2.3921559813083274e-55,\n",
       "   'also the unsupervised nature of the approach highlights an intermediate step of determining the set of possible roles for each argument'),\n",
       "  (1.024230599867279e-58,\n",
       "   'the first step in our algorithm is to use the verb lexicon to determine the argument slots and the roles available for them'),\n",
       "  (0.0,\n",
       "   'while framenet uses semantic roles specific to a particular situation such as speaker message admire adore appreciate cherish enjoy addressee and propbank uses roles specific to a verb such as arg arg arg verbnet uses an intermediate level of thematic roles such as agent theme recipient')],\n",
       " [(1.2694213618410215e-53,\n",
       "   'we approach this problem as one of statistical machine translation smt within the noisy channel model of brown et al'),\n",
       "  (4.3282405204241275e-58,\n",
       "   'to generate paraphrases of a given input a standard smt decoding approach was used this is described in more detail below'),\n",
       "  (6.561070211529736e-75,\n",
       "   'a second important contribution of this work is a method for building and tracking the quality of large alignable monolingual corpora from structured news data on the web')],\n",
       " [(1.3025322270214294e-62,\n",
       "   'traditionally in japanese morphological analysis we assume that a lexicon which lists a pair of a word and its corresponding partofspeech is available'),\n",
       "  (0.0,\n",
       "   'wordlevel templates are employed when the words are lexicalized ie those that belong to particle auxiliary verb or suffix'),\n",
       "  (0.0,\n",
       "   'while the relative rates of lerror and serror are almost the same in hmms and crfs the number of lerrors with memms amounts to which is of total errors and is even larger than that of naive hmms')],\n",
       " [(3.3831943841531054e-60,\n",
       "   'to our knowledge our work is the first to systematically investigate issues of processing architecture and feature representation for chinese pos tagging'),\n",
       "  (2.4424567442664837e-66,\n",
       "   'as a first step in our investigation we built a chinese word segmenter capable of performing word segmentation without using pos tag information'),\n",
       "  (0.0,\n",
       "   'zhou and su investigated an approach to build a chinese analyzer that integrated word segmentation pos tagging and parsing based on a hidden markov model')],\n",
       " [(1.1345067299282456e-49,\n",
       "   'a simple way to accomplish this is to use map adaptation using a prior distribution on the model parameters'),\n",
       "  (1.6520162314177764e-92,\n",
       "   'as expected adding more background training data improves performance but a very small amount of domain specific data also helps significantly if one can make use of it in an effective way'),\n",
       "  (0.0,\n",
       "   'when evaluating on the mismatched outofdomain test data the gram baseline is outperformed by the improvement brought by the adaptation technique using a very small amount of matched bn data kwds is about relative')],\n",
       " [(3.33878744079618e-51,\n",
       "   'this is because each word occurring in the text is highly relevant to the predefined topics to be identified'),\n",
       "  (5.446084685868433e-54,\n",
       "   'as illustrated in section our method can automatically select relevant and compact features from a number of feature candidates'),\n",
       "  (4.523763779580435e-67,\n",
       "   'this information allows us to analyze how the system classifies the input sentence in a category and what kind of features are used in the classification')],\n",
       " [(6.642310976453753e-59,\n",
       "   'text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user'),\n",
       "  (0.0,\n",
       "   'when computing degree centrality we have treated each edge as a vote to determine the overall prestige value of each node'),\n",
       "  (0.0,\n",
       "   'we use pagerank to weight each vote so that a vote that comes from a more prestigious sentence has a greater value in the centrality of a sentence')],\n",
       " [(3.967608905671406e-50,\n",
       "   'since the focus of this paper is the comparison of the performance of different systems we need a set of translation systems'),\n",
       "  (7.468531467293052e-63,\n",
       "   'some researchers have recently used relative human bleu scores by comparing machine bleu scores with high quality human translation scores'),\n",
       "  (3.465373780697669e-75,\n",
       "   'in a third preliminary experiment we compared for each of the broad samples the bleu score for the spanish system against the bleu score for the danish system')],\n",
       " [(2.1164356770165283e-95,\n",
       "   'instead we are defining a different relation which determines a connection between two sentences if there is a similarity relation between them where similarity is measured as a function of their content overlap'),\n",
       "  (0.0,\n",
       "   'while the size of the abstracts ranges from to words with an average size of words we have deliberately selected a very small abstract for the purpose of illustration'),\n",
       "  (0.0,\n",
       "   'while the final vertex scores and therefore rankings differ significantly as compared to their unweighted alternatives the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs')],\n",
       " [(2.0224860238376283e-68,\n",
       "   'in the case of the latter results indicate that our approach may allow for further improvements to be gained given knowledge of the topic of the text'),\n",
       "  (3.026527979309635e-72,\n",
       "   'moreover is not restricted to words of a particular part of speech nor even restricted to single words but can be used with multiple word phrases'),\n",
       "  (1.3234906011556575e-82,\n",
       "   'with these entities tagged a number of classes of features may be extracted representing various relationships between topic entities and value phrases similar to those described in section')],\n",
       " [(1.695050280416491e-55,\n",
       "   'our results show that adding syntactic information to the evaluation metric improves both sentencelevel and corpuslevel correlation with human judgments'),\n",
       "  (1.3429780304700861e-64,\n",
       "   'this type of feature cannot capture the grammaticality of the sentence in part because they do not take into account sentencelevel information'),\n",
       "  (4.304624397723218e-78,\n",
       "   'our experiments measure how well these metrics correlate with human judgments both for individual sentences and over a large test set translated by mt systems of varying quality')],\n",
       " [(9.13508444736962e-57,\n",
       "   'we also measure precision recall and fmeasure calculated with respect to the true values in each of the test data sets'),\n",
       "  (5.253026122559187e-62,\n",
       "   'for nouns and verbs we use a measure of semantic similarity based on wordnet while for the other word classes we apply lexical matching'),\n",
       "  (5.189746597399928e-62,\n",
       "   'in the unsupervised setting the best performance is achieved using a method that combines several similarity metrics into one for an overall accuracy of')],\n",
       " [(0.0,\n",
       "   'without this line algorithm could be considered as a generalization of the jimenez andmarzal algorithm to the case of acyclic monotonic hy pergraphs'),\n",
       "  (0.0,\n",
       "   'with the derivations thus ranked we can introduce anonrecursive representation for derivations that is analogous to the use of backpointers in parser implementa tion'),\n",
       "  (0.0,\n",
       "   'while charniak and johnson propose using an algorithm similar to our algorithm but with multiple passes to improve efficiency')],\n",
       " [(1.3408776437485188e-48,\n",
       "   'this set of features and corresponding actions is then used to train a classifier resulting in a complete parser'),\n",
       "  (2.8912800675976986e-52,\n",
       "   'therefore the number of shift and binary reduce actions is linear with the number of words in the input string'),\n",
       "  (8.901101322622798e-56,\n",
       "   'these instances can be obtained by running the algorithm on a corpus of sentences for which the correct parse trees are known')],\n",
       " [(4.478412115917706e-56,\n",
       "   'we used the charniak parser to get a phrase type feature of a frame element and the parse tree path feature'),\n",
       "  (7.220965502590308e-60,\n",
       "   'as for opinion topic identification little research has been conducted and only in a very limited domain product reviews'),\n",
       "  (3.4093412730650387e-67,\n",
       "   'in product reviews for example opinion topics are often the product itself or its specific features such as design and quality eg')],\n",
       " [(2.795081432285279e-48,\n",
       "   'the results also show that the simple system combination procedure that we have employed is effective in our setting'),\n",
       "  (1.597942154640379e-49,\n",
       "   'on a to quality scale the difference between the phrasebased and syntaxbased systems was on average between and points'),\n",
       "  (5.743518530730255e-76,\n",
       "   'when using a phrasebased translation model one can easily extract the phrase pair the mutual the mutual and use it during the phrasebased model estimation phrase and in decoding')],\n",
       " [(0.0,\n",
       "   'zens and ney describe a noisyor combination where sj is the probability that sj is not in the translation of t and psjti is a lexical probability'),\n",
       "  (0.0,\n",
       "   'we tested different phrasetable smoothing techniques in two different translation settings european language pairs with relatively small corpora and chinese to english translation with large corpora'),\n",
       "  (0.0,\n",
       "   'we show that any type of smoothing is a better idea than the relativefrequency estimates that are often used')],\n",
       " [(5.200494118892326e-49,\n",
       "   'we do not require labeled training data in the new domain to demonstrate an improvement over our baseline models'),\n",
       "  (9.135559286214484e-54,\n",
       "   'here structural learning is different from learning with structured outputs a common paradigm for discriminative natural language processing models'),\n",
       "  (2.976087034738942e-54,\n",
       "   'to the best of our knowledge this is the first work to use unlabeled data from both domains to find feature correspondences')],\n",
       " [(2.6977823225773695e-48,\n",
       "   'in this paper we present a method which extends the applicability of ilp to a more complex set of problems'),\n",
       "  (1.9375537190692675e-48,\n",
       "   'this is an online algorithm that learns by parsing each sentence and comparing the result with a gold standard'),\n",
       "  (2.8833237099585047e-74,\n",
       "   'although slower than the baseline approach our method can still parse large sentences more than tokens in a reasonable amount of time less than a minute')],\n",
       " [(0.0,\n",
       "   'while the developmentset results would induce us to utilize the standard threshold value of which is suboptimal on the test set the bagr agreementlink policy still achieves noticeable improvement over not using agreement links test set vs'),\n",
       "  (0.0,\n",
       "   'while such functionality is well beyond the scope of our current study we are optimistic that we can develop methods to exploit additional types of relationships in future work'),\n",
       "  (0.0,\n",
       "   'when we impose the constraint that all speech segments uttered by the same speaker receive the same label via samespeaker links both testset and tion accuracy in percent')],\n",
       " [(0.0,\n",
       "   'wilson et al proposed supervised learning dividing the resources into prior polarity and context polarity which are similar to polar atoms and syntactic patterns in this paper respectively'),\n",
       "  (0.0,\n",
       "   'wilson et al prepared prior polarities from existing resources and learned the context polarities by using prior polarities and annotated corpora'),\n",
       "  (0.0,\n",
       "   'when the window size is oo implying anywhere within a discourse the ratio is larger than the baseline by only and thus these types of coherency are not reliable even though the number of clues is relatively large')],\n",
       " [(2.8220207770168126e-50,\n",
       "   'we present an approach for the joint extraction of entities and relations in the con text of opinion recognition and analysis'),\n",
       "  (3.0425338605789467e-62,\n",
       "   'while good performance in entity or relation extraction can contribute to better performance ofthe final system this is not always the case'),\n",
       "  (8.725539832973377e-65,\n",
       "   'the global inference procedure is implemented via integer linear programming ilp to produce an optimal and coherent extraction of entities and relations')],\n",
       " [(9.662276386065273e-55,\n",
       "   'in the nlp literature a common approach is to model the conditional distribution of label sequences given the label sequences'),\n",
       "  (6.5044366061089035e-59,\n",
       "   'we showed that in this framework it is possible to perform accurate broadcoverage tagging with state of the art sequence learning methods'),\n",
       "  (4.4689952044640065e-76,\n",
       "   'it is also possible that this kind of shallow semantic information can help building more sophisticated linguistic analysis as in full syntactic parsing and semantic role labeling')],\n",
       " [(9.214105037356802e-51,\n",
       "   'the context vector is formed by taking the aggregate of the word vectors of the words in the augmented context'),\n",
       "  (8.580189624489356e-52,\n",
       "   'each of the word pairs have been scored by humans on a scale of to where is the most related'),\n",
       "  (2.753494094923158e-56,\n",
       "   'the sense of the target word that is most related to its context is selected as the intended sense of the target word')],\n",
       " [(8.739179002229939e-61,\n",
       "   'using this model we obtain a classification accuracy of only which is much lower than the accuracy previously achieved at the document level'),\n",
       "  (2.737850816078829e-65,\n",
       "   'although their research may have some similar goals they do not take a computational approach to analyzing large collections of documents'),\n",
       "  (0.0,\n",
       "   'words in the document are then sampled from a multinomial distribution where ln is the length of the document')],\n",
       " [(9.921107823686319e-57,\n",
       "   'for most parsers their ranking for a specific language differs at most a few places from their overall ranking'),\n",
       "  (1.82454604853067e-58,\n",
       "   'that approach is based on a conversion from constituent structure to dependency structure by recursively defining a head for each constituent'),\n",
       "  (2.379068787255279e-62,\n",
       "   'the parsing order directly determines what information will be available from the history when the next decision needs to be made')],\n",
       " [(3.294205699807919e-55,\n",
       "   'however if we only look at performance for sentences of length less than the labeled accuracy is still only'),\n",
       "  (1.278325245314271e-57,\n",
       "   'the results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language'),\n",
       "  (1.5670802801331048e-65,\n",
       "   'thus a joint model of parsing and labeling could not easily include them without some form of reranking or approximate parameter estimation')],\n",
       " [(2.465973923521668e-68,\n",
       "   'we use svm classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion'),\n",
       "  (0.0,\n",
       "   'we projectivize training data by a minimal transformation lifting nonprojective arcs one step at a time and extending the arc label of lifted arcs using the encoding scheme called head by nivre and nilsson which means that a lifted arc is assigned the label rth where r is the original label and h is the label of the original head in the nonprojective dependency graph'),\n",
       "  (0.0,\n",
       "   'we have the best reported score for japanese swedish and turkish and the score for arabic danish dutch portuguese spanish and overall does not differ significantly from the best one')],\n",
       " [(1.1905588924339233e-53,\n",
       "   'that is they do not merely learn correspondences between phrases but also segmentations of the source and target sentences'),\n",
       "  (3.343038447943486e-61,\n",
       "   'the primary increase in richness from generative wordlevel models to generative phraselevel models is due to the additional latent segmentation variable'),\n",
       "  (4.032121350884616e-65,\n",
       "   'the generative model defined below is evaluated based on the bleu score it produces in an endtoend machine translation system from english to french')],\n",
       " [(6.727938088499049e-51,\n",
       "   'for arabicenglish using features based only on words of the target sentence the classification error rate can be reduced to'),\n",
       "  (2.835729923673796e-52,\n",
       "   'we use a stateoftheart baseline system which would have obtained a good rank in the last nist evaluation nist'),\n",
       "  (7.295609855726588e-56,\n",
       "   'we evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a wordaligned corpus')],\n",
       " [(6.646043387737838e-72,\n",
       "   'in this work we applied syntax based resources the target language parser to annotate and generalize phrase translation tables extracted via existing phrase extraction techniques'),\n",
       "  (0.0,\n",
       "   'while yamada and knight represent syntactical information in the decoding process through a series of transformation operations we operate directly at the phrase level'),\n",
       "  (0.0,\n",
       "   'while our submission time system syn using lm for rescoring only shows no improvement over the baseline we clearly see the impact of integrating the language model into the kbest list extraction process')],\n",
       " [(3.0614261838006264e-53,\n",
       "   'we also define a direct probability model and use a lineartime dynamic programming algorithm to search for the best derivation'),\n",
       "  (3.0614261838006264e-53,\n",
       "   'we also define a direct probability model and use a lineartime dynamic programming algorithm to search for the best derivation'),\n",
       "  (3.008881743206597e-63,\n",
       "   'since we are translating in the other direction we use the first english reference as the source input and the chinese as the single reference')],\n",
       " [(3.609377341605021e-55,\n",
       "   'the algorithms ability to separate the merged graph into its previous parts can be measured in an unsupervised way'),\n",
       "  (0.0,\n",
       "   'while the games goal is to arrive at some funny derivative of the original message by passing it through several noisy channels the cw algorithm aims at finding groups of nodes that broadcast the same message to their neighbors'),\n",
       "  (0.0,\n",
       "   'when generating swgraphs with the steyverstenenbaum model we fixed m to and varied n and the merge rate r which is the fraction of nodes of the smaller graph that is merged with nodes of the larger graph')],\n",
       " [(1.6106759159298933e-49,\n",
       "   'finally we explore for the first time the utility of a joint phrasal translation model as a word alignment method'),\n",
       "  (9.861239066871475e-54,\n",
       "   'this section describes how to use our phrasal itg first as a translation model and then as a phrasal aligner'),\n",
       "  (6.013009406796496e-58,\n",
       "   'in the experiments described in section all systems that do not use itg will take advantage of the complete training set')],\n",
       " [(4.654861185503475e-49,\n",
       "   'one of the strengths of the factored model is it allows for ngram distributions over factors on the target'),\n",
       "  (0.0,\n",
       "   'where not otherwise specified the postag and supertag sequence models are gram mod els and the language model is a gram model'),\n",
       "  (0.0,\n",
       "   'we use a model which combines more specificdependencies on source words and source ccg su pertags with a more general model which only has dependancies on the source word see equation we explore two different ways of balancing the sta tistical evidence from these multiple sources')],\n",
       " [(2.897121346548579e-54,\n",
       "   'to our knowledge this approach to dynamic adaptation for smt is novel and it is one of the main contributions of the paper'),\n",
       "  (4.2227871050834836e-73,\n",
       "   'in this setting tm adaptation is much less effective not significantly better than the baseline performance of combined lm and tm adaptation is also lower'),\n",
       "  (1.3688819756693888e-80,\n",
       "   'the most successful is to weight component models in proportion to maximumlikelihood em weights for the current text given an ngram language model mixture trained on corpus components')],\n",
       " [(1.6826555943516442e-52,\n",
       "   'the data used in this years shared task was similar to the data used in last years shared task'),\n",
       "  (3.188777606406319e-61,\n",
       "   'in order to draw judges attention to these regions we highlighted the selected source phrases and the corresponding phrases in the translations'),\n",
       "  (2.225827999295426e-62,\n",
       "   'we measured the correlation of the automatic evaluation metrics with the different types of human judgments on data conditions and report these in section')],\n",
       " [(3.3250342379971644e-66,\n",
       "   'if more than one reference translation is available the translation is scored against each reference independently and the best scoring pair is used'),\n",
       "  (2.2268797992333843e-86,\n",
       "   'bleu is fast and easy to run and it can be used as a target function in parameter optimization training procedures that are commonly used in stateoftheart statistical mt systems och'),\n",
       "  (0.0,\n",
       "   'when optimizing correlation with the sum of adequacy and fluency optimal values fall in between the values found for adequacy and fluency')],\n",
       " [(1.3215275354362302e-58,\n",
       "   'we found similar results when comparing the judgements of the classifier to rater agreement was high and kappa was low'),\n",
       "  (2.4011139900573533e-61,\n",
       "   'results of the comparison between the classifier and the test set showed that the overall proportion of agreement between the text and the classifier was'),\n",
       "  (1.3251303288633703e-66,\n",
       "   'our work is novel in that we are the first to report specific performance results for a preposition error detector trained and evaluated on general corpora')],\n",
       " [(4.325143207684901e-72,\n",
       "   'the induced clusters are compared to the sets of examples tagged with the given gold standard word senses classes and evaluated using the fscore measure for clusters'),\n",
       "  (0.0,\n",
       "   'wsd is performed by assigning to each in duced cluster a score equal to the sum of weights of hyperedges found in the local context of the target word'),\n",
       "  (0.0,\n",
       "   'with this goal onmind we gave all the participants an unlabeled cor pus and asked them to induce the senses and create a clustering solution on it')],\n",
       " [(0.0,\n",
       "   'when this mapping was not straightforward we just adopted the wordnet sense inventory for that wordwe released the entire sense groupings those in duced from the manual mapping for words in the test set plus those automatically derived on the other words and made them available to the participants'),\n",
       "  (0.0,\n",
       "   'we report about the use of semantic resources as well as semantically annotated corpora sc semcor dso defence science organ isation corpus se senseval corpora omwe open mind word expert xwn extended word net wn wordnet glosses andor relations wnd wordnet domains as well as information about the use of unannotated corpora uc training tr mfs based on the semcor sense frequencies and the coarse senses provided by the organizers cs'),\n",
       "  (0.0,\n",
       "   'we observe that on a technical domain suchas computer science most supervised systems per formed worse due to the nature of their training set')],\n",
       " [(0.0,\n",
       "   'whilstresearchers believe that it will ultimately prove useful for applications which need some degree of se mantic interpretation the jury is still out on this point'),\n",
       "  (0.0,\n",
       "   'we tookthe word with the largest similarity or smallest dis tance for sd and l for best and the top for oot'),\n",
       "  (0.0,\n",
       "   'we thank serge sharoff for the use of his internet corpus julie weeds for the software we used for producing the distributional similarity baselines and suzanne stevenson for suggesting the oot task')],\n",
       " [(0.0,\n",
       "   'while we expected a raise in the case of the us census names the other two cases just show that there is a high and unpredictable variability which would require much larger data sets to have reliable population samples'),\n",
       "  (0.0,\n",
       "   'we reused the web corpus mann which contains names randomly picked from the us census and was well suited for the task'),\n",
       "  (0.0,\n",
       "   'we preferred how ever to leave the corpus as is and concentrate our efforts in producing clean training and test datasets rather than investing time in improving trial data')],\n",
       " [(0.0,\n",
       "   'wvalis hybrid approach outperforms the other systems in task b and using relaxed scoring in task c as well'),\n",
       "  (0.0,\n",
       "   'with this more complete temporal annotation it is no longer possible to simply evaluate the entire graph by scoring pairwise comparisons'),\n",
       "  (0.0,\n",
       "   'when applied to the test data the task b system was run first in order to supplythe necessary features to the task a and task c sys temslccte automatically identifies temporal refer ring expressions events and temporal relations in text using a hybrid approach leveraging variousnlp tools and linguistic resources at lcc')],\n",
       " [(0.0,\n",
       "   'whether or not the more coarsegrained senses are effective in improving natural language processing applications remains to be seen'),\n",
       "  (0.0,\n",
       "   'we selecteda total of lemmas verbs and nouns con sidering the degree of polysemy and total instances that were annotated'),\n",
       "  (0.0,\n",
       "   'we proposed two levels of participation in thistask i closed the systems could use only the an notated data provided and nothing else')],\n",
       " [(1.6006257070130986e-53,\n",
       "   'figure gives an example of a nonprojective graph for a sentence that has also been extracted from the penn treebank'),\n",
       "  (9.411994246341567e-56,\n",
       "   'in the supervised setting this model can be trained with maximum likelihood estimation which amounts to simple counts over the data'),\n",
       "  (5.718897726545127e-57,\n",
       "   'we show below that this is a crucial feature in the development of the complexity results we have obtained in the previous sections')],\n",
       " [(4.949862416404553e-50,\n",
       "   'the data points that we have available consist of a set of human judgments each ranking the output of systems'),\n",
       "  (6.411683399734764e-52,\n",
       "   'we evaluated translation tasks with three different types of judgments for most of them for a total of different conditions'),\n",
       "  (7.12114146417567e-58,\n",
       "   'rather than weighting individual systems it incorporated weighted features that indicated which language the system was originally translating from')],\n",
       " [(1.2278738643832893e-55,\n",
       "   'in this paper we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better mt performance'),\n",
       "  (9.965239021682813e-75,\n",
       "   'better segmentation performance should lead to better mt performance observation we have shown in hypothesis that it is helpful to segment chinese texts into words first'),\n",
       "  (5.8068835636925e-75,\n",
       "   'first we found that neither characterbased nor a standard word segmentation standard are optimal for mt and show that an intermediate granularity is much more effective')],\n",
       " [(0.0,\n",
       "   'with parallel computing processing time wall time can often be cut down by one or two orders of magnitude'),\n",
       "  (0.0,\n",
       "   'with compatible interface mgiza is suitable for a dropin replacement for giza while pgiza can utilize huge computation resources which is suitable for building large scale systems that cannot be built using a single machine'),\n",
       "  (0.0,\n",
       "   'while working on the required modification to giza to run the alignment step in parallel we identified a bug which needed to be fixed')],\n",
       " [(0.0,\n",
       "   'while a limited amount of gold standard annotated data was prepared for the parser evaluation shared task this is the main source of goldstandard sd data which is currently available'),\n",
       "  (0.0,\n",
       "   'when the relation between a head and its dependent can be identified more precisely relations further down in the hierarchy are used but when it is unclear more generic dependencies are possible dp dp'),\n",
       "  (0.0,\n",
       "   'when seeking a goldstandard dependency scheme for parser evaluation the ultimate goal of such an evaluation is an important question')],\n",
       " [(4.1222164818517985e-48,\n",
       "   'experiments on the penn wsj treebank show that the model achieves stateoftheart performance for both constituent and dependency accuracy'),\n",
       "  (1.7305570567220414e-58,\n",
       "   'the efficiency of the parsing algorithm is important in applying the parsing model to test sentences and also when training the model using discriminative methods'),\n",
       "  (1.042333402283773e-58,\n",
       "   'in this case we could define which can be computed with insideoutside style algorithms applied to the data structures from eisner')],\n",
       " [(0.0,\n",
       "   'zhang et al achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus'),\n",
       "  (0.0,\n",
       "   'wsem is the weight assigned to the semantic task the macro labeled fi score which was used for the ranking of the participating systems is computed as the harmonic mean of lmp and lmr'),\n",
       "  (0.0,\n",
       "   'who within an ensemblebased architecture implemented a joint syntacticsemantic model using maltparser with labels enriched with semantic information lluis and marquez who used a modified version of the eisner algorithm to jointly predict syntactic and semantic dependencies and finally sun et al who integrated dependency label classification and argument identification using a maximumentropy markov model')],\n",
       " [(3.955133630400983e-55,\n",
       "   'in general system combinations performed as well as the best individual systems but not statistically significantly better than them'),\n",
       "  (1.3695349265422177e-64,\n",
       "   'we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for more than metrics'),\n",
       "  (0.0,\n",
       "   'wpf and wpbleu popovic and ney these metrics are based on words and part of speech sequences wpf is an ngram based fmeasure which takes into account both word ngrams and part of speech ngrams wpbleu is a combnination of the normal blue score and a part of speechbased bleu score')],\n",
       " [(2.05294321858464e-55,\n",
       "   'hierarchical phrasebased translation requires a translation grammar extracted from a parallel corpus where grammar rules include associated feature values'),\n",
       "  (0.0,\n",
       "   'where there is a clear point of departure for research a basic implementation of each interface is provided as an abstract class to minimize the work necessary for new extensions'),\n",
       "  (0.0,\n",
       "   'when designing our toolkit we applied general principles of software engineering to achieve three major goals extensibility endtoend coherence and scalability')],\n",
       " [(3.5036817354278975e-68,\n",
       "   'we proposed to generate synthetic parallel data by translating monolingual adaptation data with a background system and to train statistical models from the synthetic corpus'),\n",
       "  (1.2044861957092345e-100,\n",
       "   'with respect to the standard procedure the total training time was reduced by almost phrase extraction produced more phrase pairs and the final translation system showed a loss in translation performance bleu score below relative'),\n",
       "  (4.574693043345974e-124,\n",
       "   'section presents previous work on the problem of adaptation in smt section introduces the exemplar task and research questions we addressed section describes the smt system and the adaptation techniques that were investigated section presents and discusses experimental results and section provides conclusions')],\n",
       " [(1.8110520023941117e-53,\n",
       "   'system level scores are determined by taking the weighted average of the document level scores in the same manner'),\n",
       "  (8.031076011911321e-54,\n",
       "   'these correct translations differ not only in their word choice but also in the order in which the words occur'),\n",
       "  (8.867319520554972e-59,\n",
       "   'in order to evaluate the state of automatic mt evaluation nist tested metrics across a number of conditions across test sets')],\n",
       " [(4.4992561141911925e-52,\n",
       "   'for example the average length of the scope of not is in the abstracts subcorpus and in the papers subcorpus'),\n",
       "  (2.6836156759352403e-56,\n",
       "   'it achieves the best results to date for this task with an error reduction of compared to current state of the art results'),\n",
       "  (2.3797613526925815e-59,\n",
       "   'the latter experiment is therefore a test of the robustness of the system when applied to different text types within the same domain')],\n",
       " [(7.958899917979437e-54,\n",
       "   'therefore they apply a baseline ner system and use the resulting predictions as features in a second level of inference'),\n",
       "  (1.4369979107829611e-67,\n",
       "   'we have presented a simple model for ner that uses expressive features to achieve new state of the art performance on the named entity recognition task'),\n",
       "  (1.2445109264519346e-67,\n",
       "   'however as we show these contributions are to a large extent independent and as we show the approaches can be used together to yield better results')],\n",
       " [(2.3797613526925815e-59,\n",
       "   'the latter experiment is therefore a test of the robustness of the system when applied to different text types within the same domain'),\n",
       "  (6.928490714960502e-60,\n",
       "   'although the system was developed and tested on biomedical text the same approach can also be applied to text from other domains'),\n",
       "  (5.176249161661187e-61,\n",
       "   'the cues that are not present in the training data cannot be learned in the test data and the same applies to their scope')],\n",
       " [(4.7131777424528887e-66,\n",
       "   'the results on the five event types involving only a single primary theme argument are shown in one merged class simple event'),\n",
       "  (1.3807223972089153e-70,\n",
       "   'although both alternatives are reasonable the need to have consistent training and evaluation data requires a consistent choice to be made for the shared task'),\n",
       "  (0.0,\n",
       "   'without constraints on the type of theme arguments the following two annotations are both legitimate the two can be seen as specifying the same event at different levels of specificity')],\n",
       " [(4.9257668085447605e-55,\n",
       "   'if a word has more than one sense then it can be found in more than one thesaurus category'),\n",
       "  (1.744231376928802e-60,\n",
       "   'we compared a subset of our lexicon with existing gold standard data to show that the annotations obtained are indeed of high quality'),\n",
       "  (6.159568328573595e-69,\n",
       "   'for these questions we listed head words from both the senses categories as two of the alternatives probability of a random choice being correct is')],\n",
       " [(6.547661109568119e-54,\n",
       "   'if we increase the size of the data tenfold but also increase the noise can learning still be successful'),\n",
       "  (0.0,\n",
       "   'you can do this by measuring the httpcastingwordscom httpgroupscsailmiteduuidturkit interannotator agreement of the turkers against ex perts on small amounts of gold standard data or by stating what controls you used and what criteria youused to block bad turkers'),\n",
       "  (0.0,\n",
       "   'workshop par ticipants struggled with how to attract turkers howto price hits hit design instructions cheating de tection etc no doubt that as work progresses so will a communal knowledge and experience of how to use mturk')],\n",
       " [(4.312326750341131e-59,\n",
       "   'we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for metrics'),\n",
       "  (8.609379827003223e-65,\n",
       "   'system combinations are listed in the order of how often their translations were ranked higher than or equal to any other system'),\n",
       "  (0.0,\n",
       "   'your edited translations the machine translations the shortage of snow in mountain the shortage of snow in mountain worries the hoteliers worries the hoteliers correct reset edited no corrections needed unable to the deserted tracks are not the deserted tracks are not putting down problem only at the exploitants of skilift putting down problem only at the exploitants of skilift correct reset edited no corrections needed unable to the lack of snow deters the people the lack of snow deters the people to reserving their stays at the ski in the hotels and pension to reserving their stays at the ski in the hotels and pension correct reset edited no corrections needed unable to thereby is always possible to thereby is always possible to track free bedrooms for all the dates in winter including christmas and nouvel an track free bedrooms for all the dates in winter including christmas and nouvel an')],\n",
       " [(2.4009493316345442e-63,\n",
       "   'the vector space was built with the most frequent tokens in the corpus a cutoff point that included all the extracted an pairs'),\n",
       "  (3.7788936603008036e-66,\n",
       "   'a very desirable result would be if any predicted compositional an vector could be reliably used instead of the extracted bigram'),\n",
       "  (0.0,\n",
       "   'wordspace vector models or distributional models of semantics henceforth dsms are computational models that build contextual semantic representations for lexical items from corpus data')],\n",
       " [(1.6525196450093657e-51,\n",
       "   'this formulation is a major shift from existing approaches that rely on extracting parsing rules from the training data'),\n",
       "  (4.750482338010256e-54,\n",
       "   'analysis over the training data shows that in examples both approaches predict a logical form that gives the correct answer'),\n",
       "  (1.7991237123283002e-57,\n",
       "   'our experimental results show that our model with response driven learning can outperform existing models trained with annotated logical forms')],\n",
       " [(1.3248914071130976e-48,\n",
       "   'task systems differ in the number of class labels used as target and in the machine learning approaches applied'),\n",
       "  (1.5346038265039035e-86,\n",
       "   'both tasks were addressed in the conll shared task in order to provide uniform manually annotated benchmark datasets for both and to compare their difficulties and stateoftheart solutions for them'),\n",
       "  (0.0,\n",
       "   'zhao et al extended the biological cue word dictionary of their system using it as a feature for classification by the frequent cues of the wikipedia dataset while ji et al')],\n",
       " [(4.051577224158549e-46,\n",
       "   'for the feature based model we use some of the features proposed in past literature and propose new features'),\n",
       "  (1.3552231448992012e-47,\n",
       "   'we experiment with three types of models unigram model a feature based model and a tree kernel based model'),\n",
       "  (4.679865973592914e-49,\n",
       "   'our feature based model that uses only features achieves similar accuracy as the unigram model that uses over features')],\n",
       " [(0.0,\n",
       "   'while the task is related to synonymy relation extraction yu and agichtein it has a novel definition of renaming one name permanently replacing the other'),\n",
       "  (0.0,\n",
       "   'while the basic task setup and entity definitions follow those of the ge task epi extends on the extraction targets by defining new event types relevant to task topics including major protein modification types and their reverse reactions'),\n",
       "  (0.0,\n",
       "   'while finding connections between event triggers and protein references is a major part of event extraction it becomes much harder if one is replaced with a coreferencing expression')],\n",
       " [(1.888341636678878e-61,\n",
       "   'this observation suggests a different event annotation scheme or a different event extraction strategy would be required for methods sections'),\n",
       "  (3.7111078578714994e-133,\n",
       "   'since the training data from the full text collection is relatively small despite of the expected rich variety of expressions in full text it is expected that generalization of a model from the abstract collection to full papers would be a key technique to get a reasonable performance'),\n",
       "  (0.0,\n",
       "   'when only primary arguments are considered the first five event types in table are classified as simple event types requiring only unary arguments')],\n",
       " [(6.865967297567521e-68,\n",
       "   'groups submitted only closed track results groups only open track results and groups submitted both closed and open track results'),\n",
       "  (1.6728802997530159e-74,\n",
       "   'the muc and ace corpora are the two that have been used most for reporting comparative results but they differ in the types of entities and coreference annotated'),\n",
       "  (0.0,\n",
       "   'word sense we trained a word sense tagger using a svm classifier and contextual word and part of speech features on all the training portion of the ontonotes data')],\n",
       " [(9.262853415711145e-54,\n",
       "   'using predicted mentions our system had an overall score of in the closed track and in the open track'),\n",
       "  (1.8688083558587036e-55,\n",
       "   'our system was ranked first in both tracks with a score of in the closed track and in the open track'),\n",
       "  (6.749463300644731e-87,\n",
       "   'our results demonstrate that despite their simplicity deterministic models for coreference resolution obtain competitive results eg we obtained the highest scores in both the closed and open tracks and respectively')],\n",
       " [(1.3286866285417352e-53,\n",
       "   'in our analysis we aimed to address the following questions tables show the system ranking for each of the translation tasks'),\n",
       "  (3.5862931113936714e-59,\n",
       "   'we also included two commercial offtheshelf mt systems two online statistical mt systems and five online rulebased mt systems'),\n",
       "  (8.915098009518687e-62,\n",
       "   'we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for evaluation metrics')],\n",
       " [(0.0,\n",
       "   'while versions tuned to various types of human judgments do not perform as well as the widely used bleu metric papineni et al a balanced tuning version of meteor consistently outperforms bleu over multiple endtoend tunetest runs on this data set'),\n",
       "  (0.0,\n",
       "   'whereas previous versions of meteor simply strip punctuation characters prior to scoring version includes a new text normalizer intended specifically for translation evaluation'),\n",
       "  (0.0,\n",
       "   'we use the nist open machine translation evaluation urduenglish parallel data przybocki plus m words of monolingual data from the english gigaword corpus parker et al to build a standard moses system hoang et al as follows')],\n",
       " [(0.0,\n",
       "   'with some minor api changes namely returning the length of the ngram matched it could also be fasterthough this would be at the expense of an optimization we explain in section'),\n",
       "  (0.0,\n",
       "   'with a good hash function collisions of the full bit hash are exceedingly rare one in billion queries for our baseline model will falsely find a key not present'),\n",
       "  (0.0,\n",
       "   'while sorted arrays could be used to implement the same data structure as probing effectively making m we abandoned this implementation because it is slower and larger than a trie implementation')],\n",
       " [(8.657265601489306e-50,\n",
       "   'the final model was selected based on its performance on the development set and the number of support vectors'),\n",
       "  (1.0106907377068651e-57,\n",
       "   'the focus is on special word relations and special phrase patterns thus several feature templates on this topic are extracted'),\n",
       "  (1.3637351436746986e-58,\n",
       "   'we also included two commercial offtheshelf mt systems three online statistical mt systems and three online rulebased mt systems')],\n",
       " [(2.4190482769075313e-46,\n",
       "   'in the terms of our in this example french is used as the source language and english as the target'),\n",
       "  (2.3788022912613886e-59,\n",
       "   'the alignment algorithm consists of two steps estimate translation probabilities and use these probabilities to search for most probable alignment path'),\n",
       "  (0.0,\n",
       "   'wordalign thus provides an example how a model such as brown et al s model that was originally designed for research in statistical machine translation can be modified to achieve practical though less ambitious goals in the near term')],\n",
       " [(0.0,\n",
       "   'whether the human language engine is organized as a pipeline plus a few feedback loops or an every module talks to every other module architecture is unknown at this point hopefully new psycholinguistic experiments will shed more light on this issue'),\n",
       "  (0.0,\n",
       "   'we have insufficient engineering data at present to make any wellsubstantiated claims about whether the oneway pipeline has the optimal costbenefit tradeoff or not and in any case this will probably depend somewhat on the circumstances of each application reiter and mellish but the circumstantial evidence on this question is striking despite the fact that so many theoretical papers have argued against pipelines and very few if any have argued for pipelines every one of the applicationsoriented systems examined in this survey chose to use the oneway pipeline architecture'),\n",
       "  (0.0,\n",
       "   'unfortunately while all of the systems possessed a module which converted semantic representations into deep syntactic ones each system used a different name for this module')],\n",
       " [(7.840942243289242e-51,\n",
       "   'with unsupervised learning the learner does not have a gold standard training corpus with which accuracy can be measured'),\n",
       "  (4.200178483860795e-51,\n",
       "   'conclusions in this paper we have presented a new algorithm for unsupervised training of a rulebased part of speech tagger'),\n",
       "  (3.869274256233568e-57,\n",
       "   'the most accurate stochastic taggers use estimates of lexical and contextual probabilities extracted from large manually annotated corpora eg')],\n",
       " [(7.160624094250102e-54,\n",
       "   'two human judges annotated the attachment decision for test examples and the method performed at accuracy on these cases'),\n",
       "  (6.212404202053201e-85,\n",
       "   'for each such vp the head verb first head noun preposition and second head noun were extracted along with the attachment decision for noun attachment for verb'),\n",
       "  (1.4651721212727428e-108,\n",
       "   'in general a probabilistic algorithm will make an estimate of this probability the decision can then be made using the test if this is true the attachment is made to the noun if not then it is made to the verb')],\n",
       " [(3.693636616667679e-50,\n",
       "   'the first tests for the presence of particular context words within a certain distance of the ambiguous target word'),\n",
       "  (1.488727407771313e-53,\n",
       "   'it classifies an ambiguous target word by matching each feature in the list in turn against the target context'),\n",
       "  (8.970962578361023e-57,\n",
       "   'instead therefore we assume that the presence of one word in the context is independent of the presence of any other word')],\n",
       " [(1.248391786681641e-54,\n",
       "   'there is a tradition in sense disambiguation of taking particularly ambiguous words and evaluating a systems performance on those words'),\n",
       "  (0.0,\n",
       "   'yet a computational system has no choice but to consider other more awkward possibilities for example this cluster might be capturing a distributional relationship between advice as one sense of counsel and royalty as one sense of court'),\n",
       "  (0.0,\n",
       "   'yarowskys algorithm for sense disambiguation can be thought of as a way of determining how rogets thesaurus categories behave with respect to contextual features')],\n",
       " [(0.0,\n",
       "   'while this automatic derivation process introduced a small percentage of errors of its own it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing'),\n",
       "  (0.0,\n",
       "   'while brackets must be correctly paired in order to derive a chunk structure it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags the few hard cases that arise can be handled completely locally'),\n",
       "  (0.0,\n",
       "   'when this approach is applied to partofspeech tagging the possible sources of evidence for templates involve the identities of words within a neighborhood of some appropriate size and their current partofspeech tag assignments')],\n",
       " [(2.6339905165507766e-56,\n",
       "   'this makes it practical to train on small handbuilt corpora for language pairs where large bilingual corpora are unavailable'),\n",
       "  (5.3126479665898637e-64,\n",
       "   'these conclusions could not have been drawn without a uniform framework for filter comparison or without a technique for automatic evaluation'),\n",
       "  (6.641548331132859e-71,\n",
       "   'given a test set of aligned sentences a better translation lexicon will contain a higher fraction of the source word target word pairs in those sentences')],\n",
       " [(2.2723124716578974e-55,\n",
       "   'supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger'),\n",
       "  (2.4031959319040814e-62,\n",
       "   'our case representation is at this point simpler only the ambiguous tags not the words themselves or any other information are used'),\n",
       "  (0.0,\n",
       "   'without optimisation it has an asymptotic retrieval complexity of nf where n is the number of items in memory and f the number of features')],\n",
       " [(2.2237341217508802e-61,\n",
       "   'due to the use of the wall street journal the product sense is more than times as common as any of the others'),\n",
       "  (8.075082155397196e-81,\n",
       "   'without this optimization testing would have been several orders of magnitude slower than making a decision by testing only a small subset of highly predictive features'),\n",
       "  (0.0,\n",
       "   'with respect to training time the symbolic methods are significantly slower since they are searching for a simple declarative representation of the concept')],\n",
       " [(0.0,\n",
       "   'weischedel et al provide the results from a battery of tritag markov model experiments in which the probability pwt of observing a word sequence w wi w wn together with a tag sequence t is given by furthermore pwiiti for unknown words is computed by the following heuristic which uses a set of predetermined endings this approximation works as well as the maxent model giving unknown word accuracyweischedel et al on the wall st journal but cannot be generalized to handle more diverse information sources'),\n",
       "  (0.0,\n",
       "   'using the set of difficult words the model performs at accuracy on the development set an insignificant improvement from the baseline accuracy of'),\n",
       "  (0.0,\n",
       "   'unlike sdt the maxent training procedure does not recursively split the data and hence does not suffer from unreliable counts due to data fragmentation')],\n",
       " [(1.5260432091751305e-52,\n",
       "   'however while the insideoutside algorithm is a grammar reestimation algorithm the algorithm presented here is just a parsing algorithm'),\n",
       "  (2.8113816000230655e-55,\n",
       "   'for a given sentence and a given parse tree there are many different derivations that could lead to that parse tree'),\n",
       "  (6.106415353487148e-75,\n",
       "   'however unlike in the hmm case where the algorithm produces a simple state sequence in the pcfg case a parse tree is produced resulting in additional constraints')],\n",
       " [(2.1474156846666287e-58,\n",
       "   'the amount of available bilingual parallel corpora is still relatively small in comparison to the large amount of available monolingual text'),\n",
       "  (1.326135786343423e-66,\n",
       "   'our system then proposes two sets of outputs for each japanese term our system proposes the top candidates from the set of noun phrases'),\n",
       "  (0.0,\n",
       "   'wwz w is the weighted mutual information in our algorithm since it is most suitable for lexicon compilation of midfrequency technical words or terms as an initial step all prw are precomputed for the seed words in both languages')],\n",
       " [(0.0,\n",
       "   'what is most interesting here is the way in which strongly selecting word w is typically the head of a noun phrase which could lead the model astray for example toy soldiers behave differently from soldiers mccawley'),\n",
       "  (0.0,\n",
       "   'thus despite the absence of class annotation in the training text it is still possible to arrive at a usable estimate of classbased probabilities'),\n",
       "  (0.0,\n",
       "   'this means that the observed countverbobj drink coffee will be distributed by adding a to the joint frequency with drink for each of the classes containing coffee')],\n",
       " [(2.241423432075864e-59,\n",
       "   'the first pass takes an input sentence shown in figure and uses tag to assign each word a pos tag'),\n",
       "  (2.820693535520238e-62,\n",
       "   'this paper presents a statistical parser for natural language that finds one or more scored syntactic parse trees for a given input sentence'),\n",
       "  (0.0,\n",
       "   'typically the procedures postulate many different values for a which cause the parser to explore many different derivations when parsing an input sentence')],\n",
       " [(1.4880250099817763e-48,\n",
       "   'the basic idea is that we can use information from parsing with one grammar to speed parsing with another'),\n",
       "  (1.4917690214786448e-51,\n",
       "   'however this does not give information about the probability of the node in the context of the full parse tree'),\n",
       "  (1.2099194720148e-149,\n",
       "   'they computed a score for each sequence as the minimum of the scores of each node in the sequence and computed a score for each node in the sequence as the minimum of three scores one based on statistics about nodes to the left one based on nodes to the right and one based on unigram statistics')],\n",
       " [(3.435326552392058e-55,\n",
       "   'the first step in the experiment was the construction of new versions of the test data in addition to the original version'),\n",
       "  (2.863687450779846e-63,\n",
       "   'thirteen million words in both languages combined were used for training and another two and a half million were used for testing'),\n",
       "  (1.8527282532350496e-73,\n",
       "   'perhaps the method presented here combined with an appropriate translation model can make some progress on the word identification problem for languages like chinese and japanese')],\n",
       " [(3.41532457583529e-48,\n",
       "   'the input to the system is a small set of seed words for a category and a representative text corpus'),\n",
       "  (6.93675045622736e-49,\n",
       "   'in this paper we present a corpusbased method that can be used to build semantic lexicons for specific categories'),\n",
       "  (2.3262924470964305e-55,\n",
       "   'criteria on a scale of to rate each words strength of association with the given category using the following criteria')],\n",
       " [(3.220094175071322e-51,\n",
       "   'in this study we evaluated the performance of three unsupervised learning algorithms on the disambiguation of words in naturally occurring text'),\n",
       "  (1.4120451006502425e-53,\n",
       "   'this assumption is based on the success of the naive bayes model when applied to supervised wordsense disambiguation eg'),\n",
       "  (1.3767960531531295e-53,\n",
       "   'for the nouns there was no significant difference between feature sets a and b when using the em algorithm')],\n",
       " [(0.0,\n",
       "   'we measured the ability of judges to agree with one another using the notion of percent agreement that was defined by gale and used extensively in discourse segmentation studies passonneau and litman hearst percent agreement reflects the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion the percent agreements computed for each of the five texts and each level of importance are given in table the agreements among judges for our experiment seem to follow the same pattern as those described by other researchers in summarization johnson that is the judges are quite consistent with respect to what they perceive as being very important and unimportant but less consistent with respect to what they perceive as being less important in contrast with the agreement observed among judges the percentage agreements computed for importance assignments that were randomly generated for the same texts followed a normal distribution with p o these results suggest that the agreement among judges is significant agreement among judges with respect to the importance of each textual unit'),\n",
       "  (0.0,\n",
       "   'we described the first experiment that shows that the concepts of rhetoncal analysis and nucleanty can be used effectively for summarizing text the experiment suggests that discoursebased methods can account for detemmimg the most important units in a text with a recall and precision as high as we showed how the concepts of rhetorical analysis and nucleanty can be treated algorithmically and we compared recall and precision figures of a summarization program that implements these concepts with recall and precision figures that pertain to a baseline algorithm and to a commercial system the microsoft office summarizer the discoursebased summanzation program that we propose outperforms both the baseline and the commercial summarizer see table however since its results do not match yet the recall and precision figures that pertain to the manual discourse analyses it is likely that improvements of the rhetorical parser algorithm will result in better performance of subsequent implemetations'),\n",
       "  (0.0,\n",
       "   'we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text we show how these concepts can be implemented and we discuss results that we obtained with a discoursebased summanzation program motivation the evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some consamples of the output in very few cases output of a summarization program with a humanmade summary or evaluated with the help of human subjects usually the results are modest unfortunately evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions the position that we take in this paper is that in order to build highquality summarization programs one needs to evaluate not only a representative set of automatically generated outputs a highly difficult problem by itself but also the adequacy of the assumptions that these programs use that way one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each with few exceptions automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text see paice for an excellent overview determining the salient parts is considered to be achievable because one or more of the following assumptions hold i important sentences in a text contain words that are used frequently lahn edmundson n important sentences contain words that are used in the tide and section headings edmundson in important sentences are located at the beginning or end of paragraphs baxendale iv important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically through training techniques lin and hovy v important sentences use words as greatest and significant or indiphrases as the main aim of this paper and the purpose of this article while nonimportant senuse words as impossible edmundson rush salvador and zamora vi important sentences and concepts highest connected entities in elaborate semantic structures skorochodko lin barzilay and elhadad and vn imponant and nonimportant sentences are derivable from a discourse representation of the text sparck jones ono surmta and mike in determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections computers are accurate tools flowever in determining the concepts that are semantically related or the discourse structure of a text computers are no longer so accurate rather they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement although it is plausible that elaborate cohesionand coherencebased structures can be used effectively in summarization we believe that before building summarization programs we should determine the extent to which these assumptions hold in this paper we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text we show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program from discourse trees to summaries an empirical view')],\n",
       " [(7.947015445899881e-54,\n",
       "   'additionally the final version will contain examples for each concept which are to be automatically extracted from the corpus'),\n",
       "  (0.0,\n",
       "   'we use the princeton wordnet technology for the database format database compilation as well as the princeton wordnet interface applying extensions only where necessary'),\n",
       "  (0.0,\n",
       "   'we therefore propose a mixed approach treating irregular particle verbs by enumeration and regular particle verbs in a compositional manner')],\n",
       " [(1.970311171385181e-51,\n",
       "   'adding in rules and resolves a total of pronouns correctly with only mistakes a precision of and recall of'),\n",
       "  (1.82832427976107e-55,\n",
       "   'i the named entity task at muc used a similar classification task and the best system performance was precision recall'),\n",
       "  (2.958036301340872e-64,\n",
       "   'this paper presents a high precision pronoun resolution system that is capable of greater than precision with and better recall for some pronouns')],\n",
       " [(1.047902307449747e-52,\n",
       "   'they reach a accuracy on a brown corpus subset and a on a subset of the wall street journal corpus'),\n",
       "  (1.9542446645265986e-60,\n",
       "   'for instance design is used as a noun repeatedly in one of the documents while its summary uses design as a verb'),\n",
       "  (0.0,\n",
       "   'wordnet does not include crosspartofspeech semantic relations so this relation cannot be used with word senses while term indexing simply and successfully does not distinguish them')],\n",
       " [(8.527281269373404e-59,\n",
       "   'furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing'),\n",
       "  (6.905336838557262e-84,\n",
       "   'furthermore as seen in figure running our parser past the first parse by a small amount of the edges required for the first parse produces still more accurate parses'),\n",
       "  (0.0,\n",
       "   'zn in the bottomup variant of the earley algorithm where a gt is a production of the original grammar')],\n",
       " [(4.211436530571403e-54,\n",
       "   'in addition a token could be tagged as other to indicate that it is not part of a named entity'),\n",
       "  (0.0,\n",
       "   'while was still the fourth highest score out of the twelve participants in the evaluation we feel that it is necessary to view this number as a crossdomain portability result rather than as an indicator of how the system can do on unseen data within its training domain'),\n",
       "  (0.0,\n",
       "   'while all of menes features have binaryvalued output the binary features are features whose associated historyview can be considered to be either on or off for a given token')],\n",
       " [(1.5146745782094738e-71,\n",
       "   'a statistical approach is present in the discourse module only where it is used to determine the probability that a noun verb phrase is the center of a sentence'),\n",
       "  (0.0,\n",
       "   'when viewed in this way a can be regarded as an index into these vectors that specifies which value is relevant to the particular choice of antecedent'),\n",
       "  (0.0,\n",
       "   'we would like to know therefore whether the pattern of pronoun references that we observe for a given referent is the result of our supposed hypothesis about pronoun reference that is the pronoun reference strategy we have provisionally adopted in order to gather statistics or whether the result of some other unidentified process')],\n",
       " [(2.053868804774777e-55,\n",
       "   'the texts are of a specific type there are only three of them and we have not used all rhetorical relations'),\n",
       "  (9.375410693934478e-72,\n",
       "   'stochastic search methods are a form of heuristic search that use the following generic algorithm use these to generate one or more new random variations'),\n",
       "  (3.600296375172288e-86,\n",
       "   'in this task one is given a set of facts all of which should be included in a text and a set of relations between facts some of which can be included in the text')],\n",
       " [(9.815910931040881e-49,\n",
       "   'we have presented a new approach to noun phrase coreference resolution that treats the problem as a clustering task'),\n",
       "  (1.0659563261163328e-54,\n",
       "   'second our approach is unsupervised and requires no annotation of training data nor a large corpus for computing statistical occurrences'),\n",
       "  (3.6114548708718814e-55,\n",
       "   'this noun phrase representation is a first approximation to the feature vector that would be required for accurate coreference resolution')],\n",
       " [(7.280614583158769e-53,\n",
       "   'performance for the combined sources is in all cases greater than for the morphology or context source used alone'),\n",
       "  (3.824189227625514e-56,\n",
       "   'thus the total number of contextual matches for the seed words was quite variable from and difficult to control'),\n",
       "  (4.360290784231496e-58,\n",
       "   'it is also necessary to have a relatively large unannotated text for bootstrapping the contextual models and classifying new named entities')],\n",
       " [(3.917227625968347e-63,\n",
       "   'the approach builds from an initial seed set for a category and is quite similar to the decision list approach described in yarowsky'),\n",
       "  (4.045380086827053e-64,\n",
       "   'many statistical or machinelearning approaches for natural language problems require a relatively large amount of supervision in the form of labeled training examples'),\n",
       "  (9.41857947026363e-86,\n",
       "   'thus an explicit assumption about the redundancy of the features that either the spelling or context alone should be sufficient to build a classifier has been built into the algorithm')],\n",
       " [(2.2895199572175035e-50,\n",
       "   'the set is then compared with the set generated from the penn treebank parse to determine the precision and recall'),\n",
       "  (1.1009681739095499e-52,\n",
       "   'we then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers'),\n",
       "  (9.951579486669806e-56,\n",
       "   'it is closer to the smaller value of precision and recall when there is a large skew in their values')],\n",
       " [(4.81659254095641e-45,\n",
       "   'the other word is the head of the phrase which is annotated with this grammatical relation in the treebank'),\n",
       "  (1.5347159108405874e-57,\n",
       "   'the instance contains information about the verb and the focus a feature for the word form and a feature for the pos of both'),\n",
       "  (7.162172238125122e-80,\n",
       "   'that paper also shows that the chunking method proposed here performs about as well as other methods and that the influence of tagging errors on np chunking is less than')]]"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_objetivo_ml_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "id": "dd3adc06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:32:10.306016Z",
     "start_time": "2023-05-01T21:32:10.265182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{746: ['for each possible sense of the word identify a relatively small number of training examples representative of that sense',\n",
       "  'this would indicate that the cost of a large sensetagged training corpus may not be necessary to achieve accurate wordsense disambiguation',\n",
       "  'words not only tend to occur in collocations that reliably indicate their sense they tend to occur in multiple such collocations'],\n",
       " 747: ['while this experiment shows that statistical models can help make choices in generation it fails as a computational strategy',\n",
       "  'word lattices are commonly used to model uncertainty in speech recognition waibel and lee and are well adapted for use with ngram models',\n",
       "  'while true irregular forms eg child i children must be kept in a small exception table the problem of multiple regular patterns usually increases the size of this table dramatically'],\n",
       " 748: ['it remains to be shown that an accurate broadcoverage parser can improve the performance of a text processing application',\n",
       "  'the word feature value of the internal nodes is intended to contain the lexical head of the nodes constituent',\n",
       "  'a search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses'],\n",
       " 749: ['it is assumed that there is a correlation between the cooccurrences of words which are translations of each other',\n",
       "  'the word cooccurrences were computed on the basis of an english corpus of and a german corpus of million words',\n",
       "  'in a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences'],\n",
       " 750: ['to our knowledge very few of the existing work on wsd has been tested and compared on a common data set',\n",
       "  'when tested on a common data set our wsd program gives higher classification accuracy than previous work on wsd',\n",
       "  'we draw our sentences containing the occurrences of the words listed above from the combined corpus of the million word brown corpus and the million word wall street journal wsj corpus'],\n",
       " 752: ['yet for intentionally knock twice this is not the case these adverbs do not commute and the semantics are distinct',\n",
       "  'yes a different vides flexibility about which parse represents its kind of efficient parser can be built for this case class',\n",
       "  'wittenburg assumes only one reading semantically so just one of its anala ccg fragment lacking orderchanging or higher yses fg is discovered while parsing'],\n",
       " 754: ['thus these algorithms improve performance not only on the measures that they were designed for but also on related criteria',\n",
       "  'we propose that by creating algorithms that optimize the evaluation criterion rather than some related criterion improved performance can be achieved',\n",
       "  'we present two new algorithms the labelled recall algorithm which maximizes the expected labelled recall rate and the bracketed recall algorithm which maximizes the bracketed recall rate'],\n",
       " 755: ['the distance measure could be extended to capture more context such as other words or tags in the sentence',\n",
       "  'we have shown that a simple statistical model based on dependencies between words can parse wall street journal news text with high accuracy',\n",
       "  'with a beam search strategy parsing speed can be improved to over sentences a minute with negligible loss in accuracy'],\n",
       " 757: ['typically values of or higher for this measure provide evidence of good reliability while values of or greater indicate high reliability',\n",
       "  'with few exceptions they rely on intuitive analyses of topic structure operational definitions of discourselevel properties eg interpreting paragraph breaks as discourse segment boundaries or theoryneutral discourse segmentations where subjects are given instructions to simply mark changes in topic',\n",
       "  'with a view toward automatically segmenting a spoken discourse we would like to directly classify phrases of all three discourse categories'],\n",
       " 758: ['written anew it probably would have been about lines characterize the relative performance of two techniques it is necessary to consider multiple training set sizes and to try both bigram and trigram models',\n",
       "  'while the original paper katz uses a single parameter k we instead use a different k for each n gt kn',\n",
       "  'while smoothing is a central issue in language modeling the literature lacks a definitive comparison between the many existing techniques'],\n",
       " 759: ['figure a shows that accuracy increases with batch size only up to a point and then starts to decrease',\n",
       "  'many corpusbased methods for natural language processing nlp are based on supervised training acquiring information from a manually annotated corpus',\n",
       "  'when training a bigram model indeed any hmm this is not true as each word is dependent on that before it'],\n",
       " 760: ['eisner proposes dependency models and gives results that show that a generative model similar to model performs best of the three',\n",
       "  'results on wall street journal text show that the parser performs at constituent precisionrecall an average improvement of over collins',\n",
       "  'when parsing the pos tags allowed for each word are limited to those which have been seen in training data for that word'],\n",
       " 762: ['wsd is useful in many natural language tasks such as choosing the correct word in machine translation and coreference resolution',\n",
       "  'we also showed as a baseline the performance of the simple strategy of always choosing the first sense of a word in the wordnet',\n",
       "  'in our approach a local context of a word is defined in terms of the syntactic dependencies between the word and other words in the same sentence'],\n",
       " 763: ['we first discuss the key concepts on which our approach relies section and the corpus analysis section that provides the empirical data for our rhetorical parsing algorithm',\n",
       "  'with its distant orbit percent farther from the sun than earth and slim atmospheric blanket mars experiences frigid weather conditions surface temperatures typically average about degrees celsius degrees fahrenheit at the equator and can dip to degrees c near the poles only the midday sun at tropical latitudes is warm enough to thaw ice on occasion but any liquid water formed in this way would evaporate almost instantly because of the low atmospheric pressure although the atmosphere holds a small amount of water and waterice clouds sometimes develop most martian weather involves blowing dust or carbon dioxide each win terfor example a blizzard of frozen carbon dioxide rages over one pole and a few meters of this dryice snow accumulate as previously frozen carbon dioxide evaporates from the opposite polar cap yet even on the summer pole where the sun remains in the sky all day long temperatures never warm enough to melt frozen waterm since parenthetical information is related only to the elementary unit that it belongs to we do not assign it an elementary textual unit status',\n",
       "  'when we began this research no empirical data supported the extent to which this ambiguity characterizes natural language texts'],\n",
       " 764: ['with this approach the english sound k corresponds to one of t ka ki ku r ke or ko depending on its context',\n",
       "  'while it is difficult to judge overall accuracysome of the phases are onomatopoetic and others are simply too hard even for good human translatorsit is easier to identify system weaknesses and most of these lie in the pw model',\n",
       "  'when word separators are removed from the katakana phrases rendering the task exceedingly difficult for people the machines performance is unchanged'],\n",
       " 765: ['our method achieves high precision more than and while our focus to date has been on adjectives it can be directly applied to other word classes',\n",
       "  'while no direct indicators of positive or negative semantic orientation have been proposed we demonstrate that conjunctions between adjectives provide indirect information about orientation',\n",
       "  'we were unable to reach a unique label out of context for several adjectives which we removed from consideration for example cheap is positive if it is used as a synonym of inexpensive but negative if it implies inferior quality'],\n",
       " 766: ['finally to our knowledge we are the first to propose using user satisfaction to determine weights on factors related to performance',\n",
       "  'a single user satisfaction measure can be calculated from a single question or as the mean of a set of ratings',\n",
       "  'section describes the use of linear regression and user satisfaction to estimate the relative contribution of the success and cost measures in a single performance function'],\n",
       " 767: ['the transformations applied to the test set improved the score from f to a reduction in the error rate',\n",
       "  'this technique provides a simple algorithm for learning a sequence of rules that can be applied to various nlp tasks',\n",
       "  'a variety of methods have recently been developed to perform word segmentation and the results have been published widely'],\n",
       " 768: ['we define the recall of a wordtoword translation model as the fraction of the bitext vocabulary represented in the model',\n",
       "  'in addition to the quantitative differences between the wordtoword model and the ibm model there is an important qualitative difference illustrated in figure',\n",
       "  'for these applications we have designed a fast algorithm for estimating a partial translation model which accounts for translational equivalence only at the word level'],\n",
       " 769: ['one approach for detecting syntactic patterns is to obtain a full parse of a sentence and then extract the required patterns',\n",
       "  'syntactic patterns are useful also for many basic computational linguistic tasks such as statistical word similarity and various disambiguation problems',\n",
       "  'recognizing shallow linguistic patterns such as basic syntactic relationships between words is a common task in applied natural language and text processing'],\n",
       " 770: ['our scoring algorithm takes this into account and computes a final precision of and for the two responses respectively',\n",
       "  'the numbers computed with respect to each entity in the document are then combined to produce final precision and recall numbers for the entire output',\n",
       "  'in addition we also describe a scoring algorithm for evaluating the crossdocument coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the muc within document coreference task'],\n",
       " 771: ['these three components form a highly relational and tightly integrated whole elements in each may point to elements in the other two',\n",
       "  'with the exception of the example sentence extraction component all the software modules are highly interactive and have substantial user interface requirements',\n",
       "  'we are building a constituent type identifier which will semiautomatically assign grammatical function gf and phrase type pt attributes to these femarked constituents eliminating the need for annotators to mark these'],\n",
       " 772: ['the test set was not used in any way in training so the test set does contain unknown words',\n",
       "  'in this paper we showed that the error distributions for three popular state of the art part of speech taggers are highly complementary',\n",
       "  'in this paper we first show that the errors made from three different state of the art part of speech taggers are strongly complementary'],\n",
       " 773: ['using this simple algorithm with a naive heuristic for matching rules we achieve surprising accuracy in an evaluation on the',\n",
       "  'by evaluating on both corpora we can measure the effect of noun phrase complexity on the treebank approach to base np identification',\n",
       "  'for all experiments we derived the training pruning and testing sets from the sections of wall street journal distributed with the penn treebank ii'],\n",
       " 774: ['we then scan all the derivations in the development set and for each occurrence of the elementary event ym xm in derivationwtk we accumulate the value w tk in the cmym xm counter to be used in the next iteration',\n",
       "  'we achieved a reduction in testdata perplexity bringing an improvement over a deleted interpolation trigram model whose perplexity was on the same trainingtest data the reduction is statistically significant according to a sign test',\n",
       "  'w t neednt be a constituent but for the parses where it is there is no restriction on which of its words is the headword or what is the nonterminal label that accompanies the headword'],\n",
       " 777: ['when a data driven method is used a model is automatically learned from the implicit structure of an annotated training corpus',\n",
       "  'the second stage can be provided with the first level outputs and with additional information eg about the original input pattern',\n",
       "  'all combination taggers outperform their best component with the best combination showing a lower error rate than the best individual tagger'],\n",
       " 780: ['the merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs',\n",
       "  'with simwn and simroget we transform wordnet and roget into the same format as the automatically constructed thesauri in the previous section',\n",
       "  'while previous methods rely on indirect tasks or subjective judgments our method allows direct and objective comparison between automatically and manually constructed thesauri'],\n",
       " 783: ['several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task',\n",
       "  'the accuracy of our technique approaches the accuracy of the best supervised methods and does so with only a tiny fraction of the supervision',\n",
       "  'the unsupervised algorithm for prepositional phrase attachment presented here is the only algorithm in the published literature that can significantly outperform the baseline without using data derived from a treebank or parser'],\n",
       " 785: ['the input to the algorithm is a parsed corpus and a set of initial seed words for the desired category',\n",
       "  'we will also present some experimental results from two corpora and discuss criteria for judging the quality of the output',\n",
       "  'the relationship between the nouns in a compound noun is very different from that in the other constructions we are considering'],\n",
       " 786: ['the centering model describes the relation between the focus of attention the choices of referring expressions and the perceived coherence of discourse',\n",
       "  'with respect to any two discourse entities x uttx pas x and y utty posy uttx and utty specifying the current utterance ui or the preceding utterance u i set up the following ordering constraints on elements in the slist table',\n",
       "  'while tensed clauses are defined as utterances on their own untensed clauses are processed with the main clause so that the cflist of the main clause contains the elements of the untensed embedded clause'],\n",
       " 787: ['when grouped by average performance they fell into several coherent classes which corresponded to the extent to which the functions focused on the intersection of the supports regions of positive probability of the distributions',\n",
       "  'we would not be able to tell whether the cause was an inherent deficiency in the l norm or just a poor choice of weight function perhaps li q r would have yielded better estimates',\n",
       "  'we treat a value of as indicating extreme dissimilarity it is worth noting at this point that there are several wellknown measures from the nlp literature that we have omitted from our experiments'],\n",
       " 788: ['overall about of the top words for each seed are parts and about of the top for each seed',\n",
       "  'finally we order the possible parts by the likelihood that they are true parts according to some appropriate metric',\n",
       "  'given a very large corpus our method finds part words with accuracy for the top words as ranked by the system'],\n",
       " 789: ['the method is applicable to any natural language where text samples of sufficient size computational morphology and a robust parser capable of extracting subcategorization frames with their fillers are available',\n",
       "  'when compared to levin s toplevel verb classes we found an agreement of our classification with her class of verbs of changes of state except for the last three verbs in the list in fig',\n",
       "  'we will sketch an understanding of the lexical representations induced by latentclass labeling in terms of the linguistic theories mentioned above aiming at an interpretation which combines computational learnability linguistic motivation and denotationalsemantic adequacy'],\n",
       " 790: ['recall from the previous discussion that we do not assume any kind of semantic relationship among the hypernyms listed for a particular cluster',\n",
       "  'wordnet has been an important research tool but it is insufficient for domainspecific text such as that encountered in the mucs message understanding conferences',\n",
       "  'within those columns majority lists the opinion of the majority of judges and any indicates the hypernyms that were accepted by even one of the judges'],\n",
       " 791: ['while their kappa results are very good for other tags the opinionstatement tagging was not very successful the distinction was very hard to make by labelers and accounted for a large proportion of our interlabeler error jurafsky et al',\n",
       "  'when there is such evidence we propose using the latent class model to correct the disagreements this model posits an unobserved latent variable to explain the correlations among the judges observations',\n",
       "  'when disagreement is symmetric the differences between the actual counts and the counts expected if the judges decisions were not correlated are symmetric that is snii for i j where i is the difference from independence'],\n",
       " 792: ['in fact the difference of mutual information values appear to be more important to the phrasal similarity than the similarity of individual words',\n",
       "  'to find out the benefit of using the dependency relationships identified by a parser instead of simple cooccurrence relationships between words we also created a database of the cooccurrence relationship between partofspeech tagged words',\n",
       "  'when the frequency count is reasonably large eg greater than a bin zn we further assume that the estimations of pa pbia and pcia in are accurate'],\n",
       " 793: ['in all cases the score for a set of questions was the average of the scores for each question',\n",
       "  'our system represents the information content of a sentence both question and text sentences as the set of words in the sentence',\n",
       "  'with respect to ease of answer key preparation pampr and autsent are clearly superior since they use the publisherprovided answer key'],\n",
       " 794: ['given the distribution of definite np types in our test set this would result in recall of and precision of',\n",
       "  'while examining the si extractions we found many similar nps for example the salvadoran government the guatemalan government and the us government the similarities indicate that some head nouns when premodified represent existential entities',\n",
       "  'when we say that a definite np is existential we say this because it completely specifies a cognitive representation of the entity in the readers mind'],\n",
       " 796: ['our final results dependency accuracy represent good progress towards the accuracy of the parser on english wall street journal text',\n",
       "  'in test data the correct answer was not available but the pos tagger output could be used if desired',\n",
       "  'collins describes results of accuracy in recovering dependencies on section of the penn wall street journal treebank using model of collins'],\n",
       " 797: ['in a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences',\n",
       "  'this was done on the basis of a list of approximately german and another list of about english function words',\n",
       "  'our method is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different languages'],\n",
       " 798: ['precision is estimated as the proportion of pages judged good by strand that were also judged to be good ie',\n",
       "  'yes by both judges this figure is recall is estimated as the number of pairs that should have been judged good by strand ie that recieved a yes from both judges that strand indeed marked good this figure is',\n",
       "  'using the cases where the two human judgments agree as ground truth precision of the system is estimated at and recall at'],\n",
       " 799: ['the estimated models are able to identify the correct parse from the set of all possible parses approximately of the time',\n",
       "  'one of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses',\n",
       "  'with a small number of features the correctparses estimator typically scores better than the pseudolikelihood estimator on the correctparses evaluation metric but the pseudolikelihood estimator always scores better on the pseudolikelihood evaluation metric'],\n",
       " 800: ['the evaluation task for the content selection stage is to measure how well we identify common phrases throughout multiple sentences',\n",
       "  'we will be investigating what factors influence the combination process and how they can be computed from input articles',\n",
       "  'given the theme shown in figure how can we determine which phrases should be selected to form the summary content'],\n",
       " 801: ['task participants could choose to either do all tasks focus on the time expression task focus on the event task or focus on the four temporal relation tasks',\n",
       "  'work on the english corpus was supported under the nsfcri grant towards a comprehensive linguistic annotation of language and the nsfint project sustainable interoperability for language technology silt funded by the national science foundation',\n",
       "  'with the task decomposition allowed by bat it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful'],\n",
       " 802: ['for each target word participants were provided with a training set in order to learn the senses of that word',\n",
       "  'systems answers were evaluated in an unsupervised manner by using two clustering evaluation measures and a supervised manner in a wsd task',\n",
       "  'word senses are more beneficial than simple word forms for a variety of tasks including information retrieval machine translation and others pantel and lin'],\n",
       " 803: ['overall the results suggest that the bidirectional and no entailment categories are more problematic than forward and backward judgments',\n",
       "  'for the other language pairs the results are lower with only out of participants above the two baselines in all datasets',\n",
       "  'in one run run they are used to train a classifier that assigns separate entailment judgments for each direction'],\n",
       " 804: ['we would like to thank inderjeet mani wlodek zadrozny rie kubota ando joyce chai and nanda kambhatla for their valuable feedback',\n",
       "  'we would also like to thank carl sable minyen kan dave evans adam budzikowski and veronika horvath for their help with the evaluation',\n",
       "  'we would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization'],\n",
       " 805: ['we would like to be able to incorporate semantics for an arbitrarily large number of words and lsa quickly becomes impractical on large sets',\n",
       "  'we introduce a semanticbased algorithm for learning morphology which only proposes affixes when the stem and stemplusaffix are sufficiently similar semantically',\n",
       "  'we insert words into a trie figure and extract potential affixes by observing those places in the trie where branching occurs'],\n",
       " 806: ['whereas earlier methods all share the same basic intuition ie that similar words occur in similar contexts i formalise this in a slightly different way each word defines a probability distribution over all contexts namely the probability of the context given the word',\n",
       "  'we can then measure the similarity of words by the similarity of their context distributions using the kullbackleibler kl divergence as a distance function',\n",
       "  'we can model the context distribution as being the product of independent distributions for each relative position in this case the kl divergence is the sum of the divergences for each independent distribution'],\n",
       " 809: ['while the bbn model does not perform at the level of model of collins on wall street journal text it is also less languagedependent eschewing the distance metric which relied on specific features of the english treebank in favor of the bigrams on nonterminals model',\n",
       "  'while results for the two languages are far from equal we believe that further tuning of the head rules and analysis of development test set errors will yield significant performance gains on chinese to close the gap',\n",
       "  'while more investigation is required we suspect part of the difference may be due to the fact that currently the bbn model uses languagespecific rules to guess part of speech tags for unknown words'],\n",
       " 811: ['in this way the information that a word is the first word in a sentence is available to the tagger',\n",
       "  'a large number of words including many of the most common words can have more than one syntactic category',\n",
       "  'rare words are defined to be words that appear less than a certain number of times in the training data here the value was used'],\n",
       " 813: ['with the input stimulusfsn the output is stimuli rather than the incorrect stimuluses that would follow from the application of the more general rule in',\n",
       "  'when the flex rule matches the input addressiv for example the c function npvordform defined elsewhere in the generator is called to determine the word form corresponding to the input the function deletes the inflection type and pos label specifications and the delimiters removes the last character of the lemma and finally attaches the characters es the word form generated is thus addresses',\n",
       "  'we present such a component a fast and robust morphological generator for english based on finitestate techniques that generates a word form given a specification of the lemma partofspeech and the type of inflection required'],\n",
       " 814: ['while this result is satisfying further investigation reveals that deterioration in the quality of the labeled data accumulated by cotraining hinders further improvement',\n",
       "  'while previous research summarized in section has investigated the theoretical basis of cotraining this study is motivated by practical concerns',\n",
       "  'we seek to apply the cotraining paradigm to problems in natural language learning with the goal of reducing the amount of humanannotated data required for developing natural language processing components'],\n",
       " 815: ['we also compared to a method in which the lexical indicator variables were used as input to a neural network',\n",
       "  'our goals are more general than those of information extraction and so this work should be helpful for that task',\n",
       "  'our results also indicate that the second noun the head is more important in determining the relationships than the first one'],\n",
       " 816: ['yet we can filter by pruning ngrams whose beginning or ending word is among the top n most frequent words',\n",
       "  'yet there is still another maybe or the reciprocal of the words frequencies semantic compositionality is not always bad',\n",
       "  'with permission and sufficient time one can repeatedly query websites that host large collections of mrds and evaluate each ngram'],\n",
       " 817: ['xu and croft offered a trainable method call local context analysis lca which replaces each query term with frequently cooccurring words',\n",
       "  'we discovered that lsa is a more accurate measure of similarity than the cosine metric stemming does not always improve segmentation accuracy and ranking is crucial to cosine but not lsa',\n",
       "  'this procedure is useful in information retrieval hearst and plaunt hearst yaari reynar summarisation reynar text understanding anaphora resolution kozima language modelling morris and hirst beeferman et al and text navigation choi b'],\n",
       " 818: ['the parsing models of charniak and collins add more complex features to the parsing model that we use as our baseline',\n",
       "  'our implementation of collins model performs at precision and recall of labeled parse constituents on the standard wall street journal training and test sets',\n",
       "  'while this has allowed for quantitative comparison of parsing techniques it has left open the question of how other types of text might affect parser performance and how portable parsing models are across corpora'],\n",
       " 819: ['when assigning timestamps we analyze both implicit time references mainly through the tense system and explicit ones temporal adverbials such as on monday in etc',\n",
       "  'when assigning a time to an event we select the time to be either the most recently assigned date or if the value of the most recently assigned date is undefined to the date of the article',\n",
       "  'we ran the timestamper program on two types of data list of eventclauses extracted by the event identifier and list of eventclauses created manually'],\n",
       " 821: ['the probability module implements classes that encode frequency distributions and probability distributions including a variety of statistical smoothing techniques',\n",
       "  'finally it can be used to show how these individual rules combine to find a complete parse for a given sentence',\n",
       "  'we used nltk as a basis for the assignments and student projects in cis an introductory computational linguistics class taught at the university of pennsylvania'],\n",
       " 822: ['in the experiments we show this class splitting technique not only enables the feasible training but also improves the accuracy',\n",
       "  'varying the size of training data we compared the change in the training time and the accuracy with and without the class splitting',\n",
       "  'in addition it is difficult to compare the techniques used in each study because they used a closed and different corpus'],\n",
       " 824: ['in the second method the cost function is defined as the maximum likelihood of the data given the model',\n",
       "  'the first word tokens were used as training data and the following word tokens were used as test data',\n",
       "  'figure shows the development of the average cost per word as a function of the increasing amount of source text'],\n",
       " 825: ['if other training data is not available a number of these sentences are presented to the users for tagging in stage',\n",
       "  'these initial set of tagged examples will then be used to train the two classifiers described in section and annotate an additional set of examples',\n",
       "  'next this tagged collection is used as training data and active learning is used to identify in the remaining corpus the examples that are hard to tag'],\n",
       " 826: ['the statistical models are trained on parallel corpora large amounts of text in one language along with their translation in another',\n",
       "  'this local context has to be translated into the other language and we can search the word with the most similar context',\n",
       "  'the baseline for this task choosing words at random results on average in only correct mapping in the entire lexicon'],\n",
       " 827: ['we evaluate existing and new similarity metrics for thesaurus extraction and experiment with the tradeoff between extraction performance and efficiency',\n",
       "  'in this paper we evaluate some existing similarity metrics and propose and motivate a new metric which outperforms the existing metrics',\n",
       "  'in these experiments we have proposed new measure and weight functions that as our evaluation has shown significantly outperform existing similarity functions'],\n",
       " 830: ['initial unigram results the classification accuracies resulting from using only unigrams as features are shown in line of figure',\n",
       "  'hence if context is in fact important as our intuitions suggest bigrams are not effective at capturing it in our setting',\n",
       "  'yet the results shown in line of figure are relatively poor the adjectives provide less useful information than unigram presence'],\n",
       " 833: ['the nouns extracted by these patterns become candidates for the lexicon and are placed in a candidate word pool',\n",
       "  'while metabootstrapping trusts individual extraction patterns to make unilateral decisions basilisk gathers collective evidence from a large set of extraction patterns',\n",
       "  'we will use the abbreviation cat to indicate that only one semantic category was bootstrapped and mcat to indicate that multiple semantic categories were simultaneously bootstrapped'],\n",
       " 834: ['the translation model captures the translation of source language words into the target language and the reordering of those words',\n",
       "  'to accomplish this we defined a simple heuristic to detect phrasal translations so we can filter them if desired',\n",
       "  'an alignment is a mapping between the words in a string in one language and the translations of those words in a string in another language'],\n",
       " 835: ['the evaluation shows that the grammar is at a stage where domain adaptation is possible in a reasonable amount of time',\n",
       "  'the grammar is being developed in a multilingual context where much value is placed on parallel and consistent semantic representations',\n",
       "  'the grammar is created for use in real world applications such that robustness and performance issues play an important role'],\n",
       " 836: ['work is also progressing on establishing a standard relational database using postgresql for storing information for the lexical entries themselves improving both scalability and clarity compared to the current simple text file representation',\n",
       "  'while the development of the matrix will be built largely on the lkb platform support will also be needed for using the emerging grammars on other processing platforms and for linking to other packages for preprocessing the linguistic input',\n",
       "  'while the details of modifier placement which parts of speech can modify which kinds of phrases etc differ across languages we believe that all languages display a distinction between scopal and intersective modification'],\n",
       " 837: ['within lfg fstructures are meant to encode a language universal level of analysis allowing for crosslinguistic parallelism at this level of abstraction',\n",
       "  'when this is noticed via the featuretable comparison it is determined why one grammar needs the feature and the other does not and thus it may be possible to eliminate the feature in one grammar or to add it to another',\n",
       "  'we report on the parallel grammar pargram project which uses the xle parser and grammar development platform for six languages english'],\n",
       " 838: ['this section describes the general formulation of the probabilistic model for parsing which has been applied to japanese statistical dependency analysis',\n",
       "  'we used sentences from the articles on january st to january th as training examples and sentences from the articles on january th as the test data',\n",
       "  'we propose a new method that is simple and efficient since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side'],\n",
       " 839: ['the gradient of a function is a vector which points in the direction in which the functions value increases most rapidly',\n",
       "  'while the loglikelihood function for me models in is twice differentiable for large scale problems the evaluation of the hessian matrix is computationally impractical and newtons method is not competitive with iterative scaling or first order methods',\n",
       "  'while parameter estimation for me models is conceptually straightforward in practice me models for typical natural language tasks are very large and may well contain many thousands of free parameters'],\n",
       " 843: ['we used a separate annotated tuning corpus of documents with a total of sentences to establish some experimental parameters',\n",
       "  'we explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms',\n",
       "  'the annotated corpus used to train and test our subjectivity classifiers the experiment corpus consists of documents with a total of sentences'],\n",
       " 844: ['with this type of agglomerative clustering the most similar pages are clustered first and outliers are assigned as stragglers at the top levels of the cluster tree',\n",
       "  'while word senses and translation ambiguities may typically have alternative meanings that must be resolved through context a personal name such as jim clark may potentially refer to hundreds or thousands of distinct individuals',\n",
       "  'while these names could be used in a undifferentiated vectorbased bagofwords model further accuracy can be gained by extracting specific types of association such as familial relationships eg son wife employment relationships eg manager of and nationality as distinct from simple term cooccurrence in a window'],\n",
       " 845: ['we would like to thank the anonymous reviewers for their helpful comments and also iain rae for computer support',\n",
       "  'we trained one of the taggers on much more labelled seed data than the other to see how this affects the cotraining process',\n",
       "  'we leave these experiments to future work but note that there is a large computational cost associated with such experiments'],\n",
       " 847: ['the first set of features apply to rare words ie those which appear less than times in the training data',\n",
       "  'collins also describes a mapping from words to word types which groups words with similar orthographic forms into classes',\n",
       "  'this paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy'],\n",
       " 848: ['within each of the regions a statistical bigram language model is used to compute the likelihood of words occurring within that region named entity type',\n",
       "  'when no gazetteer or other additional training resources are used the combined system attains a performance of f on the english development data integrating name location and person gazetteers and named entity systems trained on additional more general data reduces the fmeasure error by a factor of to on the english data',\n",
       "  'we note that the numbers are roughly twice as large for the development data in german as they are for english'],\n",
       " 849: ['when using characterlevel models for wordevaluated tasks one would not want multiple characters inside a single word to receive different labels',\n",
       "  'we present two models in which the basic units are characters and character grams instead of words and word phrases',\n",
       "  'we found knowing that the previous word was an other wasnt particularly useful without also knowing its partofspeech eg a preceding preposition might indicate a location'],\n",
       " 851: ['we have compared two systems that use this basic technique one taking a statistical approach and the other a linguistic approach',\n",
       "  'the results of the linguistically motivated approach show that we can build a working system with minimal linguistic knowledge and circumvent the need for large amounts of training data',\n",
       "  'we used two variants of hmm hedge one which selects headline words from the first words of the story and one which selects words from the first sentence of the story'],\n",
       " 852: ['in this case we use the highest performing model from section in order to label arguments with semantic roles',\n",
       "  'for the latter we use the best performing model from section in order to find semantic roles given syntactic features from the parse',\n",
       "  'the goal of parsing syntactic analysis is ultimately to provide the first step towards giving a semantic interpretation of a string of words'],\n",
       " 853: ['using the gold standard parses provides an upper bound on the performance of the system based on automatic parses',\n",
       "  'in this paper we examine how the syntactic representations used by different statistical parsers affect the performance of such a system',\n",
       "  'head words of nodes in the parse tree are determined using the same deterministic set of head word rules used by collins'],\n",
       " 854: ['from those nouns also occurring in wordnet we selected the most frequent nouns and a set of low frequency nouns',\n",
       "  'we will first explain the wordnetbased distance measure lin and then explain how we determine the similarity between neighbour sets generated using different measures',\n",
       "  'in the syntactic domain language models which can be used to evaluate alternative interpretations of text and speech require probabilistic information about words and their cooccurrences which is often not available due to the sparse data problem'],\n",
       " 855: ['section describes the data that we use presents our experimental results and shows examples of patterns that are learned',\n",
       "  'in our second experiment we used the learned extraction patterns to classify previously unlabeled sentences from the unannotated text collection',\n",
       "  'these patterns match sentences such as the fact is and is a fact which apparently are often used in subjective contexts'],\n",
       " 856: ['finally we used as one of the features the average semantic orientation score of the words in the sentence',\n",
       "  'these labels are used only to provide the correct classification labels during training and evaluation and are not included in the feature space',\n",
       "  'we first discuss how such words are automatically found by our system and then describe the method by which we aggregate this information across the sentence'],\n",
       " 857: ['the system used allows for different ensemble techniques to be applied meaning that a number of classifiers are generated and then combined to predict the class',\n",
       "  'the machine learning approach used for the experiments is that of rule induction ie the model that is constructed from the given examples consists of a set of rules',\n",
       "  'when pos patterns are used to extract potential terms the problem lies in how to restrict the number of terms and only keep the ones that are relevant'],\n",
       " 859: ['in the closed test participants could only use training material from the training data for the particular corpus being testing on',\n",
       "  'the problem with this literature has always been that it is very hard to compare systems due to the lack of any common standard test set',\n",
       "  'while we are familiar with the approaches taken in several of the tested systems we leave it up to the individual participants to describe their approaches and hopefully elucidate which aspects of their approaches are most responsible for their successes and failures the participants papers all appear in this volume'],\n",
       " 861: ['as for each closed track we first extracted all the common words and tokens that appear in the training corpus',\n",
       "  'we have taken six tracks academia sinica closed asc u penn chinese tree bank open and closedctboc hong kong cityu closed hkc peking university open and closedpkoc',\n",
       "  'we apply to word segmentation classbased hmm which is a generalized approach covering both common words and unknown words wi iff wi is listed in the segmentation lexicon per iff wi is unlisted personal name loc iff wi is unlisted location name org iff wi is unlisted organization name time iff wi is unlisted time expression num iff wi is unlisted numeric expression str iffwi is unlisted symbol string beg iff beginning of a sentence end iff ending of a sentence other otherwise'],\n",
       " 862: ['in order to get a reliable sense for how good these scores are we compare them with the level of agreement across human judges',\n",
       "  'in terms of relative performance the semantic similarity based approach of methods and outperform the distribution based approach of methods and in terms of fscore on of the sets of results reported',\n",
       "  'these agreement scores give us an upper bound for classification accuracy on each task from which it is possible to benchmark the classification accuracy of the classifiers on that same task'],\n",
       " 863: ['additionally in lexical acquisition and for word sense disambiguation it is important that related senses of words are identified',\n",
       "  'we also give results for comparison obtained on the same data for another wide coverage parser minipar lin b',\n",
       "  'whilst statistics are useful indicators of noncompositionality there are compositional multiwords which have low values for these statistics yet are highly noncompositional'],\n",
       " 864: ['for both tests values closer to indicate random distribution of the data whereas values closer to indicate a strong correlation',\n",
       "  'with simple decomposable mwes we can expect the constituents and particularly the head to be hypernyms ancestor nodes or synonyms of the mwe',\n",
       "  'with nondecomposable mwes eg kick the bucket shoot the breeze hot dog no decompositional analysis is possible and the mwe is semantically impenetrable'],\n",
       " 865: ['moreover we have shown that in practical parsing the algorithm performs incremental processing for the majority of input structures',\n",
       "  'we will try to make this more precise in a minute but first we want to discuss the relation between incrementality and determinism',\n",
       "  'we will represent parser configurations by triples i a where is the stack represented as a list i is the list of remaining input tokens and a is the current arc relation for the dependency graph'],\n",
       " 867: ['with an average of five senses per word the average value for the agreement by chance is measured at resulting in a micro statistic of',\n",
       "  'we measure two figures microaverage where number of senses agreement by chance and are determined as an average for all words in the set and macroaverage where intertagger agreement agreement by chance and are individually determined for each of the words in the set and then combined in an overall average',\n",
       "  'we describe in this paper the task definition resources participating systems and comparative results for the english lexical sample task which was organized as part of the senseval evaluation exercise'],\n",
       " 869: ['therefore how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years',\n",
       "  'with the duc data we computed pearsons product moment correlation coefficients spearmans rank order correlation coefficients and kendalls correlation coefficients between systems average rouge scores and their human assigned average coverage scores using single reference and multiple references',\n",
       "  'when multiple references are used we compute pairwise summarylevel rougen between a candidate summary s and every reference ri in the reference set'],\n",
       " 871: ['we thank chuck wooters don baron chris oei and andreas stolcke for software assistance ashley krupski for contributions to the annotation scheme andrei popescubelis for analysis and comments on a release of the meetings and barbara peskin and jane edwards for general advice and feedback',\n",
       "  'we suggest various ways to group the large set of labels into a smaller set of classes depending on the research focus',\n",
       "  'we provide a brief summary of the annotation system and labeling procedure interannotator reliability statistics overall distributional statistics a description of auxiliary files distributed with the corpus and information on how to obtain the data'],\n",
       " 872: ['we seek an inference algorithm that can produce a coherent labeling of entities and relations in a given sentence',\n",
       "  'similarly we assume a learning mechanism that can recognize the semantic relation between two given phrases in a sentence',\n",
       "  'following this work we model inference as an optimization problem and show how to cast it as a linear program'],\n",
       " 873: ['these feature vectors are in fact the first order context vectors of the feature words and not target word',\n",
       "  'first order context vectors represent the context of each instance of a target word as a vector of features that occur in that context',\n",
       "  'most words in natural language have multiple possible meanings that can only be determined by considering the context in which they occur'],\n",
       " 874: ['section describes the data used in the experiments the evaluation metrics and the models and algorithms used in the learning process',\n",
       "  'for this purpose we define a number of features that can be used to define different models of parser state',\n",
       "  'results from the experiments are given in section while conclusions and suggestions for further research are presented in section'],\n",
       " 876: ['while previous programs with similar goals gildea and jurafsky were statisticsbased this tool will be based completely on handcoded rules and lexical resources',\n",
       "  'whether or not one adapts an a la carte approach nombank and propbank projects provide users with data to recognize regularizations of lexically and syntactically related sentence structures',\n",
       "  'when complete nombank will provide argument structure for instances of about common nouns in the penn treebank ii corpus'],\n",
       " 877: ['we wanted to explore how well the annotators agreed on relatively abstract classifications such as requires extrapolation from actual findings and thus we refrained from writing instructions such as if the sentence contains a form of suggest then mark it as speculative into the guidelines',\n",
       "  'we report here the isolation ofhuman zinc finger hzf a putative zincfinger transcription factor by motifdirected differential display of mrna extracted from histaminestimulated human vein endothelial cells',\n",
       "  'we may use it to test a kr systems ability to predict b as the connecting aspect between a and c and to do this using data prior to the publication'],\n",
       " 878: ['we are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process',\n",
       "  'in this project the syntactic and semantic annotation is being done on a corpus which is also being annotated for entities as described in section',\n",
       "  'work over the last few years in literature data mining for biology has progressed from linguistically unsophisticated models to the adaptation of natural language processing nlp techniques that use full parsers park et al yakushiji et al and coreference to extract relations that span multiple sentences pustejovsky et al hahn et al for an overview see hirschman et al'],\n",
       " 879: ['in this paper we describe a dynamic programming approach to discriminative parsing that is an alternative to maximum entropy estimation',\n",
       "  'for example in the work of collins of the correct parses were not in the candidate pool of best parses',\n",
       "  'we provide an efficient algorithm for learning such models and show experimental evidence of the models improved performance over a natural baseline model and a lexicalized probabilistic contextfree grammar'],\n",
       " 880: ['there are several ways to improve the accuracy of the current algorithm and to detect relations between low frequency verb pairs',\n",
       "  'hence verb semantics could help in many natural language processing nlp tasks that deal with events or relations between entities',\n",
       "  'y or at least x yed or at least xed not only xed but yed not just xed but yed the probabilities in the denominator are difficult to calculate directly from search engine results'],\n",
       " 881: ['our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the web for related syntactic entailment templates',\n",
       "  'broad coverage lexicons are widely available or may be constructed using known term acquisition techniques making it a feasible and scalable input requirement',\n",
       "  'yet this method also suffers from certain limitations a it identifies only templates with prespecified structures b accuracy seems more limited due to the weaker notion of similarity and c coverage is limited to the scope of an available corpus'],\n",
       " 882: ['the bilingual parser without the english head span filter gives a small recall improvement on average at similar precision',\n",
       "  'yamada and knight introduced treetostring alignment on japanese data and gildea performed treetotree alignment on the korean treebank allowing for nonisomorphic structures he applied this to wordtoword alignment',\n",
       "  'xia et al compare the rule templates of lexicalized tree adjoining grammars extracted from treebanks in english chinese and korean'],\n",
       " 884: ['one is that for a given verb the majority of the constituents in a syntactic tree are not its semantic arguments',\n",
       "  'most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels',\n",
       "  'this paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input generally a syntactic parse tree has yet to be fully exploited'],\n",
       " 885: ['also the unsupervised nature of the approach highlights an intermediate step of determining the set of possible roles for each argument',\n",
       "  'the first step in our algorithm is to use the verb lexicon to determine the argument slots and the roles available for them',\n",
       "  'while framenet uses semantic roles specific to a particular situation such as speaker message admire adore appreciate cherish enjoy addressee and propbank uses roles specific to a verb such as arg arg arg verbnet uses an intermediate level of thematic roles such as agent theme recipient'],\n",
       " 886: ['we approach this problem as one of statistical machine translation smt within the noisy channel model of brown et al',\n",
       "  'to generate paraphrases of a given input a standard smt decoding approach was used this is described in more detail below',\n",
       "  'a second important contribution of this work is a method for building and tracking the quality of large alignable monolingual corpora from structured news data on the web'],\n",
       " 887: ['traditionally in japanese morphological analysis we assume that a lexicon which lists a pair of a word and its corresponding partofspeech is available',\n",
       "  'wordlevel templates are employed when the words are lexicalized ie those that belong to particle auxiliary verb or suffix',\n",
       "  'while the relative rates of lerror and serror are almost the same in hmms and crfs the number of lerrors with memms amounts to which is of total errors and is even larger than that of naive hmms'],\n",
       " 888: ['to our knowledge our work is the first to systematically investigate issues of processing architecture and feature representation for chinese pos tagging',\n",
       "  'as a first step in our investigation we built a chinese word segmenter capable of performing word segmentation without using pos tag information',\n",
       "  'zhou and su investigated an approach to build a chinese analyzer that integrated word segmentation pos tagging and parsing based on a hidden markov model'],\n",
       " 889: ['a simple way to accomplish this is to use map adaptation using a prior distribution on the model parameters',\n",
       "  'as expected adding more background training data improves performance but a very small amount of domain specific data also helps significantly if one can make use of it in an effective way',\n",
       "  'when evaluating on the mismatched outofdomain test data the gram baseline is outperformed by the improvement brought by the adaptation technique using a very small amount of matched bn data kwds is about relative'],\n",
       " 890: ['this is because each word occurring in the text is highly relevant to the predefined topics to be identified',\n",
       "  'as illustrated in section our method can automatically select relevant and compact features from a number of feature candidates',\n",
       "  'this information allows us to analyze how the system classifies the input sentence in a category and what kind of features are used in the classification'],\n",
       " 891: ['text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user',\n",
       "  'when computing degree centrality we have treated each edge as a vote to determine the overall prestige value of each node',\n",
       "  'we use pagerank to weight each vote so that a vote that comes from a more prestigious sentence has a greater value in the centrality of a sentence'],\n",
       " 892: ['since the focus of this paper is the comparison of the performance of different systems we need a set of translation systems',\n",
       "  'some researchers have recently used relative human bleu scores by comparing machine bleu scores with high quality human translation scores',\n",
       "  'in a third preliminary experiment we compared for each of the broad samples the bleu score for the spanish system against the bleu score for the danish system'],\n",
       " 893: ['instead we are defining a different relation which determines a connection between two sentences if there is a similarity relation between them where similarity is measured as a function of their content overlap',\n",
       "  'while the size of the abstracts ranges from to words with an average size of words we have deliberately selected a very small abstract for the purpose of illustration',\n",
       "  'while the final vertex scores and therefore rankings differ significantly as compared to their unweighted alternatives the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs'],\n",
       " 894: ['in the case of the latter results indicate that our approach may allow for further improvements to be gained given knowledge of the topic of the text',\n",
       "  'moreover is not restricted to words of a particular part of speech nor even restricted to single words but can be used with multiple word phrases',\n",
       "  'with these entities tagged a number of classes of features may be extracted representing various relationships between topic entities and value phrases similar to those described in section'],\n",
       " 897: ['our results show that adding syntactic information to the evaluation metric improves both sentencelevel and corpuslevel correlation with human judgments',\n",
       "  'this type of feature cannot capture the grammaticality of the sentence in part because they do not take into account sentencelevel information',\n",
       "  'our experiments measure how well these metrics correlate with human judgments both for individual sentences and over a large test set translated by mt systems of varying quality'],\n",
       " 899: ['we also measure precision recall and fmeasure calculated with respect to the true values in each of the test data sets',\n",
       "  'for nouns and verbs we use a measure of semantic similarity based on wordnet while for the other word classes we apply lexical matching',\n",
       "  'in the unsupervised setting the best performance is achieved using a method that combines several similarity metrics into one for an overall accuracy of'],\n",
       " 900: ['without this line algorithm could be considered as a generalization of the jimenez andmarzal algorithm to the case of acyclic monotonic hy pergraphs',\n",
       "  'with the derivations thus ranked we can introduce anonrecursive representation for derivations that is analogous to the use of backpointers in parser implementa tion',\n",
       "  'while charniak and johnson propose using an algorithm similar to our algorithm but with multiple passes to improve efficiency'],\n",
       " 901: ['this set of features and corresponding actions is then used to train a classifier resulting in a complete parser',\n",
       "  'therefore the number of shift and binary reduce actions is linear with the number of words in the input string',\n",
       "  'these instances can be obtained by running the algorithm on a corpus of sentences for which the correct parse trees are known'],\n",
       " 902: ['we used the charniak parser to get a phrase type feature of a frame element and the parse tree path feature',\n",
       "  'as for opinion topic identification little research has been conducted and only in a very limited domain product reviews',\n",
       "  'in product reviews for example opinion topics are often the product itself or its specific features such as design and quality eg'],\n",
       " 904: ['the results also show that the simple system combination procedure that we have employed is effective in our setting',\n",
       "  'on a to quality scale the difference between the phrasebased and syntaxbased systems was on average between and points',\n",
       "  'when using a phrasebased translation model one can easily extract the phrase pair the mutual the mutual and use it during the phrasebased model estimation phrase and in decoding'],\n",
       " 905: ['zens and ney describe a noisyor combination where sj is the probability that sj is not in the translation of t and psjti is a lexical probability',\n",
       "  'we tested different phrasetable smoothing techniques in two different translation settings european language pairs with relatively small corpora and chinese to english translation with large corpora',\n",
       "  'we show that any type of smoothing is a better idea than the relativefrequency estimates that are often used'],\n",
       " 906: ['we do not require labeled training data in the new domain to demonstrate an improvement over our baseline models',\n",
       "  'here structural learning is different from learning with structured outputs a common paradigm for discriminative natural language processing models',\n",
       "  'to the best of our knowledge this is the first work to use unlabeled data from both domains to find feature correspondences'],\n",
       " 907: ['in this paper we present a method which extends the applicability of ilp to a more complex set of problems',\n",
       "  'this is an online algorithm that learns by parsing each sentence and comparing the result with a gold standard',\n",
       "  'although slower than the baseline approach our method can still parse large sentences more than tokens in a reasonable amount of time less than a minute'],\n",
       " 908: ['while the developmentset results would induce us to utilize the standard threshold value of which is suboptimal on the test set the bagr agreementlink policy still achieves noticeable improvement over not using agreement links test set vs',\n",
       "  'while such functionality is well beyond the scope of our current study we are optimistic that we can develop methods to exploit additional types of relationships in future work',\n",
       "  'when we impose the constraint that all speech segments uttered by the same speaker receive the same label via samespeaker links both testset and tion accuracy in percent'],\n",
       " 909: ['wilson et al proposed supervised learning dividing the resources into prior polarity and context polarity which are similar to polar atoms and syntactic patterns in this paper respectively',\n",
       "  'wilson et al prepared prior polarities from existing resources and learned the context polarities by using prior polarities and annotated corpora',\n",
       "  'when the window size is oo implying anywhere within a discourse the ratio is larger than the baseline by only and thus these types of coherency are not reliable even though the number of clues is relatively large'],\n",
       " 910: ['we present an approach for the joint extraction of entities and relations in the con text of opinion recognition and analysis',\n",
       "  'while good performance in entity or relation extraction can contribute to better performance ofthe final system this is not always the case',\n",
       "  'the global inference procedure is implemented via integer linear programming ilp to produce an optimal and coherent extraction of entities and relations'],\n",
       " 911: ['in the nlp literature a common approach is to model the conditional distribution of label sequences given the label sequences',\n",
       "  'we showed that in this framework it is possible to perform accurate broadcoverage tagging with state of the art sequence learning methods',\n",
       "  'it is also possible that this kind of shallow semantic information can help building more sophisticated linguistic analysis as in full syntactic parsing and semantic role labeling'],\n",
       " 912: ['the context vector is formed by taking the aggregate of the word vectors of the words in the augmented context',\n",
       "  'each of the word pairs have been scored by humans on a scale of to where is the most related',\n",
       "  'the sense of the target word that is most related to its context is selected as the intended sense of the target word'],\n",
       " 913: ['using this model we obtain a classification accuracy of only which is much lower than the accuracy previously achieved at the document level',\n",
       "  'although their research may have some similar goals they do not take a computational approach to analyzing large collections of documents',\n",
       "  'words in the document are then sampled from a multinomial distribution where ln is the length of the document'],\n",
       " 914: ['for most parsers their ranking for a specific language differs at most a few places from their overall ranking',\n",
       "  'that approach is based on a conversion from constituent structure to dependency structure by recursively defining a head for each constituent',\n",
       "  'the parsing order directly determines what information will be available from the history when the next decision needs to be made'],\n",
       " 916: ['however if we only look at performance for sentences of length less than the labeled accuracy is still only',\n",
       "  'the results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language',\n",
       "  'thus a joint model of parsing and labeling could not easily include them without some form of reranking or approximate parameter estimation'],\n",
       " 917: ['we use svm classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion',\n",
       "  'we projectivize training data by a minimal transformation lifting nonprojective arcs one step at a time and extending the arc label of lifted arcs using the encoding scheme called head by nivre and nilsson which means that a lifted arc is assigned the label rth where r is the original label and h is the label of the original head in the nonprojective dependency graph',\n",
       "  'we have the best reported score for japanese swedish and turkish and the score for arabic danish dutch portuguese spanish and overall does not differ significantly from the best one'],\n",
       " 918: ['that is they do not merely learn correspondences between phrases but also segmentations of the source and target sentences',\n",
       "  'the primary increase in richness from generative wordlevel models to generative phraselevel models is due to the additional latent segmentation variable',\n",
       "  'the generative model defined below is evaluated based on the bleu score it produces in an endtoend machine translation system from english to french'],\n",
       " 919: ['for arabicenglish using features based only on words of the target sentence the classification error rate can be reduced to',\n",
       "  'we use a stateoftheart baseline system which would have obtained a good rank in the last nist evaluation nist',\n",
       "  'we evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a wordaligned corpus'],\n",
       " 921: ['in this work we applied syntax based resources the target language parser to annotate and generalize phrase translation tables extracted via existing phrase extraction techniques',\n",
       "  'while yamada and knight represent syntactical information in the decoding process through a series of transformation operations we operate directly at the phrase level',\n",
       "  'while our submission time system syn using lm for rescoring only shows no improvement over the baseline we clearly see the impact of integrating the language model into the kbest list extraction process'],\n",
       " 922: ['we also define a direct probability model and use a lineartime dynamic programming algorithm to search for the best derivation',\n",
       "  'we also define a direct probability model and use a lineartime dynamic programming algorithm to search for the best derivation',\n",
       "  'since we are translating in the other direction we use the first english reference as the source input and the chinese as the single reference'],\n",
       " 924: ['the algorithms ability to separate the merged graph into its previous parts can be measured in an unsupervised way',\n",
       "  'while the games goal is to arrive at some funny derivative of the original message by passing it through several noisy channels the cw algorithm aims at finding groups of nodes that broadcast the same message to their neighbors',\n",
       "  'when generating swgraphs with the steyverstenenbaum model we fixed m to and varied n and the merge rate r which is the fraction of nodes of the smaller graph that is merged with nodes of the larger graph'],\n",
       " 925: ['finally we explore for the first time the utility of a joint phrasal translation model as a word alignment method',\n",
       "  'this section describes how to use our phrasal itg first as a translation model and then as a phrasal aligner',\n",
       "  'in the experiments described in section all systems that do not use itg will take advantage of the complete training set'],\n",
       " 926: ['one of the strengths of the factored model is it allows for ngram distributions over factors on the target',\n",
       "  'where not otherwise specified the postag and supertag sequence models are gram mod els and the language model is a gram model',\n",
       "  'we use a model which combines more specificdependencies on source words and source ccg su pertags with a more general model which only has dependancies on the source word see equation we explore two different ways of balancing the sta tistical evidence from these multiple sources'],\n",
       " 927: ['to our knowledge this approach to dynamic adaptation for smt is novel and it is one of the main contributions of the paper',\n",
       "  'in this setting tm adaptation is much less effective not significantly better than the baseline performance of combined lm and tm adaptation is also lower',\n",
       "  'the most successful is to weight component models in proportion to maximumlikelihood em weights for the current text given an ngram language model mixture trained on corpus components'],\n",
       " 928: ['the data used in this years shared task was similar to the data used in last years shared task',\n",
       "  'in order to draw judges attention to these regions we highlighted the selected source phrases and the corresponding phrases in the translations',\n",
       "  'we measured the correlation of the automatic evaluation metrics with the different types of human judgments on data conditions and report these in section'],\n",
       " 930: ['if more than one reference translation is available the translation is scored against each reference independently and the best scoring pair is used',\n",
       "  'bleu is fast and easy to run and it can be used as a target function in parameter optimization training procedures that are commonly used in stateoftheart statistical mt systems och',\n",
       "  'when optimizing correlation with the sum of adequacy and fluency optimal values fall in between the values found for adequacy and fluency'],\n",
       " 932: ['we found similar results when comparing the judgements of the classifier to rater agreement was high and kappa was low',\n",
       "  'results of the comparison between the classifier and the test set showed that the overall proportion of agreement between the text and the classifier was',\n",
       "  'our work is novel in that we are the first to report specific performance results for a preposition error detector trained and evaluated on general corpora'],\n",
       " 933: ['the induced clusters are compared to the sets of examples tagged with the given gold standard word senses classes and evaluated using the fscore measure for clusters',\n",
       "  'wsd is performed by assigning to each in duced cluster a score equal to the sum of weights of hyperedges found in the local context of the target word',\n",
       "  'with this goal onmind we gave all the participants an unlabeled cor pus and asked them to induce the senses and create a clustering solution on it'],\n",
       " 934: ['when this mapping was not straightforward we just adopted the wordnet sense inventory for that wordwe released the entire sense groupings those in duced from the manual mapping for words in the test set plus those automatically derived on the other words and made them available to the participants',\n",
       "  'we report about the use of semantic resources as well as semantically annotated corpora sc semcor dso defence science organ isation corpus se senseval corpora omwe open mind word expert xwn extended word net wn wordnet glosses andor relations wnd wordnet domains as well as information about the use of unannotated corpora uc training tr mfs based on the semcor sense frequencies and the coarse senses provided by the organizers cs',\n",
       "  'we observe that on a technical domain suchas computer science most supervised systems per formed worse due to the nature of their training set'],\n",
       " 935: ['whilstresearchers believe that it will ultimately prove useful for applications which need some degree of se mantic interpretation the jury is still out on this point',\n",
       "  'we tookthe word with the largest similarity or smallest dis tance for sd and l for best and the top for oot',\n",
       "  'we thank serge sharoff for the use of his internet corpus julie weeds for the software we used for producing the distributional similarity baselines and suzanne stevenson for suggesting the oot task'],\n",
       " 936: ['while we expected a raise in the case of the us census names the other two cases just show that there is a high and unpredictable variability which would require much larger data sets to have reliable population samples',\n",
       "  'we reused the web corpus mann which contains names randomly picked from the us census and was well suited for the task',\n",
       "  'we preferred how ever to leave the corpus as is and concentrate our efforts in producing clean training and test datasets rather than investing time in improving trial data'],\n",
       " 937: ['wvalis hybrid approach outperforms the other systems in task b and using relaxed scoring in task c as well',\n",
       "  'with this more complete temporal annotation it is no longer possible to simply evaluate the entire graph by scoring pairwise comparisons',\n",
       "  'when applied to the test data the task b system was run first in order to supplythe necessary features to the task a and task c sys temslccte automatically identifies temporal refer ring expressions events and temporal relations in text using a hybrid approach leveraging variousnlp tools and linguistic resources at lcc'],\n",
       " 938: ['whether or not the more coarsegrained senses are effective in improving natural language processing applications remains to be seen',\n",
       "  'we selecteda total of lemmas verbs and nouns con sidering the degree of polysemy and total instances that were annotated',\n",
       "  'we proposed two levels of participation in thistask i closed the systems could use only the an notated data provided and nothing else'],\n",
       " 940: ['figure gives an example of a nonprojective graph for a sentence that has also been extracted from the penn treebank',\n",
       "  'in the supervised setting this model can be trained with maximum likelihood estimation which amounts to simple counts over the data',\n",
       "  'we show below that this is a crucial feature in the development of the complexity results we have obtained in the previous sections'],\n",
       " 941: ['the data points that we have available consist of a set of human judgments each ranking the output of systems',\n",
       "  'we evaluated translation tasks with three different types of judgments for most of them for a total of different conditions',\n",
       "  'rather than weighting individual systems it incorporated weighted features that indicated which language the system was originally translating from'],\n",
       " 942: ['in this paper we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better mt performance',\n",
       "  'better segmentation performance should lead to better mt performance observation we have shown in hypothesis that it is helpful to segment chinese texts into words first',\n",
       "  'first we found that neither characterbased nor a standard word segmentation standard are optimal for mt and show that an intermediate granularity is much more effective'],\n",
       " 943: ['with parallel computing processing time wall time can often be cut down by one or two orders of magnitude',\n",
       "  'with compatible interface mgiza is suitable for a dropin replacement for giza while pgiza can utilize huge computation resources which is suitable for building large scale systems that cannot be built using a single machine',\n",
       "  'while working on the required modification to giza to run the alignment step in parallel we identified a bug which needed to be fixed'],\n",
       " 944: ['while a limited amount of gold standard annotated data was prepared for the parser evaluation shared task this is the main source of goldstandard sd data which is currently available',\n",
       "  'when the relation between a head and its dependent can be identified more precisely relations further down in the hierarchy are used but when it is unclear more generic dependencies are possible dp dp',\n",
       "  'when seeking a goldstandard dependency scheme for parser evaluation the ultimate goal of such an evaluation is an important question'],\n",
       " 945: ['experiments on the penn wsj treebank show that the model achieves stateoftheart performance for both constituent and dependency accuracy',\n",
       "  'the efficiency of the parsing algorithm is important in applying the parsing model to test sentences and also when training the model using discriminative methods',\n",
       "  'in this case we could define which can be computed with insideoutside style algorithms applied to the data structures from eisner'],\n",
       " 946: ['zhang et al achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus',\n",
       "  'wsem is the weight assigned to the semantic task the macro labeled fi score which was used for the ranking of the participating systems is computed as the harmonic mean of lmp and lmr',\n",
       "  'who within an ensemblebased architecture implemented a joint syntacticsemantic model using maltparser with labels enriched with semantic information lluis and marquez who used a modified version of the eisner algorithm to jointly predict syntactic and semantic dependencies and finally sun et al who integrated dependency label classification and argument identification using a maximumentropy markov model'],\n",
       " 948: ['in general system combinations performed as well as the best individual systems but not statistically significantly better than them',\n",
       "  'we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for more than metrics',\n",
       "  'wpf and wpbleu popovic and ney these metrics are based on words and part of speech sequences wpf is an ngram based fmeasure which takes into account both word ngrams and part of speech ngrams wpbleu is a combnination of the normal blue score and a part of speechbased bleu score'],\n",
       " 949: ['hierarchical phrasebased translation requires a translation grammar extracted from a parallel corpus where grammar rules include associated feature values',\n",
       "  'where there is a clear point of departure for research a basic implementation of each interface is provided as an abstract class to minimize the work necessary for new extensions',\n",
       "  'when designing our toolkit we applied general principles of software engineering to achieve three major goals extensibility endtoend coherence and scalability'],\n",
       " 950: ['we proposed to generate synthetic parallel data by translating monolingual adaptation data with a background system and to train statistical models from the synthetic corpus',\n",
       "  'with respect to the standard procedure the total training time was reduced by almost phrase extraction produced more phrase pairs and the final translation system showed a loss in translation performance bleu score below relative',\n",
       "  'section presents previous work on the problem of adaptation in smt section introduces the exemplar task and research questions we addressed section describes the smt system and the adaptation techniques that were investigated section presents and discusses experimental results and section provides conclusions'],\n",
       " 951: ['system level scores are determined by taking the weighted average of the document level scores in the same manner',\n",
       "  'these correct translations differ not only in their word choice but also in the order in which the words occur',\n",
       "  'in order to evaluate the state of automatic mt evaluation nist tested metrics across a number of conditions across test sets'],\n",
       " 952: ['for example the average length of the scope of not is in the abstracts subcorpus and in the papers subcorpus',\n",
       "  'it achieves the best results to date for this task with an error reduction of compared to current state of the art results',\n",
       "  'the latter experiment is therefore a test of the robustness of the system when applied to different text types within the same domain'],\n",
       " 953: ['therefore they apply a baseline ner system and use the resulting predictions as features in a second level of inference',\n",
       "  'we have presented a simple model for ner that uses expressive features to achieve new state of the art performance on the named entity recognition task',\n",
       "  'however as we show these contributions are to a large extent independent and as we show the approaches can be used together to yield better results'],\n",
       " 954: ['the latter experiment is therefore a test of the robustness of the system when applied to different text types within the same domain',\n",
       "  'although the system was developed and tested on biomedical text the same approach can also be applied to text from other domains',\n",
       "  'the cues that are not present in the training data cannot be learned in the test data and the same applies to their scope'],\n",
       " 955: ['the results on the five event types involving only a single primary theme argument are shown in one merged class simple event',\n",
       "  'although both alternatives are reasonable the need to have consistent training and evaluation data requires a consistent choice to be made for the shared task',\n",
       "  'without constraints on the type of theme arguments the following two annotations are both legitimate the two can be seen as specifying the same event at different levels of specificity'],\n",
       " 956: ['if a word has more than one sense then it can be found in more than one thesaurus category',\n",
       "  'we compared a subset of our lexicon with existing gold standard data to show that the annotations obtained are indeed of high quality',\n",
       "  'for these questions we listed head words from both the senses categories as two of the alternatives probability of a random choice being correct is'],\n",
       " 957: ['if we increase the size of the data tenfold but also increase the noise can learning still be successful',\n",
       "  'you can do this by measuring the httpcastingwordscom httpgroupscsailmiteduuidturkit interannotator agreement of the turkers against ex perts on small amounts of gold standard data or by stating what controls you used and what criteria youused to block bad turkers',\n",
       "  'workshop par ticipants struggled with how to attract turkers howto price hits hit design instructions cheating de tection etc no doubt that as work progresses so will a communal knowledge and experience of how to use mturk'],\n",
       " 958: ['we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for metrics',\n",
       "  'system combinations are listed in the order of how often their translations were ranked higher than or equal to any other system',\n",
       "  'your edited translations the machine translations the shortage of snow in mountain the shortage of snow in mountain worries the hoteliers worries the hoteliers correct reset edited no corrections needed unable to the deserted tracks are not the deserted tracks are not putting down problem only at the exploitants of skilift putting down problem only at the exploitants of skilift correct reset edited no corrections needed unable to the lack of snow deters the people the lack of snow deters the people to reserving their stays at the ski in the hotels and pension to reserving their stays at the ski in the hotels and pension correct reset edited no corrections needed unable to thereby is always possible to thereby is always possible to track free bedrooms for all the dates in winter including christmas and nouvel an track free bedrooms for all the dates in winter including christmas and nouvel an'],\n",
       " 959: ['the vector space was built with the most frequent tokens in the corpus a cutoff point that included all the extracted an pairs',\n",
       "  'a very desirable result would be if any predicted compositional an vector could be reliably used instead of the extracted bigram',\n",
       "  'wordspace vector models or distributional models of semantics henceforth dsms are computational models that build contextual semantic representations for lexical items from corpus data'],\n",
       " 960: ['this formulation is a major shift from existing approaches that rely on extracting parsing rules from the training data',\n",
       "  'analysis over the training data shows that in examples both approaches predict a logical form that gives the correct answer',\n",
       "  'our experimental results show that our model with response driven learning can outperform existing models trained with annotated logical forms'],\n",
       " 961: ['task systems differ in the number of class labels used as target and in the machine learning approaches applied',\n",
       "  'both tasks were addressed in the conll shared task in order to provide uniform manually annotated benchmark datasets for both and to compare their difficulties and stateoftheart solutions for them',\n",
       "  'zhao et al extended the biological cue word dictionary of their system using it as a feature for classification by the frequent cues of the wikipedia dataset while ji et al'],\n",
       " 962: ['for the feature based model we use some of the features proposed in past literature and propose new features',\n",
       "  'we experiment with three types of models unigram model a feature based model and a tree kernel based model',\n",
       "  'our feature based model that uses only features achieves similar accuracy as the unigram model that uses over features'],\n",
       " 963: ['while the task is related to synonymy relation extraction yu and agichtein it has a novel definition of renaming one name permanently replacing the other',\n",
       "  'while the basic task setup and entity definitions follow those of the ge task epi extends on the extraction targets by defining new event types relevant to task topics including major protein modification types and their reverse reactions',\n",
       "  'while finding connections between event triggers and protein references is a major part of event extraction it becomes much harder if one is replaced with a coreferencing expression'],\n",
       " 964: ['this observation suggests a different event annotation scheme or a different event extraction strategy would be required for methods sections',\n",
       "  'since the training data from the full text collection is relatively small despite of the expected rich variety of expressions in full text it is expected that generalization of a model from the abstract collection to full papers would be a key technique to get a reasonable performance',\n",
       "  'when only primary arguments are considered the first five event types in table are classified as simple event types requiring only unary arguments'],\n",
       " 965: ['groups submitted only closed track results groups only open track results and groups submitted both closed and open track results',\n",
       "  'the muc and ace corpora are the two that have been used most for reporting comparative results but they differ in the types of entities and coreference annotated',\n",
       "  'word sense we trained a word sense tagger using a svm classifier and contextual word and part of speech features on all the training portion of the ontonotes data'],\n",
       " 966: ['using predicted mentions our system had an overall score of in the closed track and in the open track',\n",
       "  'our system was ranked first in both tracks with a score of in the closed track and in the open track',\n",
       "  'our results demonstrate that despite their simplicity deterministic models for coreference resolution obtain competitive results eg we obtained the highest scores in both the closed and open tracks and respectively'],\n",
       " 967: ['in our analysis we aimed to address the following questions tables show the system ranking for each of the translation tasks',\n",
       "  'we also included two commercial offtheshelf mt systems two online statistical mt systems and five online rulebased mt systems',\n",
       "  'we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for evaluation metrics'],\n",
       " 968: ['while versions tuned to various types of human judgments do not perform as well as the widely used bleu metric papineni et al a balanced tuning version of meteor consistently outperforms bleu over multiple endtoend tunetest runs on this data set',\n",
       "  'whereas previous versions of meteor simply strip punctuation characters prior to scoring version includes a new text normalizer intended specifically for translation evaluation',\n",
       "  'we use the nist open machine translation evaluation urduenglish parallel data przybocki plus m words of monolingual data from the english gigaword corpus parker et al to build a standard moses system hoang et al as follows'],\n",
       " 969: ['with some minor api changes namely returning the length of the ngram matched it could also be fasterthough this would be at the expense of an optimization we explain in section',\n",
       "  'with a good hash function collisions of the full bit hash are exceedingly rare one in billion queries for our baseline model will falsely find a key not present',\n",
       "  'while sorted arrays could be used to implement the same data structure as probing effectively making m we abandoned this implementation because it is slower and larger than a trie implementation'],\n",
       " 970: ['the final model was selected based on its performance on the development set and the number of support vectors',\n",
       "  'the focus is on special word relations and special phrase patterns thus several feature templates on this topic are extracted',\n",
       "  'we also included two commercial offtheshelf mt systems three online statistical mt systems and three online rulebased mt systems'],\n",
       " 971: ['in the terms of our in this example french is used as the source language and english as the target',\n",
       "  'the alignment algorithm consists of two steps estimate translation probabilities and use these probabilities to search for most probable alignment path',\n",
       "  'wordalign thus provides an example how a model such as brown et al s model that was originally designed for research in statistical machine translation can be modified to achieve practical though less ambitious goals in the near term'],\n",
       " 972: ['whether the human language engine is organized as a pipeline plus a few feedback loops or an every module talks to every other module architecture is unknown at this point hopefully new psycholinguistic experiments will shed more light on this issue',\n",
       "  'we have insufficient engineering data at present to make any wellsubstantiated claims about whether the oneway pipeline has the optimal costbenefit tradeoff or not and in any case this will probably depend somewhat on the circumstances of each application reiter and mellish but the circumstantial evidence on this question is striking despite the fact that so many theoretical papers have argued against pipelines and very few if any have argued for pipelines every one of the applicationsoriented systems examined in this survey chose to use the oneway pipeline architecture',\n",
       "  'unfortunately while all of the systems possessed a module which converted semantic representations into deep syntactic ones each system used a different name for this module'],\n",
       " 973: ['with unsupervised learning the learner does not have a gold standard training corpus with which accuracy can be measured',\n",
       "  'conclusions in this paper we have presented a new algorithm for unsupervised training of a rulebased part of speech tagger',\n",
       "  'the most accurate stochastic taggers use estimates of lexical and contextual probabilities extracted from large manually annotated corpora eg'],\n",
       " 974: ['two human judges annotated the attachment decision for test examples and the method performed at accuracy on these cases',\n",
       "  'for each such vp the head verb first head noun preposition and second head noun were extracted along with the attachment decision for noun attachment for verb',\n",
       "  'in general a probabilistic algorithm will make an estimate of this probability the decision can then be made using the test if this is true the attachment is made to the noun if not then it is made to the verb'],\n",
       " 975: ['the first tests for the presence of particular context words within a certain distance of the ambiguous target word',\n",
       "  'it classifies an ambiguous target word by matching each feature in the list in turn against the target context',\n",
       "  'instead therefore we assume that the presence of one word in the context is independent of the presence of any other word'],\n",
       " 976: ['there is a tradition in sense disambiguation of taking particularly ambiguous words and evaluating a systems performance on those words',\n",
       "  'yet a computational system has no choice but to consider other more awkward possibilities for example this cluster might be capturing a distributional relationship between advice as one sense of counsel and royalty as one sense of court',\n",
       "  'yarowskys algorithm for sense disambiguation can be thought of as a way of determining how rogets thesaurus categories behave with respect to contextual features'],\n",
       " 977: ['while this automatic derivation process introduced a small percentage of errors of its own it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing',\n",
       "  'while brackets must be correctly paired in order to derive a chunk structure it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags the few hard cases that arise can be handled completely locally',\n",
       "  'when this approach is applied to partofspeech tagging the possible sources of evidence for templates involve the identities of words within a neighborhood of some appropriate size and their current partofspeech tag assignments'],\n",
       " 978: ['this makes it practical to train on small handbuilt corpora for language pairs where large bilingual corpora are unavailable',\n",
       "  'these conclusions could not have been drawn without a uniform framework for filter comparison or without a technique for automatic evaluation',\n",
       "  'given a test set of aligned sentences a better translation lexicon will contain a higher fraction of the source word target word pairs in those sentences'],\n",
       " 979: ['supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger',\n",
       "  'our case representation is at this point simpler only the ambiguous tags not the words themselves or any other information are used',\n",
       "  'without optimisation it has an asymptotic retrieval complexity of nf where n is the number of items in memory and f the number of features'],\n",
       " 980: ['due to the use of the wall street journal the product sense is more than times as common as any of the others',\n",
       "  'without this optimization testing would have been several orders of magnitude slower than making a decision by testing only a small subset of highly predictive features',\n",
       "  'with respect to training time the symbolic methods are significantly slower since they are searching for a simple declarative representation of the concept'],\n",
       " 981: ['weischedel et al provide the results from a battery of tritag markov model experiments in which the probability pwt of observing a word sequence w wi w wn together with a tag sequence t is given by furthermore pwiiti for unknown words is computed by the following heuristic which uses a set of predetermined endings this approximation works as well as the maxent model giving unknown word accuracyweischedel et al on the wall st journal but cannot be generalized to handle more diverse information sources',\n",
       "  'using the set of difficult words the model performs at accuracy on the development set an insignificant improvement from the baseline accuracy of',\n",
       "  'unlike sdt the maxent training procedure does not recursively split the data and hence does not suffer from unreliable counts due to data fragmentation'],\n",
       " 982: ['however while the insideoutside algorithm is a grammar reestimation algorithm the algorithm presented here is just a parsing algorithm',\n",
       "  'for a given sentence and a given parse tree there are many different derivations that could lead to that parse tree',\n",
       "  'however unlike in the hmm case where the algorithm produces a simple state sequence in the pcfg case a parse tree is produced resulting in additional constraints'],\n",
       " 984: ['the amount of available bilingual parallel corpora is still relatively small in comparison to the large amount of available monolingual text',\n",
       "  'our system then proposes two sets of outputs for each japanese term our system proposes the top candidates from the set of noun phrases',\n",
       "  'wwz w is the weighted mutual information in our algorithm since it is most suitable for lexicon compilation of midfrequency technical words or terms as an initial step all prw are precomputed for the seed words in both languages'],\n",
       " 985: ['what is most interesting here is the way in which strongly selecting word w is typically the head of a noun phrase which could lead the model astray for example toy soldiers behave differently from soldiers mccawley',\n",
       "  'thus despite the absence of class annotation in the training text it is still possible to arrive at a usable estimate of classbased probabilities',\n",
       "  'this means that the observed countverbobj drink coffee will be distributed by adding a to the joint frequency with drink for each of the classes containing coffee'],\n",
       " 986: ['the first pass takes an input sentence shown in figure and uses tag to assign each word a pos tag',\n",
       "  'this paper presents a statistical parser for natural language that finds one or more scored syntactic parse trees for a given input sentence',\n",
       "  'typically the procedures postulate many different values for a which cause the parser to explore many different derivations when parsing an input sentence'],\n",
       " 987: ['the basic idea is that we can use information from parsing with one grammar to speed parsing with another',\n",
       "  'however this does not give information about the probability of the node in the context of the full parse tree',\n",
       "  'they computed a score for each sequence as the minimum of the scores of each node in the sequence and computed a score for each node in the sequence as the minimum of three scores one based on statistics about nodes to the left one based on nodes to the right and one based on unigram statistics'],\n",
       " 988: ['the first step in the experiment was the construction of new versions of the test data in addition to the original version',\n",
       "  'thirteen million words in both languages combined were used for training and another two and a half million were used for testing',\n",
       "  'perhaps the method presented here combined with an appropriate translation model can make some progress on the word identification problem for languages like chinese and japanese'],\n",
       " 989: ['the input to the system is a small set of seed words for a category and a representative text corpus',\n",
       "  'in this paper we present a corpusbased method that can be used to build semantic lexicons for specific categories',\n",
       "  'criteria on a scale of to rate each words strength of association with the given category using the following criteria'],\n",
       " 990: ['in this study we evaluated the performance of three unsupervised learning algorithms on the disambiguation of words in naturally occurring text',\n",
       "  'this assumption is based on the success of the naive bayes model when applied to supervised wordsense disambiguation eg',\n",
       "  'for the nouns there was no significant difference between feature sets a and b when using the em algorithm'],\n",
       " 992: ['we measured the ability of judges to agree with one another using the notion of percent agreement that was defined by gale and used extensively in discourse segmentation studies passonneau and litman hearst percent agreement reflects the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion the percent agreements computed for each of the five texts and each level of importance are given in table the agreements among judges for our experiment seem to follow the same pattern as those described by other researchers in summarization johnson that is the judges are quite consistent with respect to what they perceive as being very important and unimportant but less consistent with respect to what they perceive as being less important in contrast with the agreement observed among judges the percentage agreements computed for importance assignments that were randomly generated for the same texts followed a normal distribution with p o these results suggest that the agreement among judges is significant agreement among judges with respect to the importance of each textual unit',\n",
       "  'we described the first experiment that shows that the concepts of rhetoncal analysis and nucleanty can be used effectively for summarizing text the experiment suggests that discoursebased methods can account for detemmimg the most important units in a text with a recall and precision as high as we showed how the concepts of rhetorical analysis and nucleanty can be treated algorithmically and we compared recall and precision figures of a summarization program that implements these concepts with recall and precision figures that pertain to a baseline algorithm and to a commercial system the microsoft office summarizer the discoursebased summanzation program that we propose outperforms both the baseline and the commercial summarizer see table however since its results do not match yet the recall and precision figures that pertain to the manual discourse analyses it is likely that improvements of the rhetorical parser algorithm will result in better performance of subsequent implemetations',\n",
       "  'we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text we show how these concepts can be implemented and we discuss results that we obtained with a discoursebased summanzation program motivation the evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some consamples of the output in very few cases output of a summarization program with a humanmade summary or evaluated with the help of human subjects usually the results are modest unfortunately evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions the position that we take in this paper is that in order to build highquality summarization programs one needs to evaluate not only a representative set of automatically generated outputs a highly difficult problem by itself but also the adequacy of the assumptions that these programs use that way one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each with few exceptions automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text see paice for an excellent overview determining the salient parts is considered to be achievable because one or more of the following assumptions hold i important sentences in a text contain words that are used frequently lahn edmundson n important sentences contain words that are used in the tide and section headings edmundson in important sentences are located at the beginning or end of paragraphs baxendale iv important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically through training techniques lin and hovy v important sentences use words as greatest and significant or indiphrases as the main aim of this paper and the purpose of this article while nonimportant senuse words as impossible edmundson rush salvador and zamora vi important sentences and concepts highest connected entities in elaborate semantic structures skorochodko lin barzilay and elhadad and vn imponant and nonimportant sentences are derivable from a discourse representation of the text sparck jones ono surmta and mike in determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections computers are accurate tools flowever in determining the concepts that are semantically related or the discourse structure of a text computers are no longer so accurate rather they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement although it is plausible that elaborate cohesionand coherencebased structures can be used effectively in summarization we believe that before building summarization programs we should determine the extent to which these assumptions hold in this paper we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text we show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program from discourse trees to summaries an empirical view'],\n",
       " 993: ['additionally the final version will contain examples for each concept which are to be automatically extracted from the corpus',\n",
       "  'we use the princeton wordnet technology for the database format database compilation as well as the princeton wordnet interface applying extensions only where necessary',\n",
       "  'we therefore propose a mixed approach treating irregular particle verbs by enumeration and regular particle verbs in a compositional manner'],\n",
       " 994: ['adding in rules and resolves a total of pronouns correctly with only mistakes a precision of and recall of',\n",
       "  'i the named entity task at muc used a similar classification task and the best system performance was precision recall',\n",
       "  'this paper presents a high precision pronoun resolution system that is capable of greater than precision with and better recall for some pronouns'],\n",
       " 995: ['they reach a accuracy on a brown corpus subset and a on a subset of the wall street journal corpus',\n",
       "  'for instance design is used as a noun repeatedly in one of the documents while its summary uses design as a verb',\n",
       "  'wordnet does not include crosspartofspeech semantic relations so this relation cannot be used with word senses while term indexing simply and successfully does not distinguish them'],\n",
       " 997: ['furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing',\n",
       "  'furthermore as seen in figure running our parser past the first parse by a small amount of the edges required for the first parse produces still more accurate parses',\n",
       "  'zn in the bottomup variant of the earley algorithm where a gt is a production of the original grammar'],\n",
       " 998: ['in addition a token could be tagged as other to indicate that it is not part of a named entity',\n",
       "  'while was still the fourth highest score out of the twelve participants in the evaluation we feel that it is necessary to view this number as a crossdomain portability result rather than as an indicator of how the system can do on unseen data within its training domain',\n",
       "  'while all of menes features have binaryvalued output the binary features are features whose associated historyview can be considered to be either on or off for a given token'],\n",
       " 999: ['a statistical approach is present in the discourse module only where it is used to determine the probability that a noun verb phrase is the center of a sentence',\n",
       "  'when viewed in this way a can be regarded as an index into these vectors that specifies which value is relevant to the particular choice of antecedent',\n",
       "  'we would like to know therefore whether the pattern of pronoun references that we observe for a given referent is the result of our supposed hypothesis about pronoun reference that is the pronoun reference strategy we have provisionally adopted in order to gather statistics or whether the result of some other unidentified process'],\n",
       " 1000: ['the texts are of a specific type there are only three of them and we have not used all rhetorical relations',\n",
       "  'stochastic search methods are a form of heuristic search that use the following generic algorithm use these to generate one or more new random variations',\n",
       "  'in this task one is given a set of facts all of which should be included in a text and a set of relations between facts some of which can be included in the text'],\n",
       " 1003: ['we have presented a new approach to noun phrase coreference resolution that treats the problem as a clustering task',\n",
       "  'second our approach is unsupervised and requires no annotation of training data nor a large corpus for computing statistical occurrences',\n",
       "  'this noun phrase representation is a first approximation to the feature vector that would be required for accurate coreference resolution'],\n",
       " 1004: ['performance for the combined sources is in all cases greater than for the morphology or context source used alone',\n",
       "  'thus the total number of contextual matches for the seed words was quite variable from and difficult to control',\n",
       "  'it is also necessary to have a relatively large unannotated text for bootstrapping the contextual models and classifying new named entities'],\n",
       " 1005: ['the approach builds from an initial seed set for a category and is quite similar to the decision list approach described in yarowsky',\n",
       "  'many statistical or machinelearning approaches for natural language problems require a relatively large amount of supervision in the form of labeled training examples',\n",
       "  'thus an explicit assumption about the redundancy of the features that either the spelling or context alone should be sufficient to build a classifier has been built into the algorithm'],\n",
       " 1006: ['the set is then compared with the set generated from the penn treebank parse to determine the precision and recall',\n",
       "  'we then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers',\n",
       "  'it is closer to the smaller value of precision and recall when there is a large skew in their values'],\n",
       " 1008: ['the other word is the head of the phrase which is annotated with this grammatical relation in the treebank',\n",
       "  'the instance contains information about the verb and the focus a feature for the word form and a feature for the pos of both',\n",
       "  'that paper also shows that the chunking method proposed here performs about as well as other methods and that the influence of tagging errors on np chunking is less than']}"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(list(frases_papers_teste.keys()),frase_objetivo_teste_exp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "d8d45665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:32:28.390664Z",
     "start_time": "2023-05-01T21:32:10.308349Z"
    }
   },
   "outputs": [],
   "source": [
    "p_uni, r_uni, f1_uni = calculo_rouge(frase_objetivo_teste_exp2,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81253d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:12.578891Z",
     "start_time": "2023-05-01T14:24:57.806389Z"
    }
   },
   "source": [
    "rouge_total=[]\n",
    "for i,j in zip(frase_objetivo_teste_exp2,list(padrao_ouro_teste_frases.values())):\n",
    "    #print (len(i))\n",
    "    rouge_1={}\n",
    "    for h in range(len(i)):\n",
    "        for z in range(len(j)):\n",
    "            rouge_1[(h,z)] = (scorer.score(j[z],i[h])['rouge1'].precision)\n",
    "    rouge_total.append(rouge_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efdbb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:12.610737Z",
     "start_time": "2023-05-01T14:25:12.579738Z"
    }
   },
   "source": [
    "maximo_rouge_paper = max_rouge(rouge_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8e466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:12.626476Z",
     "start_time": "2023-05-01T14:25:12.610737Z"
    }
   },
   "source": [
    "precisao_exp3_pt2, revocacao_exp3_pt2 = rouge_precisao_revocacao(maximo_rouge_paper, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e017984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:12.689954Z",
     "start_time": "2023-05-01T14:25:12.631716Z"
    }
   },
   "source": [
    "lista=[]\n",
    "for i,j in zip(range(len(probabilidades_unigramas)),list(frases_papers_teste.keys()) ):\n",
    "    lista.append(sorted(zip(probabilidades_unigramas[i],frases_papers_teste[j]), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ab71a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.174718Z",
     "start_time": "2023-05-01T14:25:12.693949Z"
    }
   },
   "source": [
    "frases_papers_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fec61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.206230Z",
     "start_time": "2023-05-01T14:25:14.174718Z"
    }
   },
   "source": [
    "frases_papers_teste[773]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19f639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.237553Z",
     "start_time": "2023-05-01T14:25:14.206230Z"
    }
   },
   "source": [
    "for i,j in zip(range(len(maximo_rouge_paper)),frases_papers_teste.keys()):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86defe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.268860Z",
     "start_time": "2023-05-01T14:25:14.237553Z"
    }
   },
   "source": [
    "len(maximo_rouge_paper[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f417582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.284931Z",
     "start_time": "2023-05-01T14:25:14.268860Z"
    }
   },
   "source": [
    "x=[]\n",
    "for i in range(len(maximo_rouge_paper)):\n",
    "    for j in range(3):\n",
    "        x.append(maximo_rouge_paper[i][j][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb41ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.316355Z",
     "start_time": "2023-05-01T14:25:14.284931Z"
    }
   },
   "source": [
    "np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29729a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.348282Z",
     "start_time": "2023-05-01T14:25:14.317176Z"
    }
   },
   "source": [
    "for i,j in zip(frases_papers_teste.keys(),maximo_rouge_paper):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db960f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.411436Z",
     "start_time": "2023-05-01T14:25:14.348282Z"
    }
   },
   "source": [
    "dict(zip(list(frases_papers_teste.keys()),frase_objetivo_teste_exp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65384397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.474877Z",
     "start_time": "2023-05-01T14:25:14.417407Z"
    }
   },
   "source": [
    "dict(zip(list(frases_papers_teste.keys()),frase_objetivo_teste_exp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331af48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.506464Z",
     "start_time": "2023-05-01T14:25:14.480284Z"
    }
   },
   "source": [
    "print('------ revocação ---------')\n",
    "print('média recall',np.mean(revocacao_exp3_pt2))\n",
    "qtd_revocacao_exp3_pt2, media_revocacao_exp3_pt2  = analise_revocacao(revocacao_exp3_pt2, lista_padrao_ouro_teste, media_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7beb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.522492Z",
     "start_time": "2023-05-01T14:25:14.509770Z"
    }
   },
   "source": [
    "print('------ precisão ---------')\n",
    "qtd_precisao_exp3_pt2, qtd_zero_exp3_pt2, media_exp3_pt2 = analise_precisao(precisao_exp3_pt2, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59814680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.569872Z",
     "start_time": "2023-05-01T14:25:14.526194Z"
    }
   },
   "source": [
    "#Analisando o tamanho das frases de teste\n",
    "x=[]\n",
    "for j in list(padrao_ouro_treino_frases.values()):\n",
    "    for i in j:\n",
    "        x.append(len(i.split(' ')))\n",
    "np.mean(x)\n",
    "\n",
    "#Extra\n",
    "#for i in lista[0]:\n",
    "    #frases = i[1].split(' ')\n",
    "    #if len(frases) >=27:\n",
    "        #print(i)\n",
    "        \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726660e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:25:14.586003Z",
     "start_time": "2023-05-01T14:25:14.573854Z"
    }
   },
   "source": [
    "padrao_ouro_teste_frases[765]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af75500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:16:38.035947Z",
     "start_time": "2023-05-01T21:16:37.968045Z"
    }
   },
   "source": [
    "print('unigrama com rest')\n",
    "print('precisão:',media2(media(max_rouge(p_uni))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni))))\n",
    "print('f-score:',media2(media(max_rouge(f1_uni))))\n",
    "funcao_80_e_0(p_uni,r_uni)\n",
    "\n",
    "print('-----')\n",
    "print('bigrama com rest')\n",
    "print('precisão:',media2(media(max_rouge(p_bi))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi))))\n",
    "funcao_80_e_0(p_bi,r_bi)\n",
    "\n",
    "print('-----')\n",
    "print('trigrama com rest')\n",
    "print('precisão:',media2(media(max_rouge(p_r_tri))))\n",
    "print('revocação:',media2(media(max_rouge(r_r_tri))))\n",
    "print('f-score:',media2(media(max_rouge(f1_r_tri))))\n",
    "funcao_80_e_0(p_r_tri,r_r_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3d313",
   "metadata": {},
   "source": [
    "## bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "1a3a848b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:13.903234Z",
     "start_time": "2023-05-01T21:36:13.895232Z"
    }
   },
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "57a5d29e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:13.918444Z",
     "start_time": "2023-05-01T21:36:13.905443Z"
    }
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "for i in range(len(list(padrao_ouro_teste_frases.values()))):\n",
    "    for j in list(padrao_ouro_teste_frases.values())[i]:\n",
    "        x.append(len(j.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "99849362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:13.934450Z",
     "start_time": "2023-05-01T21:36:13.920444Z"
    }
   },
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in range(len(list(padrao_ouro_treino_frases.values()))):\n",
    "    for j in list(padrao_ouro_treino_frases.values())[i]:\n",
    "        y.append(len(j.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "123e197d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:13.949886Z",
     "start_time": "2023-05-01T21:36:13.936919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.62478452403754"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "07b51766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:13.965700Z",
     "start_time": "2023-05-01T21:36:13.951796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "3b69a807",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:13.981685Z",
     "start_time": "2023-05-01T21:36:13.967680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "aa6e8a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:13.997433Z",
     "start_time": "2023-05-01T21:36:13.983890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mode(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "33574349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:14.013452Z",
     "start_time": "2023-05-01T21:36:13.999449Z"
    }
   },
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in range(len(list(padrao_ouro_treino_frases.values()))):\n",
    "    for j in list(padrao_ouro_treino_frases.values())[i]:\n",
    "        y.append(len(j.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "8f769c8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:14.029432Z",
     "start_time": "2023-05-01T21:36:14.018436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "d13bed80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:14.045509Z",
     "start_time": "2023-05-01T21:36:14.032503Z"
    }
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "for i in list(padrao_ouro_teste_frases.values()):\n",
    "    x.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "31081e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:14.060688Z",
     "start_time": "2023-05-01T21:36:14.047494Z"
    }
   },
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in list(padrao_ouro_treino_frases.items()):\n",
    "    y.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "e548cfa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:14.076687Z",
     "start_time": "2023-05-01T21:36:14.062706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.783018867924529"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x)+np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "5fab0208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:20.945351Z",
     "start_time": "2023-05-01T21:36:14.078191Z"
    }
   },
   "outputs": [],
   "source": [
    "bigramas_teste          = gerador_ngramas(token_frases_teste,2)\n",
    "probabilidades_bigramas  = gerador_probabilidades(bigramas_teste, model_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "b0d9a4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:20.960701Z",
     "start_time": "2023-05-01T21:36:20.946568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilidades_bigramas==probabilidades_trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "id": "d126dd62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:20.987643Z",
     "start_time": "2023-05-01T21:36:20.962680Z"
    }
   },
   "outputs": [],
   "source": [
    "lista=[]\n",
    "for i,j in zip(range(len(probabilidades_bigramas)),list(frases_papers_teste.keys()) ):\n",
    "    lista.append(sorted(zip(probabilidades_bigramas[i],frases_papers_teste[j]), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "02696fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:21.061770Z",
     "start_time": "2023-05-01T21:36:20.987643Z"
    }
   },
   "outputs": [],
   "source": [
    "todos=[]\n",
    "for i in range(len(lista)):\n",
    "    x=[]\n",
    "    for i in list(lista[i]):\n",
    "        frases = i[1].split(' ')\n",
    "        if (len(frases) >=19) and i[0]!=1:\n",
    "            x.append(i)\n",
    "    todos.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "cafb5d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:21.077029Z",
     "start_time": "2023-05-01T21:36:21.062363Z"
    }
   },
   "outputs": [],
   "source": [
    "frases_objetivo_ml_teste=[]\n",
    "for i in range(len(todos)):\n",
    "    frases_objetivo_ml_teste.append(todos[i][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "675c5d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:21.129115Z",
     "start_time": "2023-05-01T21:36:21.077912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.0,\n",
       "   'words that cooccur with the target word in unusually great frequency especially in certain collocational relationships will tend to be reliable indicators of one of the target words senses eg flock and bulldozer for crane'),\n",
       "  (0.0,\n",
       "   'words not only tend to occur in collocations that reliably indicate their sense they tend to occur in multiple such collocations'),\n",
       "  (0.0,\n",
       "   'words in the entry appearing in the most reliable collocational relationships with the target word are given the most weight based on the criteria given in yarowsky')],\n",
       " [(0.0,\n",
       "   'word lattices are commonly used to model uncertainty in speech recognition waibel and lee and are well adapted for use with ngram models'),\n",
       "  (0.0,\n",
       "   'while true irregular forms eg child i children must be kept in a small exception table the problem of multiple regular patterns usually increases the size of this table dramatically'),\n",
       "  (0.0,\n",
       "   'while this experiment shows that statistical models can help make choices in generation it fails as a computational strategy')],\n",
       " [(0.0,\n",
       "   'where m lt n and k lt ki lt n for all i lt m for example a model pf ihi hh might be interpolated as follows where e ai hi hh for all histories hi hh'),\n",
       "  (0.0,\n",
       "   'usually an ngram model refers to a markov process where the probability of a particular token being generating is dependent on the values of the previous n tokens generated by the same process'),\n",
       "  (0.0,\n",
       "   'traditionally disambiguation problems in parsing have been addressed by enumerating possibilities and explicitly declaring knowledge which might aid the disambiguation process')],\n",
       " [(0.0,\n",
       "   'with criteria like the corpus frequency of a word its specificity for a given domain and the salience of its cooccurrence patterns it should be possible to make a selection of corresponding vocabularies in the two languages'),\n",
       "  (0.0,\n",
       "   'this study suggests that the identification of word translations should also be possible with nonparallel and even unrelated texts'),\n",
       "  (0.0,\n",
       "   'this similarity measure leads to a value of zero for identical matrices and to a value of in the case that a nonzero entry in one of the matrices always corresponds to a zerovalue in the other')],\n",
       " [(0.0,\n",
       "   'when tested on this large data set lexas performs better than the default strategy of picking the most frequent sense'),\n",
       "  (0.0,\n",
       "   'when tested on a large separately collected data set our program performs better than the default strategy of picking the most frequent sense'),\n",
       "  (0.0,\n",
       "   'when tested on a common data set our wsd program gives higher classification accuracy than previous work on wsd')],\n",
       " [(0.0,\n",
       "   'yet for intentionally knock twice this is not the case these adverbs do not commute and the semantics are distinct'),\n",
       "  (0.0,\n",
       "   'yes a different vides flexibility about which parse represents its kind of efficient parser can be built for this case class'),\n",
       "  (0.0,\n",
       "   'wittenburg assumes only one reading semantically so just one of its anala ccg fragment lacking orderchanging or higher yses fg is discovered while parsing')],\n",
       " [(0.0,\n",
       "   'we propose that by creating algorithms that optimize the evaluation criterion rather than some related criterion improved performance can be achieved'),\n",
       "  (0.0,\n",
       "   'we present two new algorithms the labelled recall algorithm which maximizes the expected labelled recall rate and the bracketed recall algorithm which maximizes the bracketed recall rate'),\n",
       "  (0.0,\n",
       "   'we have used the technique outlined in this paper in other work goodman to efficiently parse the dop model in that model the only previously known algorithm which summed over all the possible derivations was a slow monte carlo algorithm bod')],\n",
       " [(0.0,\n",
       "   'with a beam search strategy parsing speed can be improved to over sentences a minute with negligible loss in accuracy'),\n",
       "  (0.0,\n",
       "   'we use the parseval measures black et al to compare performance number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse of constituents which violate constituent boundaries with a constituent in the treebank parse'),\n",
       "  (0.0,\n",
       "   'we use the notation to state that the jth word in the reduced sentence is a modifier to the hith word with relationship r')],\n",
       " [(0.0,\n",
       "   'with few exceptions they rely on intuitive analyses of topic structure operational definitions of discourselevel properties eg interpreting paragraph breaks as discourse segment boundaries or theoryneutral discourse segmentations where subjects are given instructions to simply mark changes in topic'),\n",
       "  (0.0,\n",
       "   'with a view toward automatically segmenting a spoken discourse we would like to directly classify phrases of all three discourse categories'),\n",
       "  (0.0,\n",
       "   'while scont phrases for both speaking styles exhibited significantly shorter preceding and subsequent pauses than other phrases only the spontaneous condition showed a significantly slower rate')],\n",
       " [(0.0,\n",
       "   'written anew it probably would have been about lines characterize the relative performance of two techniques it is necessary to consider multiple training set sizes and to try both bigram and trigram models'),\n",
       "  (0.0,\n",
       "   'while the original paper katz uses a single parameter k we instead use a different k for each n gt kn'),\n",
       "  (0.0,\n",
       "   'while smoothing is a central issue in language modeling the literature lacks a definitive comparison between the many existing techniques')],\n",
       " [(0.0,\n",
       "   'when using sample selection a learning program examines many unlabeled not annotated examples selecting for labeling only those that are most informative for the learner at each stage of training seung opper and sompolinsky freund et al lewis and gale cohn atlas and ladner'),\n",
       "  (0.0,\n",
       "   'when training a bigram model indeed any hmm this is not true as each word is dependent on that before it'),\n",
       "  (0.0,\n",
       "   'when the statistics for a parameter are insufficient the variance of the posterior distribution of the estimates is large and hence there will be large differences in the values of the parameter chosen for different committee members')],\n",
       " [(0.0,\n",
       "   'when parsing the pos tags allowed for each word are limited to those which have been seen in training data for that word'),\n",
       "  (0.0,\n",
       "   'we use the parseval measures black et al to compare performance number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse crossing brackets number of constituents which violate constituent boundaries with a constituent in the treebank parse'),\n",
       "  (0.0,\n",
       "   'we intend to perform experiments to compare the perplexity of the various models and a structurally similar pure pcfg')],\n",
       " [(0.0,\n",
       "   'yarowsky yarowsky proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus thus avoided the need to handannotate any examples'),\n",
       "  (0.0,\n",
       "   'wsd is useful in many natural language tasks such as choosing the correct word in machine translation and coreference resolution'),\n",
       "  (0.0,\n",
       "   'word is the word related to w via the dependency relationship and posit ion can either be head or mod')],\n",
       " [(0.0,\n",
       "   'with its distant orbit percent farther from the sun than earth and slim atmospheric blanket mars experiences frigid weather conditions surface temperatures typically average about degrees celsius degrees fahrenheit at the equator and can dip to degrees c near the poles only the midday sun at tropical latitudes is warm enough to thaw ice on occasion but any liquid water formed in this way would evaporate almost instantly because of the low atmospheric pressure although the atmosphere holds a small amount of water and waterice clouds sometimes develop most martian weather involves blowing dust or carbon dioxide each win terfor example a blizzard of frozen carbon dioxide rages over one pole and a few meters of this dryice snow accumulate as previously frozen carbon dioxide evaporates from the opposite polar cap yet even on the summer pole where the sun remains in the sky all day long temperatures never warm enough to melt frozen waterm since parenthetical information is related only to the elementary unit that it belongs to we do not assign it an elementary textual unit status'),\n",
       "  (0.0,\n",
       "   'when we began this research no empirical data supported the extent to which this ambiguity characterizes natural language texts'),\n",
       "  (0.0,\n",
       "   'we used the valid clause boundaries assigned by judges as indicators of discourse usages of cue phrases and we determined manually the cue phrases that signalled a discourse relation')],\n",
       " [(0.0,\n",
       "   'with this approach the english sound k corresponds to one of t ka ki ku r ke or ko depending on its context'),\n",
       "  (0.0,\n",
       "   'while it is difficult to judge overall accuracysome of the phases are onomatopoetic and others are simply too hard even for good human translatorsit is easier to identify system weaknesses and most of these lie in the pw model'),\n",
       "  (0.0,\n",
       "   'when word separators are removed from the katakana phrases rendering the task exceedingly difficult for people the machines performance is unchanged')],\n",
       " [(0.0,\n",
       "   'while no direct indicators of positive or negative semantic orientation have been proposed we demonstrate that conjunctions between adjectives provide indirect information about orientation'),\n",
       "  (0.0,\n",
       "   'we were unable to reach a unique label out of context for several adjectives which we removed from consideration for example cheap is positive if it is used as a synonym of inexpensive but negative if it implies inferior quality'),\n",
       "  (0.0,\n",
       "   'we want to select the partition pmin that minimizes i subject to the additional constraint that for each adjective x in a cluster c where c is the complement of cluster c ie the other member of the partition')],\n",
       " [(0.0,\n",
       "   'without the ability to calculate performance over subdialogues it would be impossible to test the effect of the different presentation strategies independently of the different confirmation strategies'),\n",
       "  (0.0,\n",
       "   'whenever an attribute value in a dialogue ie data avm matches the value in its scenario key the number in the appropriate diagonal cell of the matrix boldface for clarity is incremented by'),\n",
       "  (0.0,\n",
       "   'when there is total agreement k ic is superior to other measures of success such as transaction success danieli and gerbino concept accuracy simpson and fraser and percent agreement gale church and yarowsky because n takes into account the inherent complexity of the task by correcting for chance expected agreement')],\n",
       " [(0.0,\n",
       "   'yet despite the disparity in initial segmentation scores the transformation sequences effect a significant error reduction in all cases which indicates that the transformation sequences are effectively able to compensate to some extent for weaknesses in the lexicon'),\n",
       "  (0.0,\n",
       "   'wu and fung demonstrate that segmentation accuracy is significantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested'),\n",
       "  (0.0,\n",
       "   'word segmentation can easily be cast as a transformationbased problem which requires an initial model a goal state into which we wish to transform the initial model the gold standard and a series of transformations to effect this improvement')],\n",
       " [(0.0,\n",
       "   'word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences'),\n",
       "  (0.0,\n",
       "   'with the exception of fung b previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts gale amp church kumano amp hirakawa fung a melamed'),\n",
       "  (0.0,\n",
       "   'where the onetoone assumption failed but a link type captured part of a correct translation it was judged incomplete whether incomplete links are correct or incorrect depends on the application')],\n",
       " [(0.0,\n",
       "   'we used the datasets created by rm for np learning their results are shown in table the fo difference is small yet they use a richer feature set which incorporates lexical information as well'),\n",
       "  (0.0,\n",
       "   'we use a simple constraint propagation algorithm that finds the best choice of nonoverlapping candidates in an input sentence ber of patterns and average length in the training data'),\n",
       "  (0.0,\n",
       "   'we thus compute for the set of all covers of a candidate c the each of these items gives an indication regarding the overall strength of the coverbased evidence for the candidate')],\n",
       " [(0.0,\n",
       "   'while the vilain provides intuitive results for coreference scoring it however does not work as well in the context of evaluating cross document coreference'),\n",
       "  (0.0,\n",
       "   'we note that this is simply one fewer than the number of elements in the partition that is ms ipsi'),\n",
       "  (0.0,\n",
       "   'we had mentioned earlier that the error of linking the the two large chains in the second response is more damaging than the error of linking one of the large chains with the smaller chain in the first response')],\n",
       " [(0.0,\n",
       "   'with the exception of the example sentence extraction component all the software modules are highly interactive and have substantial user interface requirements'),\n",
       "  (0.0,\n",
       "   'we are building a constituent type identifier which will semiautomatically assign grammatical function gf and phrase type pt attributes to these femarked constituents eliminating the need for annotators to mark these'),\n",
       "  (0.0,\n",
       "   'used in nlp the framenet database should make it possible for a system which finds a valencebearing lexical item in a text to know for each of its senses where its individual arguments are likely to be found')],\n",
       " [(0.0,\n",
       "   'whereas the transformationbased tagger enforces multiple constraints by having multiple rules fire the maximumentropy tagger can have all of these constraints play a role at setting the probability estimates for the models parameters'),\n",
       "  (0.0,\n",
       "   'when the tag distribution for a context has low entropy it is a very good predictor of the correct tag when the identical environment occurs in unseen data'),\n",
       "  (0.0,\n",
       "   'we used a version of examplebased learning to determine whether these tagger differences could be exploited to determine examplebased learning has also been applied succesfully in building a single part of speech tagger the tag of a word we use the previous word current word next word and the output of each tagger for the previous current and next word')],\n",
       " [(0.0,\n",
       "   'while we are investigating the latter in current work local repair heuristics have the advantage of keeping the training and bracketing algorithms both simple and fast'),\n",
       "  (0.0,\n",
       "   'while previous empirical methods for base np identification have been rather complex this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task'),\n",
       "  (0.0,\n",
       "   'while initial results are promising the full impact of such heuristics on overall performance can be determined only if they are systematically learned and tested using available training data')],\n",
       " [(0.0,\n",
       "   'we then scan all the derivations in the development set and for each occurrence of the elementary event ym xm in derivationwtk we accumulate the value w tk in the cmym xm counter to be used in the next iteration'),\n",
       "  (0.0,\n",
       "   'we achieved a reduction in testdata perplexity bringing an improvement over a deleted interpolation trigram model whose perplexity was on the same trainingtest data the reduction is statistically significant according to a sign test'),\n",
       "  (0.0,\n",
       "   'w t neednt be a constituent but for the parses where it is there is no restriction on which of its words is the headword or what is the nonterminal label that accompanies the headword')],\n",
       " [(0.0,\n",
       "   'z system starts with a basic corpus annotation each word is tagged with its most likely tag and then searches through a space of transformation rules in order to reduce the discrepancy between its current annotation and the correct one in our case rules were learned'),\n",
       "  (0.0,\n",
       "   'when used on test the pairwise voting strategy tagpair clearly outperforms the other voting strategies but does not yet approach the level where all tying majority votes are handled correctly'),\n",
       "  (0.0,\n",
       "   'when abstracting away from individual tags precision and recall are equal and measure how many tokens are tagged correctly in this case we also use the more generic term accuracy')],\n",
       " [(0.0,\n",
       "   'with simwn and simroget we transform wordnet and roget into the same format as the automatically constructed thesauri in the previous section'),\n",
       "  (0.0,\n",
       "   'while previous methods rely on indirect tasks or subjective judgments our method allows direct and objective comparison between automatically and manually constructed thesauri'),\n",
       "  (0.0,\n",
       "   'when w r or w is the wild card the frequency counts of all the dependency triples that matches the rest of the pattern are summed up')],\n",
       " [(0.0,\n",
       "   'while we will be given the candidate attachment sites during testing the training procedure assumes no a priori information about potential attachment sites'),\n",
       "  (0.0,\n",
       "   'while this form of attachment ambiguity is usually easy for people to resolve a computer requires detailed knowledge about words eg washed vs bought in order to successfully resolve such ambiguities and predict the correct interpretation'),\n",
       "  (0.0,\n",
       "   'while the tuple num to num is more frequent than rise to num the conditional probabilities prefer a v which is the choice that maximizes pr v n p a')],\n",
       " [(0.0,\n",
       "   'while we know that the head of the entire compound is carrier in order to properly evaluate the word in question we must determine which of the words following it is its head'),\n",
       "  (0.0,\n",
       "   'whereas the ramps algorithm produced just terms not already present in wordnet for the two categories combined our algorithm produced or over for every valid terms produced'),\n",
       "  (0.0,\n",
       "   'we will also present some experimental results from two corpora and discuss criteria for judging the quality of the output')],\n",
       " [(0.0,\n",
       "   'with respect to any two discourse entities x uttx pas x and y utty posy uttx and utty specifying the current utterance ui or the preceding utterance u i set up the following ordering constraints on elements in the slist table'),\n",
       "  (0.0,\n",
       "   'while tensed clauses are defined as utterances on their own untensed clauses are processed with the main clause so that the cflist of the main clause contains the elements of the untensed embedded clause'),\n",
       "  (0.0,\n",
       "   'when the pronoun her in lc is encountered friedman is the first element of the slist since friedman is unused and in the current utterance')],\n",
       " [(0.0,\n",
       "   'when grouped by average performance they fell into several coherent classes which corresponded to the extent to which the functions focused on the intersection of the supports regions of positive probability of the distributions'),\n",
       "  (0.0,\n",
       "   'we would not be able to tell whether the cause was an inherent deficiency in the l norm or just a poor choice of weight function perhaps li q r would have yielded better estimates'),\n",
       "  (0.0,\n",
       "   'we treat a value of as indicating extreme dissimilarity it is worth noting at this point that there are several wellknown measures from the nlp literature that we have omitted from our experiments')],\n",
       " [(0.0,\n",
       "   'ww states that w appears in the patterns ab as a whole while pp states that p appears as a part'),\n",
       "  (0.0,\n",
       "   'words that appear in only one of the two patterns are suspect but to use this rule we need sufficient counts on the good words to be sure we have a representative sample'),\n",
       "  (0.0,\n",
       "   'while invariance with respect to frequency is generally a good property such invariant metrics can lead to bad results when used with sparse data')],\n",
       " [(0.0,\n",
       "   'when compared to levin s toplevel verb classes we found an agreement of our classification with her class of verbs of changes of state except for the last three verbs in the list in fig'),\n",
       "  (0.0,\n",
       "   'we will sketch an understanding of the lexical representations induced by latentclass labeling in terms of the linguistic theories mentioned above aiming at an interpretation which combines computational learnability linguistic motivation and denotationalsemantic adequacy'),\n",
       "  (0.0,\n",
       "   'we use a statistical subcatinduction system which estimates probability distributions and corpus frequencies for pairs of a head and a subcat frame carroll and rooth')],\n",
       " [(0.0,\n",
       "   'wordnet has been an important research tool but it is insufficient for domainspecific text such as that encountered in the mucs message understanding conferences'),\n",
       "  (0.0,\n",
       "   'within those columns majority lists the opinion of the majority of judges and any indicates the hypernyms that were accepted by even one of the judges'),\n",
       "  (0.0,\n",
       "   'with the addition of some improvements we have identified we believe that these automatic methods can be used to construct truly useful hierarchies')],\n",
       " [(0.0,\n",
       "   'while their kappa results are very good for other tags the opinionstatement tagging was not very successful the distinction was very hard to make by labelers and accounted for a large proportion of our interlabeler error jurafsky et al'),\n",
       "  (0.0,\n",
       "   'when there is such evidence we propose using the latent class model to correct the disagreements this model posits an unobserved latent variable to explain the correlations among the judges observations'),\n",
       "  (0.0,\n",
       "   'when disagreement is symmetric the differences between the actual counts and the counts expected if the judges decisions were not correlated are symmetric that is snii for i j where i is the difference from independence')],\n",
       " [(0.0,\n",
       "   'when the frequency count is reasonably large eg greater than a bin zn we further assume that the estimations of pa pbia and pcia in are accurate'),\n",
       "  (0.0,\n",
       "   'we use the following condition to determine whether or not a collocation is compositional a collocation a is noncompositional if there does not exist another collocation such that a is obtained by substituting the head or the modifier in a with a similar word and b there is an overlap between the confidence interval of the mutual information values of a and'),\n",
       "  (0.0,\n",
       "   'we use is ft i to denote the frequency count of all the collocations that match the pattern h r m where h and m are either words or the wild card and r is either a dependency type or the wild card')],\n",
       " [(0.0,\n",
       "   'with respect to ease of answer key preparation pampr and autsent are clearly superior since they use the publisherprovided answer key'),\n",
       "  (0.0,\n",
       "   'with proper choice of test material it should be possible to challenge systems to successively higher levels of performance'),\n",
       "  (0.0,\n",
       "   'why questions are by far the hardest performance around because they require understanding of rhetorical structure and because answers tend to be whole clauses often occurring as standalone sentences rather than phrases embedded in a context that matches the query closely')],\n",
       " [(0.0,\n",
       "   'while examining the si extractions we found many similar nps for example the salvadoran government the guatemalan government and the us government the similarities indicate that some head nouns when premodified represent existential entities'),\n",
       "  (0.0,\n",
       "   'when we say that a definite np is existential we say this because it completely specifies a cognitive representation of the entity in the readers mind'),\n",
       "  (0.0,\n",
       "   'when we combined all three methods si ehp and do recall increased to without any corresponding loss of precision')],\n",
       " [(0.0,\n",
       "   'within the corpus each word was annotated with all of the pos tags that would be possible given its spelling using the output of a morphological analysis program and also with the single one of those tags that a statistical pos tagging program had predicted to be the correct tag hake and hladka'),\n",
       "  (0.0,\n",
       "   'when tested with the final version of the parser on the full development set those two strategies performed at the same level'),\n",
       "  (0.0,\n",
       "   'we write nonterminals as x x x is the nonterminal label and x is a w t pair where w is the associated headword and t as the pos tag')],\n",
       " [(0.0,\n",
       "   'with the resulting vector we now perform a similarity computation to all vectors in the cooccurrence matrix of the target language'),\n",
       "  (0.0,\n",
       "   'whereas for parallel texts in some studies up to of the word alignments have been shown to be correct the accuracy for nonparallel texts has been around up to now'),\n",
       "  (0.0,\n",
       "   'when comparing an english and a german cooccurrence matrix of corresponding words he found a high correlation between the cooccurrence patterns of the two matrices when the rows and columns of both matrices were in corresponding word order and a low correlation when the rows and columns were in random order')],\n",
       " [(0.0,\n",
       "   'yes by both judges this figure is recall is estimated as the number of pairs that should have been judged good by strand ie that recieved a yes from both judges that strand indeed marked good this figure is'),\n",
       "  (0.0,\n",
       "   'using the cases where the two human judgments agree as ground truth precision of the system is estimated at and recall at'),\n",
       "  (0.0,\n",
       "   'using figures from the babel survey of multilinguality on the web http fwww isoc org it is possible to estimate that as of june there were on the order of primarily nonenglish web servers ranging over languages')],\n",
       " [(0.0,\n",
       "   'with a small number of features the correctparses estimator typically scores better than the pseudolikelihood estimator on the correctparses evaluation metric but the pseudolikelihood estimator always scores better on the pseudolikelihood evaluation metric'),\n",
       "  (0.0,\n",
       "   'while it is possible to construct ubgs for which the number of possible parses is unmanageably high for many grammars it is quite manageable to enumerate the set of possible parses and thereby directly evaluate eo fwiy w yi'),\n",
       "  (0.0,\n",
       "   'we would have liked to have included features concerning specific lexical items to capture headtohead dependencies but we felt that our corpora were so small that the associated parameters could not be accurately estimated')],\n",
       " [(0.0,\n",
       "   'while we used circumstantials to illustrate the issues we also handled revision for a variety of other categories in the same manner'),\n",
       "  (0.0,\n",
       "   'while we have tuned the system to perform with minor errors on the manual set of themes we have created the missing article in the fourth sentence of the summary in figure is an example we need more robust input data from the theme construction component which is still under development to train the generator before beginning large scale testing'),\n",
       "  (0.0,\n",
       "   'while some researchers address this problem by selecting a subset of the repetitions carbonell and goldstein this approach is not always satisfactory')],\n",
       " [(0.0,\n",
       "   'work on the english corpus was supported under the nsfcri grant towards a comprehensive linguistic annotation of language and the nsfint project sustainable interoperability for language technology silt funded by the national science foundation'),\n",
       "  (0.0,\n",
       "   'with the task decomposition allowed by bat it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful'),\n",
       "  (0.0,\n",
       "   'which gets us in a position to present the list of task introduced by tempeval including some motivation as to why we feel that it is a good idea to split up temporal relation classification into sub tasks')],\n",
       " [(0.0,\n",
       "   'word senses are more beneficial than simple word forms for a variety of tasks including information retrieval machine translation and others pantel and lin'),\n",
       "  (0.0,\n",
       "   'when hsk is the solution is perfectly homogeneous because each cluster only contains data points that belong to a single class'),\n",
       "  (0.0,\n",
       "   'when hks is the solution is perfectly complete because all data points of a class belong to the same cluster')],\n",
       " [(0.0,\n",
       "   'wiki pages the task consists of automatically detecting and resolving differences in the information they provide in order to produce aligned mutually enriched versions of the two documents'),\n",
       "  (0.0,\n",
       "   'we report on the training and test data used for evaluation the process of their creation the participating systems teams runs the approaches adopted and the results achieved'),\n",
       "  (0.0,\n",
       "   'towards this objective a crucial requirement is to identify the information in one page that is either equivalent or novel more informative with respect to the content of the other')],\n",
       " [(0.0,\n",
       "   'we would like to thank inderjeet mani wlodek zadrozny rie kubota ando joyce chai and nanda kambhatla for their valuable feedback'),\n",
       "  (0.0,\n",
       "   'we would also like to thank carl sable minyen kan dave evans adam budzikowski and veronika horvath for their help with the evaluation'),\n",
       "  (0.0,\n",
       "   'we would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization')],\n",
       " [(0.0,\n",
       "   'we would like to be able to incorporate semantics for an arbitrarily large number of words and lsa quickly becomes impractical on large sets'),\n",
       "  (0.0,\n",
       "   'we introduce a semanticbased algorithm for learning morphology which only proposes affixes when the stem and stemplusaffix are sufficiently similar semantically'),\n",
       "  (0.0,\n",
       "   'we insert words into a trie figure and extract potential affixes by observing those places in the trie where branching occurs')],\n",
       " [(0.0,\n",
       "   'whereas earlier methods all share the same basic intuition ie that similar words occur in similar contexts i formalise this in a slightly different way each word defines a probability distribution over all contexts namely the probability of the context given the word'),\n",
       "  (0.0,\n",
       "   'we can then measure the similarity of words by the similarity of their context distributions using the kullbackleibler kl divergence as a distance function'),\n",
       "  (0.0,\n",
       "   'we can model the context distribution as being the product of independent distributions for each relative position in this case the kl divergence is the sum of the divergences for each independent distribution')],\n",
       " [(0.0,\n",
       "   'while the bbn model does not perform at the level of model of collins on wall street journal text it is also less languagedependent eschewing the distance metric which relied on specific features of the english treebank in favor of the bigrams on nonterminals model'),\n",
       "  (0.0,\n",
       "   'while results for the two languages are far from equal we believe that further tuning of the head rules and analysis of development test set errors will yield significant performance gains on chinese to close the gap'),\n",
       "  (0.0,\n",
       "   'while more investigation is required we suspect part of the difference may be due to the fact that currently the bbn model uses languagespecific rules to guess part of speech tags for unknown words')],\n",
       " [(0.0,\n",
       "   'within this classification the greatest hopes for tagging improvement appear to come from minimizing errors in the second and third classes of this classification'),\n",
       "  (0.0,\n",
       "   'while progress is slow because each new feature applies only to a limited range of cases nevertheless the improvement in accuracy as compared to previous results is noticeable particularly for the individual decisions on which we focused'),\n",
       "  (0.0,\n",
       "   'we will not discuss in detail the characteristics of the model or the parameter estimation procedure used improved iterative scaling')],\n",
       " [(0.0,\n",
       "   'with the input stimulusfsn the output is stimuli rather than the incorrect stimuluses that would follow from the application of the more general rule in'),\n",
       "  (0.0,\n",
       "   'when the flex rule matches the input addressiv for example the c function npvordform defined elsewhere in the generator is called to determine the word form corresponding to the input the function deletes the inflection type and pos label specifications and the delimiters removes the last character of the lemma and finally attaches the characters es the word form generated is thus addresses'),\n",
       "  (0.0,\n",
       "   'we present such a component a fast and robust morphological generator for english based on finitestate techniques that generates a word form given a specification of the lemma partofspeech and the type of inflection required')],\n",
       " [(0.0,\n",
       "   'while this result is satisfying further investigation reveals that deterioration in the quality of the labeled data accumulated by cotraining hinders further improvement'),\n",
       "  (0.0,\n",
       "   'while previous research summarized in section has investigated the theoretical basis of cotraining this study is motivated by practical concerns'),\n",
       "  (0.0,\n",
       "   'we seek to apply the cotraining paradigm to problems in natural language learning with the goal of reducing the amount of humanannotated data required for developing natural language processing components')],\n",
       " [(0.0,\n",
       "   'when a word maps to a general mesh term like treatment y zeros are appended to the end of the descriptor to stand in place of the missing values so for example treatment in model is y and in model is y etc'),\n",
       "  (0.0,\n",
       "   'we want to support relationships between entities that are shown to be important in cognitive linguistics in particular we intend to support the kinds of inferences that arise from talmys force dynamics talmy'),\n",
       "  (0.0,\n",
       "   'we used a feedforward network trained with conjugate gradient descent number corresponds to the level of the mesh hierarchy used for classification')],\n",
       " [(0.0,\n",
       "   'yet we can filter by pruning ngrams whose beginning or ending word is among the top n most frequent words'),\n",
       "  (0.0,\n",
       "   'yet there is still another maybe or the reciprocal of the words frequencies semantic compositionality is not always bad'),\n",
       "  (0.0,\n",
       "   'with permission and sufficient time one can repeatedly query websites that host large collections of mrds and evaluate each ngram')],\n",
       " [(0.0,\n",
       "   'xu and croft offered a trainable method call local context analysis lca which replaces each query term with frequently cooccurring words'),\n",
       "  (0.0,\n",
       "   'we discovered that lsa is a more accurate measure of similarity than the cosine metric stemming does not always improve segmentation accuracy and ranking is crucial to cosine but not lsa'),\n",
       "  (0.0,\n",
       "   'this procedure is useful in information retrieval hearst and plaunt hearst yaari reynar summarisation reynar text understanding anaphora resolution kozima language modelling morris and hirst beeferman et al and text navigation choi b')],\n",
       " [(0.0,\n",
       "   'while this has allowed for quantitative comparison of parsing techniques it has left open the question of how other types of text might affect parser performance and how portable parsing models are across corpora'),\n",
       "  (0.0,\n",
       "   'while this does not reflect the stateoftheart performance on the wsj task achieved by the more the complex models of charniak and collins we regard it as a reasonable baseline for the investigation of corpus effects on statistical parsing'),\n",
       "  (0.0,\n",
       "   'we examine these questions by comparing results for the brown and wsj corpora and also consider which parts of the parsers probability model are particularly tuned to the corpus on which it was trained')],\n",
       " [(0.0,\n",
       "   'when assigning timestamps we analyze both implicit time references mainly through the tense system and explicit ones temporal adverbials such as on monday in etc'),\n",
       "  (0.0,\n",
       "   'when assigning a time to an event we select the time to be either the most recently assigned date or if the value of the most recently assigned date is undefined to the date of the article'),\n",
       "  (0.0,\n",
       "   'we ran the timestamper program on two types of data list of eventclauses extracted by the event identifier and list of eventclauses created manually')],\n",
       " [(0.0,\n",
       "   'we used nltk as a basis for the assignments and student projects in cis an introductory computational linguistics class taught at the university of pennsylvania'),\n",
       "  (0.0,\n",
       "   'we hope that nltk will allow computational linguistics classes to include more handson experience with using and building nlp components and systems'),\n",
       "  (0.0,\n",
       "   'we describe nltk the natural language toolkit which we have developed in conjunction with a course we have taught at the university of pennsylvania')],\n",
       " [(0.0,\n",
       "   'while polynomial kernels in the svm learning can effectively generate feature conjunctions kernel functions that can effectively generate feature disjunctions are not known'),\n",
       "  (0.0,\n",
       "   'we used a feature set hw pre suf sub posipc and the inner product kernel the training time was measured on a machine with four mhz pentiumiiis and gb ram'),\n",
       "  (0.0,\n",
       "   'we use the maximum entropy tagging method described in kazama et al for the experiments which is a variant of ratnaparkhi modified to use hmm state features')],\n",
       " [(0.0,\n",
       "   'word induction from natural language text without word boundaries is also studied in deligne and bimbot hua where mdlbased model optimization measures are used'),\n",
       "  (0.0,\n",
       "   'whether this is due to the cost function or the splitting strategy cannot be deduced based on these experiments'),\n",
       "  (0.0,\n",
       "   'when the vocabulary is very large say words the basic problems in the estimation of the language model are if words are used as basic representational units in the language model the number of basic units is very high and the estimated word ngrams are poor due to sparse data')],\n",
       " [(0.0,\n",
       "   'with that goal open mind word expert tracks for each contributor the number of items tagged for each topic'),\n",
       "  (0.0,\n",
       "   'with open mind word expert we aim at creating a very large sense tagged corpus by making use of the incredible resource of knowledge constituted by the millions of web users combined with techniques for active learning'),\n",
       "  (0.0,\n",
       "   'while the first two sources are well known to the nlp community the open mind common sense constitutes a fairly new textual corpus')],\n",
       " [(0.0,\n",
       "   'with this computational means at hand we can now measure the spelling similarity between every german and english word and sort possible word pairs accordingly'),\n",
       "  (0.0,\n",
       "   'when words are adopted into another language their spelling might change slightly in a manner that can not be simply generalized in a rule'),\n",
       "  (0.0,\n",
       "   'we examined the german words in our lexicon and tried to find english words that have the exact same spelling')],\n",
       " [(0.0,\n",
       "   'with this gold standard in place it is possible to use precision and recall measures to evaluate the quality of the extracted thesaurus'),\n",
       "  (0.0,\n",
       "   'when these are used with weighted attributes if the weight is greater than zero then it is considered in the set'),\n",
       "  (0.0,\n",
       "   'we would like to thank stephen clark caroline sporleder tara murphy and the anonymous reviewers for their comments on drafts of this paper')],\n",
       " [(0.0,\n",
       "   'yet the results shown in line of figure are relatively poor the adjectives provide less useful information than unigram presence'),\n",
       "  (0.0,\n",
       "   'while the tie rates suggest that the brevity of the humanproduced lists is a factor in the relatively poor performance results it is not the case that size alone necessarily limits accuracy'),\n",
       "  (0.0,\n",
       "   'what accounts for these two differences difficulty and types of information proving useful between topic and sentiment classification and how might we improve the latter')],\n",
       " [(0.0,\n",
       "   'while metabootstrapping trusts individual extraction patterns to make unilateral decisions basilisk gathers collective evidence from a large set of extraction patterns'),\n",
       "  (0.0,\n",
       "   'we will use the abbreviation cat to indicate that only one semantic category was bootstrapped and mcat to indicate that multiple semantic categories were simultaneously bootstrapped'),\n",
       "  (0.0,\n",
       "   'we were surprised that the improvement for metabootstrapping was much we also measured the recall of basilisks lexicons after words had been learned based on the gold standard data shown in table')],\n",
       " [(0.0,\n",
       "   'yarowsky et al states that during their work on noun phrase bracketing they found a strong cohesion among noun phrases even when comparing english to czech a relatively free word order language'),\n",
       "  (0.0,\n",
       "   'while reflecting genuine ambiguity an smt system would likely pursue only one of the alternatives and only a portion of the crossings would come into play'),\n",
       "  (0.0,\n",
       "   'when two annotators disagree the union of the p alignments produced by each annotator is recorded as the p alignment in the corpus')],\n",
       " [(0.0,\n",
       "   'while grammar development is carried out in the lkb copestake processing both in the application domain and for the purposes of running test suites is done with the highly efficient pet parser callmeier'),\n",
       "  (0.0,\n",
       "   'when an argument is realized its sat value on the mother node is specified as sat and its synsem is unified with its val value on the subcategorizing head'),\n",
       "  (0.0,\n",
       "   'we model this within the sign by positing a feature empathy within context and linking it to the relevant arguments indices')],\n",
       " [(0.0,\n",
       "   'work is also progressing on establishing a standard relational database using postgresql for storing information for the lexical entries themselves improving both scalability and clarity compared to the current simple text file representation'),\n",
       "  (0.0,\n",
       "   'while the development of the matrix will be built largely on the lkb platform support will also be needed for using the emerging grammars on other processing platforms and for linking to other packages for preprocessing the linguistic input'),\n",
       "  (0.0,\n",
       "   'while the details of modifier placement which parts of speech can modify which kinds of phrases etc differ across languages we believe that all languages display a distinction between scopal and intersective modification')],\n",
       " [(0.0,\n",
       "   'within lfg fstructures are meant to encode a language universal level of analysis allowing for crosslinguistic parallelism at this level of abstraction'),\n",
       "  (0.0,\n",
       "   'when this is noticed via the featuretable comparison it is determined why one grammar needs the feature and the other does not and thus it may be possible to eliminate the feature in one grammar or to add it to another'),\n",
       "  (0.0,\n",
       "   'we report on the parallel grammar pargram project which uses the xle parser and grammar development platform for six languages english')],\n",
       " [(0.0,\n",
       "   'we used sentences from the articles on january st to january th as training examples and sentences from the articles on january th as the test data'),\n",
       "  (0.0,\n",
       "   'we propose a new method that is simple and efficient since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side'),\n",
       "  (0.0,\n",
       "   'we presented a new japanese dependency parser using a cascaded chunking model which achieves accuracy using the kyoto university corpus')],\n",
       " [(0.0,\n",
       "   'while the loglikelihood function for me models in is twice differentiable for large scale problems the evaluation of the hessian matrix is computationally impractical and newtons method is not competitive with iterative scaling or first order methods'),\n",
       "  (0.0,\n",
       "   'while parameter estimation for me models is conceptually straightforward in practice me models for typical natural language tasks are very large and may well contain many thousands of free parameters'),\n",
       "  (0.0,\n",
       "   'while parameter estimation for me models is conceptually straightforward in practice me models for typical natural language tasks are usually quite large and frequently contain hundreds of thousands of free parameters')],\n",
       " [(0.0,\n",
       "   'weaksubjective was used for three situations words that have weak subjective connotations such as aberration which implies something out of the ordinary but does not evoke a strong sense of judgement words that have multiple senses or uses where one is subjective but the other is not'),\n",
       "  (0.0,\n",
       "   'we used the nouns as feature sets rather than define a separate feature for each word so the classifier could generalize over the set to minimize sparse data problems'),\n",
       "  (0.0,\n",
       "   'we used a separate annotated tuning corpus of documents with a total of sentences to establish some experimental parameters')],\n",
       " [(0.0,\n",
       "   'with this type of agglomerative clustering the most similar pages are clustered first and outliers are assigned as stragglers at the top levels of the cluster tree'),\n",
       "  (0.0,\n",
       "   'while word senses and translation ambiguities may typically have alternative meanings that must be resolved through context a personal name such as jim clark may potentially refer to hundreds or thousands of distinct individuals'),\n",
       "  (0.0,\n",
       "   'while these names could be used in a undifferentiated vectorbased bagofwords model further accuracy can be gained by extracting specific types of association such as familial relationships eg son wife employment relationships eg manager of and nationality as distinct from simple term cooccurrence in a window')],\n",
       " [(0.0,\n",
       "   'we would like to thank the anonymous reviewers for their helpful comments and also iain rae for computer support'),\n",
       "  (0.0,\n",
       "   'we trained one of the taggers on much more labelled seed data than the other to see how this affects the cotraining process'),\n",
       "  (0.0,\n",
       "   'we leave these experiments to future work but note that there is a large computational cost associated with such experiments')],\n",
       " [(0.0,\n",
       "   'zhou and su used a wide variety of features which suggests that the relatively poor performance of the taggers used in conll was largely due to the feature sets used rather than the machine learning method'),\n",
       "  (0.0,\n",
       "   'we used three data sets the english and german data for the conll shared task tjong kim sang and de meulder and the dutch data for the conll shared task tjong kim sang'),\n",
       "  (0.0,\n",
       "   'we report reasonable precision and recall fscore for the conll english test data and an fscore of for the conll german test data')],\n",
       " [(0.0,\n",
       "   'within each of the regions a statistical bigram language model is used to compute the likelihood of words occurring within that region named entity type'),\n",
       "  (0.0,\n",
       "   'when no gazetteer or other additional training resources are used the combined system attains a performance of f on the english development data integrating name location and person gazetteers and named entity systems trained on additional more general data reduces the fmeasure error by a factor of to on the english data'),\n",
       "  (0.0,\n",
       "   'we note that the numbers are roughly twice as large for the development data in german as they are for english')],\n",
       " [(0.0,\n",
       "   'when using characterlevel models for wordevaluated tasks one would not want multiple characters inside a single word to receive different labels'),\n",
       "  (0.0,\n",
       "   'we present two models in which the basic units are characters and character grams instead of words and word phrases'),\n",
       "  (0.0,\n",
       "   'we found knowing that the previous word was an other wasnt particularly useful without also knowing its partofspeech eg a preceding preposition might indicate a location')],\n",
       " [(0.0,\n",
       "   'we used two variants of hmm hedge one which selects headline words from the first words of the story and one which selects words from the first sentence of the story'),\n",
       "  (0.0,\n",
       "   'we treat summarization as a type of translation from a verbose language to a concise one and compare automatically generated headlines to human generated headlines'),\n",
       "  (0.0,\n",
       "   'we should be able to quickly produce a comparable system for other languages especially in light of current multilingual initiatives that include automatic parser induction for new languages eg the tides initiative')],\n",
       " [(0.0,\n",
       "   'we will discuss tags and an important principle guiding their formation the extraction procedure from the ptb that is described in chen including extensions to extract a tag from the propbank and finally the extraction of deeper linguistic features from the resulting tag'),\n",
       "  (0.0,\n",
       "   'we use deep linguistic features to predict semantic roles on syntactic arguments and show that these perform considerably better than surfaceoriented features'),\n",
       "  (0.0,\n",
       "   'we suggest that the syntactic representation chosen in the ptb is less well suited for semantic processing than the other deeper syntactic representations')],\n",
       " [(0.0,\n",
       "   'we would expect a higher performance for the ccgbased system if the analyses in ccgbank resembled more closely those in propbank'),\n",
       "  (0.0,\n",
       "   'we speculate that much of the performance improvement we show could be obtained with traditional ie nonccgbased parsers if they were designed to recover more of the information present in the penn treebank in particular the trace coindexation'),\n",
       "  (0.0,\n",
       "   'we present a system for automatically identifying propbankstyle semantic roles based on the output of a statistical parser for combinatory categorial grammar')],\n",
       " [(0.0,\n",
       "   'we would like to thank john carroll for the use of his parser adam kilgarriff and bill keller for valuable discussions and the uk epsrc for its studentship to the first author'),\n",
       "  (0.0,\n",
       "   'we will refer to these verbs as the features of n f n where dnv is the degree of association between noun n and verb v possible association functions will be defined in the context of each model described below'),\n",
       "  (0.0,\n",
       "   'we will now consider some different possibilities for measuring the degree of association between a noun n and a verb v in the combinatorial model we simply consider whether a verb has ever been seen to cooccur with the noun')],\n",
       " [(0.0,\n",
       "   'when those sentences are removed the average pairwise percentage agreement increases to and the average pairwise r value increases to'),\n",
       "  (0.0,\n",
       "   'western countries were left frustrated and impotent after robert mugabe formally declared that he had overwhelmingly won zimbabwes presidential election annotators are also asked to judge the strength of each private state'),\n",
       "  (0.0,\n",
       "   'we have developed a bootstrapping process for subjectivity classification that explores three ideas highprecision classifiers can be used to automatically identify subjective and objective sentences from unannotated texts this data can be used as a training set to automatically learn extraction patterns associated with subjectivity and the learned patterns can be used to grow the training set allowing this entire process to be bootstrapped')],\n",
       " [(0.0,\n",
       "   'wiebe et al report that this expectation is borne out of the time for opinion documents and of the time for factual documents'),\n",
       "  (0.0,\n",
       "   'while words and ngrams had little performance effect for the opinion class they increased the recall for the fact class around five fold compared to the approach by wiebe et al'),\n",
       "  (0.0,\n",
       "   'while we have found presenting information organized in separate opinion and fact classes useful our goal is to introduce further analysis of each sentence so that opinion sentences can be linked to particular perspectives on a specific subject')],\n",
       " [(0.0,\n",
       "   'when pos patterns are used to extract potential terms the problem lies in how to restrict the number of terms and only keep the ones that are relevant'),\n",
       "  (0.0,\n",
       "   'when inspecting manually assigned keywords the vast majority turn out to be nouns or noun phrases with adjectives and as discussed in section the research on term extraction focuses on noun patterns'),\n",
       "  (0.0,\n",
       "   'when extracting the terms from the test set according to the ngram approach the data consisted of negative examples and positive examples thus in total examples were classified by the trained model')],\n",
       " [(0.0,\n",
       "   'while we are familiar with the approaches taken in several of the tested systems we leave it up to the individual participants to describe their approaches and hopefully elucidate which aspects of their approaches are most responsible for their successes and failures the participants papers all appear in this volume'),\n",
       "  (0.0,\n",
       "   'while the test sets turned out to be large enough to measure significant differences between systems in most cases a larger test set would allow even better statistics'),\n",
       "  (0.0,\n",
       "   'while it would be fairly straightforward in many cases to automatically score both alternatives we felt we could provide a more objective measure if we went strictly by the particular segmentation standard being tested on and simply did not get into the business of deciding upon allowable alternatives')],\n",
       " [(0.0,\n",
       "   'we have taken six tracks academia sinica closed asc u penn chinese tree bank open and closedctboc hong kong cityu closed hkc peking university open and closedpkoc'),\n",
       "  (0.0,\n",
       "   'we apply to word segmentation classbased hmm which is a generalized approach covering both common words and unknown words wi iff wi is listed in the segmentation lexicon per iff wi is unlisted personal name loc iff wi is unlisted location name org iff wi is unlisted organization name time iff wi is unlisted time expression num iff wi is unlisted numeric expression str iffwi is unlisted symbol string beg iff beginning of a sentence end iff ending of a sentence other otherwise'),\n",
       "  (0.0,\n",
       "   'we also thank richard sproat qing ma fei xia and other sighan colleagues for their elaborate organization and enthusiastic help in the first international chinese word segmentation bakeoff')],\n",
       " [(0.0,\n",
       "   'while these results may appear at first glance to be less than conclusive we must bear in mind that we are working with limited amounts of data and relatively simplistic models of a cognitively intensive task'),\n",
       "  (0.0,\n",
       "   'while the results are far from stable such variation is perhaps to be expected on a test like this since the nature of context space models means that rogue items sometimes get extremely high similarity scores and we are performing the regression over only vpcs vpccomponent pairs'),\n",
       "  (0.0,\n",
       "   'while method softens the reliance upon productivity as a test for compositionality it still confuses institutionalisation with noncompositionality somewhat in its reliance upon substitution')],\n",
       " [(0.0,\n",
       "   'whilst statistics are useful indicators of noncompositionality there are compositional multiwords which have low values for these statistics yet are highly noncompositional'),\n",
       "  (0.0,\n",
       "   'whilst it is possible to put every single occurrence of a verb and particle combination into a lexicon this is not desirable'),\n",
       "  (0.0,\n",
       "   'whether such an unexpectedly high cooccurrence frequency warrants an entry in the lexicon depends on the type of lexicon being built')],\n",
       " [(0.0,\n",
       "   'with simple decomposable mwes we can expect the constituents and particularly the head to be hypernyms ancestor nodes or synonyms of the mwe'),\n",
       "  (0.0,\n",
       "   'with nondecomposable mwes eg kick the bucket shoot the breeze hot dog no decompositional analysis is possible and the mwe is semantically impenetrable'),\n",
       "  (0.0,\n",
       "   'while taking note of the low correlation with wordnet similarities therefore we move straight on to look at the hyponymy test')],\n",
       " [(0.0,\n",
       "   'we will try to make this more precise in a minute but first we want to discuss the relation between incrementality and determinism'),\n",
       "  (0.0,\n",
       "   'we will represent parser configurations by triples i a where is the stack represented as a list i is the list of remaining input tokens and a is the current arc relation for the dependency graph'),\n",
       "  (0.0,\n",
       "   'we will formalize deterministic dependency parsing in a way which is inspired by traditional shiftreduce parsing for contextfree grammars using a buffer of input tokens and a stack for storing previously processed input')],\n",
       " [(0.0,\n",
       "   'with an average of five senses per word the average value for the agreement by chance is measured at resulting in a micro statistic of'),\n",
       "  (0.0,\n",
       "   'we measure two figures microaverage where number of senses agreement by chance and are determined as an average for all words in the set and macroaverage where intertagger agreement agreement by chance and are individually determined for each of the words in the set and then combined in an overall average'),\n",
       "  (0.0,\n",
       "   'we describe in this paper the task definition resources participating systems and comparative results for the english lexical sample task which was organized as part of the senseval evaluation exercise')],\n",
       " [(0.0,\n",
       "   'with the duc data we computed pearsons product moment correlation coefficients spearmans rank order correlation coefficients and kendalls correlation coefficients between systems average rouge scores and their human assigned average coverage scores using single reference and multiple references'),\n",
       "  (0.0,\n",
       "   'when multiple references are used we compute pairwise summarylevel rougen between a candidate summary s and every reference ri in the reference set'),\n",
       "  (0.0,\n",
       "   'when applying to summarylevel we take the union lcs matches between a reference summary sentence ri and every candidate summary sentence cj')],\n",
       " [(0.0,\n",
       "   'we thank chuck wooters don baron chris oei and andreas stolcke for software assistance ashley krupski for contributions to the annotation scheme andrei popescubelis for analysis and comments on a release of the meetings and barbara peskin and jane edwards for general advice and feedback'),\n",
       "  (0.0,\n",
       "   'we suggest various ways to group the large set of labels into a smaller set of classes depending on the research focus'),\n",
       "  (0.0,\n",
       "   'we provide a brief summary of the annotation system and labeling procedure interannotator reliability statistics overall distributional statistics a description of auxiliary files distributed with the corpus and information on how to obtain the data')],\n",
       " [(0.0,\n",
       "   'while snow can be used as a classifier and predicts using a winnertakeall mechanism over the activation value of the target classes we can also rely directly on the raw activation value it outputs which is the weighted linear sum of the active features to estimate the posteriors'),\n",
       "  (0.0,\n",
       "   'while many statistical methods make stupid mistakes ie inconsistency among predictions that no human ever makes as we show our approach improves also the quality of the inference significantly'),\n",
       "  (0.0,\n",
       "   'when the relation classifier is trained using the true entity labels the performance is much worse than using the predicted entity labels')],\n",
       " [(0.0,\n",
       "   'with smaller quantities of data there is less possibility of finding instances that use exactly the same set of words'),\n",
       "  (0.0,\n",
       "   'while there has been some previous work in sense discrimination eg schutze pedersen and bruce pedersen and bruce schutze fukumoto and suzuki by comparison it is much less than that devoted to word sense disambiguation which is the process of assigning a meaning to a word from a predefined set of possibilities'),\n",
       "  (0.0,\n",
       "   'when we cluster test instances we specify an upper limit on the number of clusters that can be discovered')],\n",
       " [(0.0,\n",
       "   'wn is a labeled directed graph d wa where a w is the set of nodes ie word tokens in the input string b a is a set of labeled arcs wi r wj where wi wj w and r r'),\n",
       "  (0.0,\n",
       "   'wj to say that there is an arc from wi to wj labeled r and wi wj to say that there is an arc from wi to wj regardless of the label we use'),\n",
       "  (0.0,\n",
       "   'while the sen tence data for validation consists of sentences the corresponding transition data contains instancesfor training only transition data is relevant and the train ing data set contains instances')],\n",
       " [(0.0,\n",
       "   'while previous programs with similar goals gildea and jurafsky were statisticsbased this tool will be based completely on handcoded rules and lexical resources'),\n",
       "  (0.0,\n",
       "   'whether or not one adapts an a la carte approach nombank and propbank projects provide users with data to recognize regularizations of lexically and syntactically related sentence structures'),\n",
       "  (0.0,\n",
       "   'when complete nombank will provide argument structure for instances of about common nouns in the penn treebank ii corpus')],\n",
       " [(0.0,\n",
       "   'we wanted to explore how well the annotators agreed on relatively abstract classifications such as requires extrapolation from actual findings and thus we refrained from writing instructions such as if the sentence contains a form of suggest then mark it as speculative into the guidelines'),\n",
       "  (0.0,\n",
       "   'we report here the isolation ofhuman zinc finger hzf a putative zincfinger transcription factor by motifdirected differential display of mrna extracted from histaminestimulated human vein endothelial cells'),\n",
       "  (0.0,\n",
       "   'we may use it to test a kr systems ability to predict b as the connecting aspect between a and c and to do this using data prior to the publication')],\n",
       " [(0.0,\n",
       "   'work over the last few years in literature data mining for biology has progressed from linguistically unsophisticated models to the adaptation of natural language processing nlp techniques that use full parsers park et al yakushiji et al and coreference to extract relations that span multiple sentences pustejovsky et al hahn et al for an overview see hirschman et al'),\n",
       "  (0.0,\n",
       "   'while this also has conceptual benefits for the annotation guidelines it has the fortunate effect of making such otherwise syntaxunfriendly malignancies as colorectal adenomas containing early cancer and acute myelomonocytic leukemia in remission amenable for mapping the component parts to syntactic nodes'),\n",
       "  (0.0,\n",
       "   'while sufficiently complex patterns can deal with these issues it requires a good amount of time and effort to build such handcrafted rules particularly since such rules are developed for each specific problem')],\n",
       " [(0.0,\n",
       "   'while like most discriminative models it is computeintensive to train it allows fast parsing remaining cubic despite the incorporation of lexical features'),\n",
       "  (0.0,\n",
       "   'we used two such auxiliary classifiers giving a prediction feature for each span these classifiers predicted only the presence or absence of a bracket over that span not bracket labels'),\n",
       "  (0.0,\n",
       "   'we used the constituent loss in our experiments marginals as qm where is the vector with components iri and qm is defined as where lir lxi yi r ir xi r and iir ixi yi r')],\n",
       " [(0.0,\n",
       "   'y or at least x yed or at least xed not only xed but yed not just xed but yed the probabilities in the denominator are difficult to calculate directly from search engine results'),\n",
       "  (0.0,\n",
       "   'wordnets cause relation between a causative and a resultative verb as in buy own would be tagged as instances of happensbefore by our system'),\n",
       "  (0.0,\n",
       "   'wordnet does not include the relation x buys y happensbefore x sells y since it is possible to sell something without having bought it eg having manufactured or stolen it')],\n",
       " [(0.0,\n",
       "   'yet this method also suffers from certain limitations a it identifies only templates with prespecified structures b accuracy seems more limited due to the weaker notion of similarity and c coverage is limited to the scope of an available corpus'),\n",
       "  (0.0,\n",
       "   'while in this paper we considered anchor sets to have equal weights we are also carrying out experiments with weights based on crosscorrelation between anchor sets'),\n",
       "  (0.0,\n",
       "   'web searching is then used to find occurrences of the input anchor set resulting in new templates that are supposed to specify the same relation as the original one born in')],\n",
       " [(0.0,\n",
       "   'yamada and knight introduced treetostring alignment on japanese data and gildea performed treetotree alignment on the korean treebank allowing for nonisomorphic structures he applied this to wordtoword alignment'),\n",
       "  (0.0,\n",
       "   'xia et al compare the rule templates of lexicalized tree adjoining grammars extracted from treebanks in english chinese and korean'),\n",
       "  (0.0,\n",
       "   'wu and alshawi et al used unsupervised learning on parallel text to induce syntactic analysis that was useful for their respective applications in phrasal translation extraction and speech translation though not necessarily similar to what a human annotator would select')],\n",
       " [(0.0,\n",
       "   'we would like to thank scott cotton for providing the propbank api which greatly simplifies the implementation of our system'),\n",
       "  (0.0,\n",
       "   'we take a critical look at the previously used features against each subtask and propose a new set of features in section'),\n",
       "  (0.0,\n",
       "   'we propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed')],\n",
       " [(0.0,\n",
       "   'while framenet uses semantic roles specific to a particular situation such as speaker message admire adore appreciate cherish enjoy addressee and propbank uses roles specific to a verb such as arg arg arg verbnet uses an intermediate level of thematic roles such as agent theme recipient'),\n",
       "  (0.0,\n",
       "   'when using general thematic roles with a small set of verb classes the probability used for the baseline works very well for subjects and objects which are primarily agents and themes respectively for our verbs'),\n",
       "  (0.0,\n",
       "   'when backing off from this probability we use statistics over more general classes of information such as conditioning over the semantic class of the verb instead of the verb itselffor this example psychological state verbs')],\n",
       " [(0.0,\n",
       "   'while translationbased approaches to obtaining data do address the problem of how to identify two strings as meaning the same thing they are limited in scalability owing to the difficulty and expense of obtaining large quantities of multiplytranslated source documents'),\n",
       "  (0.0,\n",
       "   'while this means certain common structural alternations eg activepassive cannot be generated we are still able to express a broad range of phenomena pings to be both unwieldy in practice and very often indicative of poor a word alignment'),\n",
       "  (0.0,\n",
       "   'while the alternations our system produces are currently limited in character the field of smt offers a host of possible enhancementsincluding reordering modelsaffording a natural path for future improvements')],\n",
       " [(0.0,\n",
       "   'wordlevel templates are employed when the words are lexicalized ie those that belong to particle auxiliary verb or suffix'),\n",
       "  (0.0,\n",
       "   'while the relative rates of lerror and serror are almost the same in hmms and crfs the number of lerrors with memms amounts to which is of total errors and is even larger than that of naive hmms'),\n",
       "  (0.0,\n",
       "   'we would like to thank kiyotaka uchimoto and masayuki asahara who explained the details of their japanese morphological analyzers')],\n",
       " [(0.0,\n",
       "   'zhou and su investigated an approach to build a chinese analyzer that integrated word segmentation pos tagging and parsing based on a hidden markov model'),\n",
       "  (0.0,\n",
       "   'with an allatonce characterbased approach an average word segmentation fmeasure of and an average pos tagging accuracy of was achieved'),\n",
       "  (0.0,\n",
       "   'with a oneatatime characterbased pos tagger the average pos tagging accuracy improved to higher than that achieved by the oneatatime wordbased pos tagger')],\n",
       " [(0.0,\n",
       "   'when evaluating on the mismatched outofdomain test data the gram baseline is outperformed by the improvement brought by the adaptation technique using a very small amount of matched bn data kwds is about relative'),\n",
       "  (0.0,\n",
       "   'we used a simple count cutoff feature selection algorithm which counts the number of occurrences of all features in a predefined set after which it discards the features whose count is less than a prespecified threshold'),\n",
       "  (0.0,\n",
       "   'we study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to datadriven approaches since vast amounts of training data are easily obtainable by simply wiping the case information in text')],\n",
       " [(0.0,\n",
       "   'when we explicitly enumerate the subtrees used in tree kernel the number of active nonzero features might amount to ten thousand or more'),\n",
       "  (0.0,\n",
       "   'when a convolution kernel is applied to sparse data kernel dot products between almost the same instances become much larger than those between different instances'),\n",
       "  (0.0,\n",
       "   'we would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications eg parse reranking collins and duffy and information extraction')],\n",
       " [(0.0,\n",
       "   'when computing degree centrality we have treated each edge as a vote to determine the overall prestige value of each node'),\n",
       "  (0.0,\n",
       "   'we use pagerank to weight each vote so that a vote that comes from a more prestigious sentence has a greater value in the centrality of a sentence'),\n",
       "  (0.0,\n",
       "   'we show three of the rouge metrics in our experiment results rouge unigrambased rouge bigrambased and rougew based on longest common subsequence weighted by the length')],\n",
       " [(0.0,\n",
       "   'while the statistical significance level is the most commonly used for historical reasons we want to validate as well the accuracy of the bootstrap resampling method at different statistical significance levels'),\n",
       "  (0.0,\n",
       "   'while bleu has become the most popular metric for machine translation evaluation some of its shortcomings have become apparent it does not work on single sentences since gram precision is often'),\n",
       "  (0.0,\n",
       "   'when we collect only sentences in sequence certain therefore better test sets of sentences may be constructed by sampling these sentences from different parts of the corpus')],\n",
       " [(0.0,\n",
       "   'while the size of the abstracts ranges from to words with an average size of words we have deliberately selected a very small abstract for the purpose of illustration'),\n",
       "  (0.0,\n",
       "   'while the final vertex scores and therefore rankings differ significantly as compared to their unweighted alternatives the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs'),\n",
       "  (0.0,\n",
       "   'we investigate and evaluate the application of textrank to two language processing tasks consisting of unsupervised keyword and sentence extraction and show that the results obtained with textrank are competitive with stateoftheart systems developed in these areas')],\n",
       " [(0.0,\n",
       "   'with these entities tagged a number of classes of features may be extracted representing various relationships between topic entities and value phrases similar to those described in section'),\n",
       "  (0.0,\n",
       "   'we describe the methods used to assign values to selected words and phrases and we introduce a method of bringing them together to create a model for the classification of texts'),\n",
       "  (0.0,\n",
       "   'topicsentence pot the average pot value of all adjectives which share a sentence with the topic of the text')],\n",
       " [(0.0,\n",
       "   'with the progress of mt research in recent years we are not satisfied with the getting correct words in the translations we also expect them to be wellformed and more readable'),\n",
       "  (0.0,\n",
       "   'while the subtree overlap metric defined above considers only subtrees of a fixed depth subtrees of other configurations may be important for discriminating good hypotheses'),\n",
       "  (0.0,\n",
       "   'while the machine learning approach improves correlation with human judgments all the metrics discussed are based on the same type of information ngram subsequences of the hypothesis translations')],\n",
       " [(0.0,\n",
       "   'while word frequency does not always constitute a good measure of word importance the distribution of words across an entire collection can be a good indicator of the specificity of the words'),\n",
       "  (0.0,\n",
       "   'while there is a large body of previous work focused on finding the semantic similarity of concepts and words the application of these wordoriented methods to text similarity has not been yet explored'),\n",
       "  (0.0,\n",
       "   'while there are several methods previously proposed for finding the semantic similarity of words to our knowledge the application of these wordoriented methods to text similarity has not been yet explored')],\n",
       " [(0.0,\n",
       "   'without this line algorithm could be considered as a generalization of the jimenez andmarzal algorithm to the case of acyclic monotonic hy pergraphs'),\n",
       "  (0.0,\n",
       "   'with the derivations thus ranked we can introduce anonrecursive representation for derivations that is analogous to the use of backpointers in parser implementa tion'),\n",
       "  (0.0,\n",
       "   'while charniak and johnson propose using an algorithm similar to our algorithm but with multiple passes to improve efficiency')],\n",
       " [(0.0,\n",
       "   'yamada and matsumoto experienced a reduction of slightly more than in dependency accuracy due to training set splitting and we expect that a similar loss is incurred here'),\n",
       "  (0.0,\n",
       "   'wong and wus parser is further differentiated from the other parsers mentioned here in that it does not use lexical items working only from partofspeech tags'),\n",
       "  (0.0,\n",
       "   'while yamada and matsumoto use a quadratic runtime algorithm with multiple passes over the input string nivre and scholz use a simplified version of the algorithm described here which handles only labeled or unlabeled dependency structures')],\n",
       " [(0.0,\n",
       "   'words in the positive class carry positive valence whereas these were randomly selected from english verbs and english adjectives those in negative class carry negative valence'),\n",
       "  (0.0,\n",
       "   'when we mark all sentences as opinionbearing it achieved and of accuracy for the annotation result of human and human respectively'),\n",
       "  (0.0,\n",
       "   'we used the charniak parser to get a phrase type feature of a frame element and the parse tree path feature')],\n",
       " [(0.0,\n",
       "   'with this scaling procedure into place we produce nonunique nbest lists for all sentences in our development corpus using all spmt submodels'),\n",
       "  (0.0,\n",
       "   'when using a phrasebased translation model one can easily extract the phrase pair the mutual the mutual and use it during the phrasebased model estimation phrase and in decoding'),\n",
       "  (0.0,\n",
       "   'when one analyzes the lexicalized xrs rules in this manner it is easy to associate with them any of the submodel probability distributions that have been proven useful in statistical phrasebased mt')],\n",
       " [(0.0,\n",
       "   'zens and ney describe a noisyor combination where sj is the probability that sj is not in the translation of t and psjti is a lexical probability'),\n",
       "  (0.0,\n",
       "   'we tested different phrasetable smoothing techniques in two different translation settings european language pairs with relatively small corpora and chinese to english translation with large corpora'),\n",
       "  (0.0,\n",
       "   'we show that any type of smoothing is a better idea than the relativefrequency estimates that are often used')],\n",
       " [(0.0,\n",
       "   'with source domain sentences and target domain sentences using supervised tagger features gives no improvement over using no source features'),\n",
       "  (0.0,\n",
       "   'with source domain sentences and target domain sentences using scl tagger features gives a relative reduction in error over using supervised tagger features and a relative reduction in error over using no source features'),\n",
       "  (0.0,\n",
       "   'with one hundred sentences of training data structural correspondence learning gives a relative reduction in error over the supervised baseline and it consistently outperforms both baseline models')],\n",
       " [(0.0,\n",
       "   'while we have presented significant improvements using additional constraints one may woneven when caching feature extraction during training mcdonald et al a still takes approximately minutes to train der whether the improvements are large enough to justify further research in this direction especially since mcdonald and pereira present an approximate algorithm which also makes more global decisions'),\n",
       "  (0.0,\n",
       "   'while we expect a longer runtime than using the chuliuedmonds as in previous work mcdonald et al b we are interested in how large the increase is'),\n",
       "  (0.0,\n",
       "   'while this is not crucial during decoding it does make discriminative online training difficult as training requires several iterations of parsing the whole corpus time st for the cross dataset using varying q values and the chuliuedmonds algorithm cle thus we investigate if it is possible to speed up our inference using a simple approximation')],\n",
       " [(0.0,\n",
       "   'while the developmentset results would induce us to utilize the standard threshold value of which is suboptimal on the test set the bagr agreementlink policy still achieves noticeable improvement over not using agreement links test set vs'),\n",
       "  (0.0,\n",
       "   'while such functionality is well beyond the scope of our current study we are optimistic that we can develop methods to exploit additional types of relationships in future work'),\n",
       "  (0.0,\n",
       "   'when we impose the constraint that all speech segments uttered by the same speaker receive the same label via samespeaker links both testset and tion accuracy in percent')],\n",
       " [(0.0,\n",
       "   'wilson et al proposed supervised learning dividing the resources into prior polarity and context polarity which are similar to polar atoms and syntactic patterns in this paper respectively'),\n",
       "  (0.0,\n",
       "   'wilson et al prepared prior polarities from existing resources and learned the context polarities by using prior polarities and annotated corpora'),\n",
       "  (0.0,\n",
       "   'when the window size is oo implying anywhere within a discourse the ratio is larger than the baseline by only and thus these types of coherency are not reliable even though the number of clues is relatively large')],\n",
       " [(0.0,\n",
       "   'withoutprogress on the joint extraction of opinion entities and their relations the capabilities of opinion based applications will remain limited'),\n",
       "  (0.0,\n",
       "   'while good performance in entity or relation extraction can contribute to better performance ofthe final system this is not always the case'),\n",
       "  (0.0,\n",
       "   'when a syntactic frame is matched to a sen tence the bracketed items should be instantiatedwith particular values corresponding to the sen tence')],\n",
       " [(0.0,\n",
       "   'wordnet fellbaum is a broadcoverage machinereadable dictionary which includes verbs mapped to word senses called synsets and common and proper nouns mapped to synsets'),\n",
       "  (0.0,\n",
       "   'word sense disambiguation at this level of granularity is a complex task which resisted all attempts of robust broadcoverage solutions'),\n",
       "  (0.0,\n",
       "   'with respect to ner wsd lies at the other end of the semantic tagging spectrum since the dictionary defines tens of thousand of very specific word senses including ner categories')],\n",
       " [(0.0,\n",
       "   'words that are very general in nature and that appear all over the place add noise to the vectors'),\n",
       "  (0.0,\n",
       "   'word vector gloss vector banerjee and pedersen encounter a similar issue when measuring semantic relatedness by counting the number of matching words between the glosses of two different concepts'),\n",
       "  (0.0,\n",
       "   'word sense disambiguation is the task of determining the meaning from multiple possibilities of a word in its given context')],\n",
       " [(0.0,\n",
       "   'words in the document are then sampled from a multinomial distribution where ln is the length of the document'),\n",
       "  (0.0,\n",
       "   'without the assistance of computers analysts have no choice but to read each document in order to identify those from a perspective of interest which is extremely timeconsuming'),\n",
       "  (0.0,\n",
       "   'while one might solicit these contrasting word pairs from domain experts our results show that statistical models such as svm and naive bayes can automatically acquire them')],\n",
       " [(0.0,\n",
       "   'within the third approach head and deprel can be predicted simultaneously or in two separate steps potentially using two different learners'),\n",
       "  (0.0,\n",
       "   'within the first approach each dependency can be labeled independently corstonoliver and aue or a news dialogue novel type of annotation d dependency c constituents dc discontinuous constituents f with functions t with types'),\n",
       "  (0.0,\n",
       "   'while the training data contained all columns although sometimes only with dummy values ie underscores the test data given to participants contained only the first')],\n",
       " [(0.0,\n",
       "   'with the availability of resources such as the penn wsj treebank much of the focus in the parsing community had been on producing syntactic representations based on phrasestructure'),\n",
       "  (0.0,\n",
       "   'we use the mira online learner to set the weights crammer and singer mcdonald et al a since we found it trained quickly and provide good performance'),\n",
       "  (0.0,\n",
       "   'we then add to the representation of the edge mi as head features mj as dependent features and also each conjunction of a feature from both sets')],\n",
       " [(0.0,\n",
       "   'we use svm classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion'),\n",
       "  (0.0,\n",
       "   'we projectivize training data by a minimal transformation lifting nonprojective arcs one step at a time and extending the arc label of lifted arcs using the encoding scheme called head by nivre and nilsson which means that a lifted arc is assigned the label rth where r is the original label and h is the label of the original head in the nonprojective dependency graph'),\n",
       "  (0.0,\n",
       "   'we have the best reported score for japanese swedish and turkish and the score for arabic danish dutch portuguese spanish and overall does not differ significantly from the best one')],\n",
       " [(0.0,\n",
       "   'while this determinism of em may be desirable in some circumstances we found that the ambiguity in h is often preferable at decoding time'),\n",
       "  (0.0,\n",
       "   'while the model and training regimen for oem differ from the model frommarcu and wong we achieved tion maximization algorithm for training oem was initialized with the heuristic parameters oh so the heuristic curve can be equivalently labeled as iteration'),\n",
       "  (0.0,\n",
       "   'while such examples of improvement are encouraging the trend of spurious determinism overwhelms this benefit by introducing errors in four related ways each of which will be explored in turn')],\n",
       " [(0.0,\n",
       "   'we use the source position j which is aligned to the last word of the target phrase in target position i'),\n",
       "  (0.0,\n",
       "   'we use a stateoftheart phrasebased translation system zens and ney zens et al including the following models an ngram language model a phrase translation model and a wordbased lexicon model'),\n",
       "  (0.0,\n",
       "   'we use a stateoftheart baseline system which would have obtained a good rank in the last nist evaluation nist')],\n",
       " [(0.0,\n",
       "   'while yamada and knight represent syntactical information in the decoding process through a series of transformation operations we operate directly at the phrase level'),\n",
       "  (0.0,\n",
       "   'while our submission time system syn using lm for rescoring only shows no improvement over the baseline we clearly see the impact of integrating the language model into the kbest list extraction process'),\n",
       "  (0.0,\n",
       "   'while no improvements were available at submission time our subsequent performance highlights the importance of tight integration of ngram language modeling within the syntax driven parsing environment')],\n",
       " [(0.0,\n",
       "   'would be while section will define the model formally we first proceed with an example translation from english to chinese note in particular that the inverted phrases between source and target throughout this paper we will use lhs and sourceside interchangeably so are rhs and targetside'),\n",
       "  (0.0,\n",
       "   'we then collect the resulting targetlanguage strings and plug them into the chineseside sr of rule r getting a translation for the subtree t'),\n",
       "  (0.0,\n",
       "   'we require each variable xi e x occurs exactly once in t and exactly once in s linear and nondeleting')],\n",
       " [(0.0,\n",
       "   'while the games goal is to arrive at some funny derivative of the original message by passing it through several noisy channels the cw algorithm aims at finding groups of nodes that broadcast the same message to their neighbors'),\n",
       "  (0.0,\n",
       "   'when generating swgraphs with the steyverstenenbaum model we fixed m to and varied n and the merge rate r which is the fraction of nodes of the smaller graph that is merged with nodes of the larger graph'),\n",
       "  (0.0,\n",
       "   'we use the following notation throughout this paper let g ve be a weighted graph with nodes viev and weighted edges vi vj wij ee with weight wij')],\n",
       " [(0.0,\n",
       "   'wu used a binary bracketing itg to segment a sentence while simultaneously wordaligning it to its translation but the model was trained heuristically with a fixed segmentation'),\n",
       "  (0.0,\n",
       "   'with this constraint in place the use of hillclimbing and sampling during em training becomes one of the largest remaining weaknesses of the cjptm'),\n",
       "  (0.0,\n",
       "   'with terminal productions producing cepts and inversions measuring distortion our phrasal itg is essentially a variation on the jptm with an alternate distortion model')],\n",
       " [(0.0,\n",
       "   'where not otherwise specified the postag and supertag sequence models are gram mod els and the language model is a gram model'),\n",
       "  (0.0,\n",
       "   'we use a model which combines more specificdependencies on source words and source ccg su pertags with a more general model which only has dependancies on the source word see equation we explore two different ways of balancing the sta tistical evidence from these multiple sources'),\n",
       "  (0.0,\n",
       "   'we tried setting the distortion limit to to see if allowing longer distance reorderings with ccg supertag sequence models could further improve performance however it resulted in a decrease in performance to a bleu score of')],\n",
       " [(0.0,\n",
       "   'zhao et al apply a slightly different sentencelevel strategy to language model adaptation first generating an nbest list with a baseline system then finding similar sentences in a monolingual targetlanguage corpus'),\n",
       "  (0.0,\n",
       "   'when using a loglinear combining framework as described in section mixture weights are set in the same way as the other loglinear parameters when performing crossdomain adaptation'),\n",
       "  (0.0,\n",
       "   'when translating however distances to the current source text are used in or instead of distances to the indomain development corpus')],\n",
       " [(0.0,\n",
       "   'word alignments are created between the source sentence and the reference translation shown and the source sentence and each of the system translations not shown'),\n",
       "  (0.0,\n",
       "   'while we consider the human evaluation to be primary it is also interesting to see how the entries were ranked by the various automatic evaluation metrics'),\n",
       "  (0.0,\n",
       "   'while these are based on a relatively few number of items and while we have not performed any tests to determine whether the differences in p are statistically significant the results are nevertheless interesting since three metrics have higher correlation than bleu tables and report p for the six metrics which were used to evaluate translations into the other languages')],\n",
       " [(0.0,\n",
       "   'when optimizing correlation with the sum of adequacy and fluency optimal values fall in between the values found for adequacy and fluency'),\n",
       "  (0.0,\n",
       "   'when n systems were available for a particular language we train the parameters n times leaving one system out in each training and pooling the segments from all other systems'),\n",
       "  (0.0,\n",
       "   'when evaluating a set of parameters on test data we compute segmentlevel correlation with human judgments for each of the systems in the test set and then report the mean over all systems')],\n",
       " [(0.0,\n",
       "   'while the training for the me classifier was done on a separate corpus and it was this classifier that contributed the most to the high precision it should be noted that some of the filters were tuned on the evaluation corpus'),\n",
       "  (0.0,\n",
       "   'while most nlp systems are a balancing act between precision and recall the domain of designing grammatical error detection systems is distinguished in its emphasis on high precision over high recall'),\n",
       "  (0.0,\n",
       "   'when we examined the errors we discovered that frequently the classifiers most probable preposition the one it assigned differed from the second most probable by just a few percentage points')],\n",
       " [(0.0,\n",
       "   'wsd is performed by assigning to each in duced cluster a score equal to the sum of weights of hyperedges found in the local context of the target word'),\n",
       "  (0.0,\n",
       "   'with this goal onmind we gave all the participants an unlabeled cor pus and asked them to induce the senses and create a clustering solution on it'),\n",
       "  (0.0,\n",
       "   'we were also surprised to see that no system could system supervised evaluation random random ramdom random table supervised evaluation of several random baselinesbeat the one cluster one word')],\n",
       " [(0.0,\n",
       "   'when this mapping was not straightforward we just adopted the wordnet sense inventory for that wordwe released the entire sense groupings those in duced from the manual mapping for words in the test set plus those automatically derived on the other words and made them available to the participants'),\n",
       "  (0.0,\n",
       "   'we report about the use of semantic resources as well as semantically annotated corpora sc semcor dso defence science organ isation corpus se senseval corpora omwe open mind word expert xwn extended word net wn wordnet glosses andor relations wnd wordnet domains as well as information about the use of unannotated corpora uc training tr mfs based on the semcor sense frequencies and the coarse senses provided by the organizers cs'),\n",
       "  (0.0,\n",
       "   'we observe that on a technical domain suchas computer science most supervised systems per formed worse due to the nature of their training set')],\n",
       " [(0.0,\n",
       "   'whilstresearchers believe that it will ultimately prove useful for applications which need some degree of se mantic interpretation the jury is still out on this point'),\n",
       "  (0.0,\n",
       "   'we tookthe word with the largest similarity or smallest dis tance for sd and l for best and the top for oot'),\n",
       "  (0.0,\n",
       "   'we thank serge sharoff for the use of his internet corpus julie weeds for the software we used for producing the distributional similarity baselines and suzanne stevenson for suggesting the oot task')],\n",
       " [(0.0,\n",
       "   'while we expected a raise in the case of the us census names the other two cases just show that there is a high and unpredictable variability which would require much larger data sets to have reliable population samples'),\n",
       "  (0.0,\n",
       "   'we reused the web corpus mann which contains names randomly picked from the us census and was well suited for the task'),\n",
       "  (0.0,\n",
       "   'we preferred how ever to leave the corpus as is and concentrate our efforts in producing clean training and test datasets rather than investing time in improving trial data')],\n",
       " [(0.0,\n",
       "   'wvalis hybrid approach outperforms the other systems in task b and using relaxed scoring in task c as well'),\n",
       "  (0.0,\n",
       "   'with this more complete temporal annotation it is no longer possible to simply evaluate the entire graph by scoring pairwise comparisons'),\n",
       "  (0.0,\n",
       "   'when applied to the test data the task b system was run first in order to supplythe necessary features to the task a and task c sys temslccte automatically identifies temporal refer ring expressions events and temporal relations in text using a hybrid approach leveraging variousnlp tools and linguistic resources at lcc')],\n",
       " [(0.0,\n",
       "   'whether or not the more coarsegrained senses are effective in improving natural language processing applications remains to be seen'),\n",
       "  (0.0,\n",
       "   'we selecteda total of lemmas verbs and nouns con sidering the degree of polysemy and total instances that were annotated'),\n",
       "  (0.0,\n",
       "   'we proposed two levels of participation in thistask i closed the systems could use only the an notated data provided and nothing else')],\n",
       " [(0.0,\n",
       "   'zx will now be equal to the sum of the weights of all trees that contain i jk a naive implementation to compute the expectation of all ln edges takes oln ln since calculating zx takes on ln for a single edge'),\n",
       "  (0.0,\n",
       "   'within this setting every edge in an induced graph gx for a sentence x will have an associated weight wk ij that maps the kth directed edge from node i to node j to a real valued numerical weight'),\n",
       "  (0.0,\n",
       "   'when this analysis is coupled with the projective parsing algorithms of eisner and paskin we begin to get a clear picture of the complexity for datadriven dependency parsing within an edgefactored framework')],\n",
       " [(0.0,\n",
       "   'with respect to mt quality we noticed that the introduction of test sets from a different domain did have an impact on the ranking of systems'),\n",
       "  (0.0,\n",
       "   'with respect to measuring the correlation between automated evaluation metrics and human judgments we found that using meteor and ulch which utilizes a variety of metrics including meteor resulted in the highest spearman correlation scores on average when translating into english'),\n",
       "  (0.0,\n",
       "   'while the judgments that we collected provide a wealth of information for developing automatic evaluation metrics we cannot not reuse them to evaluate our translation systems after we update their parameters or change their behavior in anyway')],\n",
       " [(0.0,\n",
       "   'word segmentation is considered an important first step for chinese natural language processing tasks because chinese words can be composed of multiple characters but with no space appearing between words'),\n",
       "  (0.0,\n",
       "   'without a standardized notion of a word traditionally the task of chinese word segmentation starts from designing a segmentation standard based on linguistic and task intuitions and then aiming to building segmenters that output words that conform to the standard'),\n",
       "  (0.0,\n",
       "   'with current statistical phrasebased mt systems one might hypothesize that segmenting into small chunks including perhaps even working with individual characters would be optimal')],\n",
       " [(0.0,\n",
       "   'with parallel computing processing time wall time can often be cut down by one or two orders of magnitude'),\n",
       "  (0.0,\n",
       "   'with compatible interface mgiza is suitable for a dropin replacement for giza while pgiza can utilize huge computation resources which is suitable for building large scale systems that cannot be built using a single machine'),\n",
       "  (0.0,\n",
       "   'while working on the required modification to giza to run the alignment step in parallel we identified a bug which needed to be fixed')],\n",
       " [(0.0,\n",
       "   'while a limited amount of gold standard annotated data was prepared for the parser evaluation shared task this is the main source of goldstandard sd data which is currently available'),\n",
       "  (0.0,\n",
       "   'when the relation between a head and its dependent can be identified more precisely relations further down in the hierarchy are used but when it is unclear more generic dependencies are possible dp dp'),\n",
       "  (0.0,\n",
       "   'when seeking a goldstandard dependency scheme for parser evaluation the ultimate goal of such an evaluation is an important question')],\n",
       " [(0.0,\n",
       "   'with the beam setting used in our experiments only of possible dependencies are considered by the tagbased model but of all correct dependencies are included'),\n",
       "  (0.0,\n",
       "   'we would argue that the collins method is considerably more complex than ours requiring a firststage generative model together with a reranking approach'),\n",
       "  (0.0,\n",
       "   'we will extend our model to include higherorder features in particular features based on sibling dependencies mcdonald and pereira and grandparent dependencies as in carreras')],\n",
       " [(0.0,\n",
       "   'zhang et al achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus'),\n",
       "  (0.0,\n",
       "   'wsem is the weight assigned to the semantic task the macro labeled fi score which was used for the ranking of the participating systems is computed as the harmonic mean of lmp and lmr'),\n",
       "  (0.0,\n",
       "   'who within an ensemblebased architecture implemented a joint syntacticsemantic model using maltparser with labels enriched with semantic information lluis and marquez who used a modified version of the eisner algorithm to jointly predict syntactic and semantic dependencies and finally sun et al who integrated dependency label classification and argument identification using a maximumentropy markov model')],\n",
       " [(0.0,\n",
       "   'wpf and wpbleu popovic and ney these metrics are based on words and part of speech sequences wpf is an ngram based fmeasure which takes into account both word ngrams and part of speech ngrams wpbleu is a combnination of the normal blue score and a part of speechbased bleu score'),\n",
       "  (0.0,\n",
       "   'word parallel corpus to create the large frenchenglish parallel corpus we conducted a targeted web crawl of bilingual web sites'),\n",
       "  (0.0,\n",
       "   'when there are no ties p can be calculated using the simplified equation where di is the difference between the rank for systemi and n is the number of systems')],\n",
       " [(0.0,\n",
       "   'where there is a clear point of departure for research a basic implementation of each interface is provided as an abstract class to minimize the work necessary for new extensions'),\n",
       "  (0.0,\n",
       "   'when designing our toolkit we applied general principles of software engineering to achieve three major goals extensibility endtoend coherence and scalability'),\n",
       "  (0.0,\n",
       "   'when a sentence is selected the count of every ngram in w that is found in the source sentence is incremented by the number of its occurrences in the source sentence')],\n",
       " [(0.0,\n",
       "   'with respect to the standard procedure the total training time was reduced by almost phrase extraction produced more phrase pairs and the final translation system showed a loss in translation performance bleu score below relative'),\n",
       "  (0.0,\n",
       "   'while data sparseness corroborates the need of large language samples in smt linguistic variability would indeed suggest to consider many alternative data sources as well'),\n",
       "  (0.0,\n",
       "   'when spanish indomain data is provided bleu score increases from to tm and rm contribute by about relative by covering the gap from to')],\n",
       " [(0.0,\n",
       "   'with the exception of the phrase substitutions the cost for all other edit operations is the same regardless of what the words in question are'),\n",
       "  (0.0,\n",
       "   'while this can make no judgement as to the preference of one type of human judgment over another it indicates differences between these human judgment types and in particular the difference between hter and adequacy and fluency'),\n",
       "  (0.0,\n",
       "   'while this analysis is not comprehensive it does give a general idea of the performance of all metrics by synthesizing the results into a single table')],\n",
       " [(0.0,\n",
       "   'with pcs on the abstracts corpus the system achieves of error reduction over current state of the art results'),\n",
       "  (0.0,\n",
       "   'with a more comprehensive list of negation signals it would be possible to identify all of them in a text'),\n",
       "  (0.0,\n",
       "   'we take the scope to the right for the baseline because it is much more frequent than the scope to the left as is shown by the statistics contained in table of section')],\n",
       " [(0.0,\n",
       "   'word class models discussed in section are computed offline are available online and provide an alternative to traditional semisupervised learning'),\n",
       "  (0.0,\n",
       "   'within the binary tree produced by the algorithm each word can be uniquely identified by its path from the root and this path can be compactly represented with a bit string'),\n",
       "  (0.0,\n",
       "   'while these two needs have motivated some of the research in ner in the last decade several other fundamental decisions must be made')],\n",
       " [(0.0,\n",
       "   'work on hedging in the machine learning field has as a goal to classify sentences into speculative or definite non speculative'),\n",
       "  (0.0,\n",
       "   'we take the scope to the right for the baseline because it is much more frequent than the scope to the left as is shown by the statistics contained in table of section'),\n",
       "  (0.0,\n",
       "   'we show that the system performs well for this task and that the same scope finding approach can be applied to both negation and hedging')],\n",
       " [(0.0,\n",
       "   'without constraints on the type of theme arguments the following two annotations are both legitimate the two can be seen as specifying the same event at different levels of specificity'),\n",
       "  (0.0,\n",
       "   'with the emergence of ner systems with performance capable of supporting practical applications the recent interest of the biotm community is shifting toward ie'),\n",
       "  (0.0,\n",
       "   'while using the final scores for weighting uses data that would not be available in practice similar weighting could likely be obtained eg using performance on the development data')],\n",
       " [(0.0,\n",
       "   'work in emotion detection can be roughly classified into that which looks for specific emotion denoting words elliott that which determines tendency of terms to cooccur with seed words whose emotions are known read that which uses handcoded rules neviarouskaya et al and that which uses machine learning and a number of emotion features including emotion denoting words alm et al'),\n",
       "  (0.0,\n",
       "   'words may evoke different emotions in different contexts and the emotion evoked by a phrase or a sentence is not simply the sum of emotions conveyed by the words in it but the emotion lexicon will be a useful component for any sophisticated emotion detecting algorithm'),\n",
       "  (0.0,\n",
       "   'wordnet affect lexicon strapparava and valitutti has a few hundred words annotated with the emotions they evoke it was created by manually identifying the emotions of a few seed words and then marking all their wordnet synonyms as having the same emotion')],\n",
       " [(0.0,\n",
       "   'you can do this by measuring the httpcastingwordscom httpgroupscsailmiteduuidturkit interannotator agreement of the turkers against ex perts on small amounts of gold standard data or by stating what controls you used and what criteria youused to block bad turkers'),\n",
       "  (0.0,\n",
       "   'workshop par ticipants struggled with how to attract turkers howto price hits hit design instructions cheating de tection etc no doubt that as work progresses so will a communal knowledge and experience of how to use mturk'),\n",
       "  (0.0,\n",
       "   'when publishing papers that use mechanical turk as a source of training data or to evaluate the outputof an nlp system report how you ensured the qual ity of your data')],\n",
       " [(0.0,\n",
       "   'your edited translations the machine translations the shortage of snow in mountain the shortage of snow in mountain worries the hoteliers worries the hoteliers correct reset edited no corrections needed unable to the deserted tracks are not the deserted tracks are not putting down problem only at the exploitants of skilift putting down problem only at the exploitants of skilift correct reset edited no corrections needed unable to the lack of snow deters the people the lack of snow deters the people to reserving their stays at the ski in the hotels and pension to reserving their stays at the ski in the hotels and pension correct reset edited no corrections needed unable to thereby is always possible to thereby is always possible to track free bedrooms for all the dates in winter including christmas and nouvel an track free bedrooms for all the dates in winter including christmas and nouvel an'),\n",
       "  (0.0,\n",
       "   'within each batch the source segments for nine of the screens were chosen from a small pool of source segments instead of being sampled from the larger pool of source segments designated for the ranking task the larger pool was used to choose source segments for nine other screens also'),\n",
       "  (0.0,\n",
       "   'with the exception of frenchenglish and englishfrench one can observe that topperforming constrained systems did as well as the unconstrained system onlineb')],\n",
       " [(0.0,\n",
       "   'wordspace vector models or distributional models of semantics henceforth dsms are computational models that build contextual semantic representations for lexical items from corpus data'),\n",
       "  (0.0,\n",
       "   'with these settings the compared spaces become less asymmetrical gold standard neighbours from a pool of just different items plus predictions prediction space neighbours from a pool of items'),\n",
       "  (0.0,\n",
       "   'widdows obtain results indicating that both the tensor product and the convolution product perform better than the simple additive model')],\n",
       " [(0.0,\n",
       "   'when positive feedback is received a new training instance for a structured learner is created from the input sentence and prediction line this training instance replaces any previous instance for the input sentence'),\n",
       "  (0.0,\n",
       "   'when a structure is found with positive feedback it is added to the training pool for a structured learner'),\n",
       "  (0.0,\n",
       "   'we use a custom implementation to optimize the objective function using the cuttingplane method this allows us to parrallelize the learning process by solving the inference problem for multiple training examples simultaneously')],\n",
       " [(0.0,\n",
       "   'zhao et al extended the biological cue word dictionary of their system using it as a feature for classification by the frequent cues of the wikipedia dataset while ji et al'),\n",
       "  (0.0,\n",
       "   'weasel words do not give a neutral account of facts rather they offer an opinion without any backup or source'),\n",
       "  (0.0,\n",
       "   'we think that this is due to the practical importance of the task for principally biomedical applications and because it addresses several open research questions')],\n",
       " [(0.0,\n",
       "   'we use the following procedure to convert a tweet into a tree representation initialize the main tree to be root'),\n",
       "  (0.0,\n",
       "   'we use stratified sampling to get a balanced dataset of tweets tweets each from classes positive negative and neutral'),\n",
       "  (0.0,\n",
       "   'we use previously proposed stateoftheart unigram model as our baseline and report an overall gain of over for two classification tasks a binary positive versus negative and a way positive versus negative versus neutral')],\n",
       " [(0.0,\n",
       "   'while the task is related to synonymy relation extraction yu and agichtein it has a novel definition of renaming one name permanently replacing the other'),\n",
       "  (0.0,\n",
       "   'while the basic task setup and entity definitions follow those of the ge task epi extends on the extraction targets by defining new event types relevant to task topics including major protein modification types and their reverse reactions'),\n",
       "  (0.0,\n",
       "   'while finding connections between event triggers and protein references is a major part of event extraction it becomes much harder if one is replaced with a coreferencing expression')],\n",
       " [(0.0,\n",
       "   'when only primary arguments are considered the first five event types in table are classified as simple event types requiring only unary arguments'),\n",
       "  (0.0,\n",
       "   'uturku was the winning system of task in bjorne et al and miwa was the best system reported after bionlpst miwa et al b'),\n",
       "  (0.0,\n",
       "   'to our disappointment however an effective use of supporting task results was not observed which thus remains as future work for further improvement')],\n",
       " [(0.0,\n",
       "   'word sense we trained a word sense tagger using a svm classifier and contextual word and part of speech features on all the training portion of the ontonotes data'),\n",
       "  (0.0,\n",
       "   'while trained morels seem able to better balance precision and recall and thus to achieve a higher fscore on the mention task itself their recall tends to be quite a bit lower than that achievable by rulebased systems designed to favor recall'),\n",
       "  (0.0,\n",
       "   'while this is lower than the figures cited in previous coreference evaluations that is as expected given that the task here includes predicting the underlying mentions and mention boundaries the insistence on exact match and given that the relatively easier appositive coreference cases are not included in this measure')],\n",
       " [(0.0,\n",
       "   'while the corpora used in haghighi and klein are different from the one in this shared task our result of b suggests that our systems performance is competitive'),\n",
       "  (0.0,\n",
       "   'we use all synsets for each mention but restrict it to mentions that are at most three sentences apart and lexical chains of length at most four'),\n",
       "  (0.0,\n",
       "   'we select for resolution only the first mentions in each cluster for two reasons a the first mention tends to be better defined fox which provides a richer environment for feature extraction and b it has fewer antecedent candidates which means fewer opportunities to make a mistake')],\n",
       " [(0.0,\n",
       "   'zmert is designed to be modular with respect to the objective function and allows bleu to be easily replaced with other automatic evaluation metrics'),\n",
       "  (0.0,\n",
       "   'yo dim minustah ap bay djob mason ki kote pou mw ta pase si mw ta vle jwenn nan djob sa yo'),\n",
       "  (0.0,\n",
       "   'working this back into pes definition we have pa gt b pa lt b and therefore pe rather than and below for a detailed breakdown by language pair')],\n",
       " [(0.0,\n",
       "   'while versions tuned to various types of human judgments do not perform as well as the widely used bleu metric papineni et al a balanced tuning version of meteor consistently outperforms bleu over multiple endtoend tunetest runs on this data set'),\n",
       "  (0.0,\n",
       "   'whereas previous versions of meteor simply strip punctuation characters prior to scoring version includes a new text normalizer intended specifically for translation evaluation'),\n",
       "  (0.0,\n",
       "   'we use the nist open machine translation evaluation urduenglish parallel data przybocki plus m words of monolingual data from the english gigaword corpus parker et al to build a standard moses system hoang et al as follows')],\n",
       " [(0.0,\n",
       "   'with some minor api changes namely returning the length of the ngram matched it could also be fasterthough this would be at the expense of an optimization we explain in section'),\n",
       "  (0.0,\n",
       "   'with a good hash function collisions of the full bit hash are exceedingly rare one in billion queries for our baseline model will falsely find a key not present'),\n",
       "  (0.0,\n",
       "   'while sorted arrays could be used to implement the same data structure as probing effectively making m we abandoned this implementation because it is slower and larger than a trie implementation')],\n",
       " [(0.0,\n",
       "   'wlvshef r s the systems integrates novel linguistic features from the source and target texts in an attempt to overcome the limitations of existing shallow features for quality estimation'),\n",
       "  (0.0,\n",
       "   'while some systems did not achieve improvements over the baseline while exploiting such features others have the uu submissions for instance exploiting both constituency and dependency trees'),\n",
       "  (0.0,\n",
       "   'when there are no ties can be calculated using the simplified equation systemlevel correlation for translations into english where di is the difference between the rank for systemi and n is the number of systems')],\n",
       " [(0.0,\n",
       "   'wordalign thus provides an example how a model such as brown et al s model that was originally designed for research in statistical machine translation can be modified to achieve practical though less ambitious goals in the near term'),\n",
       "  (0.0,\n",
       "   'wordalign starts with an initial rough alignment i which maps french positions to english positions if the mapping is partial we use linear extrapolation to make it complete'),\n",
       "  (0.0,\n",
       "   'we used this stratified sampling because we wanted to make more accurate statements about our error rate by tokens than we would have obtained from random sampling or even from equal weighting of the quintiles')],\n",
       " [(0.0,\n",
       "   'whether the human language engine is organized as a pipeline plus a few feedback loops or an every module talks to every other module architecture is unknown at this point hopefully new psycholinguistic experiments will shed more light on this issue'),\n",
       "  (0.0,\n",
       "   'we have insufficient engineering data at present to make any wellsubstantiated claims about whether the oneway pipeline has the optimal costbenefit tradeoff or not and in any case this will probably depend somewhat on the circumstances of each application reiter and mellish but the circumstantial evidence on this question is striking despite the fact that so many theoretical papers have argued against pipelines and very few if any have argued for pipelines every one of the applicationsoriented systems examined in this survey chose to use the oneway pipeline architecture'),\n",
       "  (0.0,\n",
       "   'unfortunately while all of the systems possessed a module which converted semantic representations into deep syntactic ones each system used a different name for this module')],\n",
       " [(0.0,\n",
       "   'with unsupervised learning the learner does not have a gold standard training corpus with which accuracy can be measured'),\n",
       "  (0.0,\n",
       "   'with markovmodel based taggers there have been two different methods proposed for adding knowledge to a tagger trained using the baumwelch algorithm'),\n",
       "  (0.0,\n",
       "   'when training a stochastic tagger using the baumwelch algorithm overtraining often does occur merialdo elworthy requiring an additional heldout training corpus for determining an appropriate number of training iterations')],\n",
       " [(0.0,\n",
       "   'wordclasses of semantically similar words may be used to help the sparse data problem both rrr and br report significant improvements through the use of wordclasses'),\n",
       "  (0.0,\n",
       "   'when evaluating an algorithm it is useful to have an idea of the lower and upper bounds on its performance'),\n",
       "  (0.0,\n",
       "   'we will use the symbol f to denote the number of times a particular tuple is seen in training data')],\n",
       " [(0.0,\n",
       "   'yarowsky used the following metric to calculate the strength of a feature f this is for the case of a confusion set of two words w and w'),\n",
       "  (0.0,\n",
       "   'yarowsky pointed out the complementarity between context words and collocations context words pick up those generalities that are best expressed in an orderindependent way while collocations capture orderdependent generalities'),\n",
       "  (0.0,\n",
       "   'yarowsky describes further refinements such as detecting and pruning features that make a zero or negative contribution to overall performance')],\n",
       " [(0.0,\n",
       "   'yet a computational system has no choice but to consider other more awkward possibilities for example this cluster might be capturing a distributional relationship between advice as one sense of counsel and royalty as one sense of court'),\n",
       "  (0.0,\n",
       "   'yarowskys algorithm for sense disambiguation can be thought of as a way of determining how rogets thesaurus categories behave with respect to contextual features'),\n",
       "  (0.0,\n",
       "   'word word similarity most informative subsumer doctor nurse health professional doctor lawyer professional person doctor man person individual doctor medicine entity doctor hospital entity doctor health virtual root doctor sickness virtual root doctors are minimally similar to medicine and hospitals since these things are all instances of something having concrete existence living or nonliving wordnet class entity but they are much more similar to lawyers since both are kinds of professional people and even more similar to nurses since both are professional people working specifically within the health professions')],\n",
       " [(0.0,\n",
       "   'while this automatic derivation process introduced a small percentage of errors of its own it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing'),\n",
       "  (0.0,\n",
       "   'while brackets must be correctly paired in order to derive a chunk structure it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags the few hard cases that arise can be handled completely locally'),\n",
       "  (0.0,\n",
       "   'when this approach is applied to partofspeech tagging the possible sources of evidence for templates involve the identities of words within a neighborhood of some appropriate size and their current partofspeech tag assignments')],\n",
       " [(0.0,\n",
       "   'words that the most precise lexicon didnt know about which were found in the second most precise lexicon were translated next'),\n",
       "  (0.0,\n",
       "   'while the true size of the source vocabulary is usually unknown recall can be estimated using a representative text sample by computing the fraction of words in the text that also appear in the lexicon'),\n",
       "  (0.0,\n",
       "   'whether a pair of words is considered a cognate pair depends on the ratio of the length of their longest not necessarily contiguous common subsequence to the length of the longer word')],\n",
       " [(0.0,\n",
       "   'without optimisation it has an asymptotic retrieval complexity of nf where n is the number of items in memory and f the number of features'),\n",
       "  (0.0,\n",
       "   'when a word is not found in the lexicon its lexical representation is computed on the basis of its form its context is determined and the resulting pattern is looked up in the unknown words case base'),\n",
       "  (0.0,\n",
       "   'weiss amp kulikowski independent training and test sets were selected from the original corpus the system was trained on the training set and the generalization accuracy percentage of correct category assignments was computed on the independent test set')],\n",
       " [(0.0,\n",
       "   'without this optimization testing would have been several orders of magnitude slower than making a decision by testing only a small subset of highly predictive features'),\n",
       "  (0.0,\n",
       "   'with respect to training time the symbolic methods are significantly slower since they are searching for a simple declarative representation of the concept'),\n",
       "  (0.0,\n",
       "   'with respect to testing time the symbolic methods perform the best since they only need to test a small number of features before making a decision')],\n",
       " [(0.0,\n",
       "   'weischedel et al provide the results from a battery of tritag markov model experiments in which the probability pwt of observing a word sequence w wi w wn together with a tag sequence t is given by furthermore pwiiti for unknown words is computed by the following heuristic which uses a set of predetermined endings this approximation works as well as the maxent model giving unknown word accuracyweischedel et al on the wall st journal but cannot be generalized to handle more diverse information sources'),\n",
       "  (0.0,\n",
       "   'using the set of difficult words the model performs at accuracy on the development set an insignificant improvement from the baseline accuracy of'),\n",
       "  (0.0,\n",
       "   'unlike sdt the maxent training procedure does not recursively split the data and hence does not suffer from unreliable counts due to data fragmentation')],\n",
       " [(0.0,\n",
       "   'with this criterion and the example grammar of figure the best parse tree would be the probability that the s constituent is correct is while the probability that the a constituent is correct is and the probability that the b constituent is correct is thus this tree has on average constituents correct'),\n",
       "  (0.0,\n",
       "   'when using a chart parser as bod did three problematic cases must be handled productions unary productions and nary ngt productions'),\n",
       "  (0.0,\n",
       "   'were previous results due only to the choice of test data or are the differences in implementation partly responsible')],\n",
       " [(0.0,\n",
       "   'wwz w is the weighted mutual information in our algorithm since it is most suitable for lexicon compilation of midfrequency technical words or terms as an initial step all prw are precomputed for the seed words in both languages'),\n",
       "  (0.0,\n",
       "   'word correlations ww wt are computed from general likelihood scores based on the cooccurrence of words in common segments'),\n",
       "  (0.0,\n",
       "   'word correlations are important statistical information which has been successfully employed to find bilingual word pairs from parallel corpora')],\n",
       " [(0.0,\n",
       "   'what is most interesting here is the way in which strongly selecting word w is typically the head of a noun phrase which could lead the model astray for example toy soldiers behave differently from soldiers mccawley'),\n",
       "  (0.0,\n",
       "   'thus despite the absence of class annotation in the training text it is still possible to arrive at a usable estimate of classbased probabilities'),\n",
       "  (0.0,\n",
       "   'this means that the observed countverbobj drink coffee will be distributed by adding a to the joint frequency with drink for each of the classes containing coffee')],\n",
       " [(0.0,\n",
       "   'typically the procedures postulate many different values for a which cause the parser to explore many different derivations when parsing an input sentence'),\n",
       "  (0.0,\n",
       "   'thus there are three parameters for the search heuristic namely k m and q and all experiments reported in this paper use k m and q table describes the top k bfs and the semantics of the supporting functions'),\n",
       "  (0.0,\n",
       "   'this paper takes a historybased approach black et al where each treebuilding procedure uses a probability model palb derived from pa b to weight any action a based on the available context or history b')],\n",
       " [(0.0,\n",
       "   'while we will almost always wish to parse using thresholds it is nice to know that multiplepass parsing can be seen as an approximation to an admissible technique where the degree of approximation is controlled by the thresholding parameter'),\n",
       "  (0.0,\n",
       "   'while we had not spent a great deal of time hand optimizing these parameters we are very encouraged by the optimization algorithms practical utility'),\n",
       "  (0.0,\n",
       "   'while we dont know of other systems that have used exactly our techniques our techniques are certainly similar to those of others')],\n",
       " [(0.0,\n",
       "   'you can skip ahead to table for a random sample of the nccs that the method validated for use in a machine translation task'),\n",
       "  (0.0,\n",
       "   'wu showed how to use an existing translation lexicon to populate a database of phrasal correspondences for use in examplebased mt'),\n",
       "  (0.0,\n",
       "   'wrongful conviction erreur judiciaire weak sister parent pauvre of both the users and providers of transportation des utilisateurs et des transporteurs understand the motivation saisir le motif swimming pool piscine ship unprocessed uranium expedier de luranium non raffine by reason of insanity pour cause dalienation mentale lagence de presse libre du qubec lagence de presse libre du qubec do cold weather research etudier leffet du froid the bread basket of the nation le grenier du canada turn back the boatload of european jews renvoyer tout ces juifs europeens generic pharmaceutical industry association generic pharmaceutical industry association')],\n",
       " [(0.0,\n",
       "   'within the context of a restricted domain many polysemous words have a strong preference for one word sense so knowing the most probable word sense in a domain can strongly constrain the ambiguity'),\n",
       "  (0.0,\n",
       "   'with the exception of the energy category we were able to find words that were judged as s or s for each category'),\n",
       "  (0.0,\n",
       "   'while these efforts may be useful for some applications we believe that they will never fully satisfy the need for semantic knowledge')],\n",
       " [(0.0,\n",
       "   'zipf pedersen kayaalp and bruce there is evidence that our data when represented by a dissimilarity matrix can be adequately characterized by a normal distribution'),\n",
       "  (0.0,\n",
       "   'yarowsky compares his method to schiitze and shows that for four words the former performs significantly better in distinguishing between two senses'),\n",
       "  (0.0,\n",
       "   'with the exception of line each ambiguous word is tagged with a single sense defined in the longman dictionary of contemporary english ldoce procter')],\n",
       " [(0.0,\n",
       "   'we measured the ability of judges to agree with one another using the notion of percent agreement that was defined by gale and used extensively in discourse segmentation studies passonneau and litman hearst percent agreement reflects the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion the percent agreements computed for each of the five texts and each level of importance are given in table the agreements among judges for our experiment seem to follow the same pattern as those described by other researchers in summarization johnson that is the judges are quite consistent with respect to what they perceive as being very important and unimportant but less consistent with respect to what they perceive as being less important in contrast with the agreement observed among judges the percentage agreements computed for importance assignments that were randomly generated for the same texts followed a normal distribution with p o these results suggest that the agreement among judges is significant agreement among judges with respect to the importance of each textual unit'),\n",
       "  (0.0,\n",
       "   'we described the first experiment that shows that the concepts of rhetoncal analysis and nucleanty can be used effectively for summarizing text the experiment suggests that discoursebased methods can account for detemmimg the most important units in a text with a recall and precision as high as we showed how the concepts of rhetorical analysis and nucleanty can be treated algorithmically and we compared recall and precision figures of a summarization program that implements these concepts with recall and precision figures that pertain to a baseline algorithm and to a commercial system the microsoft office summarizer the discoursebased summanzation program that we propose outperforms both the baseline and the commercial summarizer see table however since its results do not match yet the recall and precision figures that pertain to the manual discourse analyses it is likely that improvements of the rhetorical parser algorithm will result in better performance of subsequent implemetations'),\n",
       "  (0.0,\n",
       "   'we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text we show how these concepts can be implemented and we discuss results that we obtained with a discoursebased summanzation program motivation the evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some consamples of the output in very few cases output of a summarization program with a humanmade summary or evaluated with the help of human subjects usually the results are modest unfortunately evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions the position that we take in this paper is that in order to build highquality summarization programs one needs to evaluate not only a representative set of automatically generated outputs a highly difficult problem by itself but also the adequacy of the assumptions that these programs use that way one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each with few exceptions automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text see paice for an excellent overview determining the salient parts is considered to be achievable because one or more of the following assumptions hold i important sentences in a text contain words that are used frequently lahn edmundson n important sentences contain words that are used in the tide and section headings edmundson in important sentences are located at the beginning or end of paragraphs baxendale iv important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically through training techniques lin and hovy v important sentences use words as greatest and significant or indiphrases as the main aim of this paper and the purpose of this article while nonimportant senuse words as impossible edmundson rush salvador and zamora vi important sentences and concepts highest connected entities in elaborate semantic structures skorochodko lin barzilay and elhadad and vn imponant and nonimportant sentences are derivable from a discourse representation of the text sparck jones ono surmta and mike in determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections computers are accurate tools flowever in determining the concepts that are semantically related or the discourse structure of a text computers are no longer so accurate rather they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement although it is plausible that elaborate cohesionand coherencebased structures can be used effectively in summarization we believe that before building summarization programs we should determine the extent to which these assumptions hold in this paper we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text we show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program from discourse trees to summaries an empirical view')],\n",
       " [(0.0,\n",
       "   'we use the princeton wordnet technology for the database format database compilation as well as the princeton wordnet interface applying extensions only where necessary'),\n",
       "  (0.0,\n",
       "   'we therefore propose a mixed approach treating irregular particle verbs by enumeration and regular particle verbs in a compositional manner'),\n",
       "  (0.0,\n",
       "   'we present the lexicalsemantic net for german germanet which integrates conceptual ontological information with lexical semantics within and across word classes')],\n",
       " [(0.0,\n",
       "   'while the winograd sentences are too difficult for current robust lexical semantic systems simpler generalizations about what can fill an argument are possible consider the price of aluminum rose today due to large purchases by alcoa inc'),\n",
       "  (0.0,\n",
       "   'whether this holds in general remains an open question but it is a central design assumption behind the system'),\n",
       "  (0.0,\n",
       "   'what distinguishes cogniac from algorithms that use similar sorts of information is that it will not resolve a pronoun in circumstances of ambiguity')],\n",
       " [(0.0,\n",
       "   'wordnet does not include crosspartofspeech semantic relations so this relation cannot be used with word senses while term indexing simply and successfully does not distinguish them'),\n",
       "  (0.0,\n",
       "   'with this collection we can see if plain disambiguation is helpful for retrieval because word senses are distinguished but synonymous word senses are not identified'),\n",
       "  (0.0,\n",
       "   'while his results are very interesting it remains unclear in our opinion whether they would be corroborated with real occurrences of ambiguous words')],\n",
       " [(0.0,\n",
       "   'zn in the bottomup variant of the earley algorithm where a gt is a production of the original grammar'),\n",
       "  (0.0,\n",
       "   'when one has a completed parse or perhaps several possible parses one simply stops parsing leaving items remaining on the agenda'),\n",
       "  (0.0,\n",
       "   'we tested the parser on section section of the wsj text with various normalization constants working on each sentence only until we reached the first full parse')],\n",
       " [(0.0,\n",
       "   'while was still the fourth highest score out of the twelve participants in the evaluation we feel that it is necessary to view this number as a crossdomain portability result rather than as an indicator of how the system can do on unseen data within its training domain'),\n",
       "  (0.0,\n",
       "   'while all of menes features have binaryvalued output the binary features are features whose associated historyview can be considered to be either on or off for a given token'),\n",
       "  (0.0,\n",
       "   'when the features attempt to determine whether or not they fire on a given history they request an appropriate history view object from the history object and then query the history view object to determine whether their firing conditions are satisfied')],\n",
       " [(0.0,\n",
       "   'when viewed in this way a can be regarded as an index into these vectors that specifies which value is relevant to the particular choice of antecedent'),\n",
       "  (0.0,\n",
       "   'we would like to know therefore whether the pattern of pronoun references that we observe for a given referent is the result of our supposed hypothesis about pronoun reference that is the pronoun reference strategy we have provisionally adopted in order to gather statistics or whether the result of some other unidentified process'),\n",
       "  (0.0,\n",
       "   'we will say that the gender class for which this relative frequency is the highest is the gender class to which the referent most probably belongs')],\n",
       " [(0.0,\n",
       "   'yet if there are enough relations to link any pair of facts which given the existence of elaboration may often be nearly the case the number of trees whose top nucleus are a specified fact grows from to to as the number of facts grows from to to'),\n",
       "  (0.0,\n",
       "   'with each example problem we have specified the number of facts the number of elaboration relations and the number of nonelaboration relations'),\n",
       "  (0.0,\n",
       "   'we score as follows for each fact that will come textually between a satellite and its nucleus constraints on information ordering our relations have preconditions which are facts that should be conveyed before them')],\n",
       " [(0.0,\n",
       "   'without this mechanism it is possible that the decision tree classifies np i and np i as coreferent and np i and npk as coreferent but np i and npk as not coreferent'),\n",
       "  (0.0,\n",
       "   'while human audiences have little trouble mapping a collection of noun phrases onto the same entity this task of noun phrase np coreference resolution can present a formidable challenge to an nlp system'),\n",
       "  (0.0,\n",
       "   'when computing a sum that involves both oo and oo we choose the more conservative route and the oo distance takes precedence ie the two noun phrases are not considered coreferent')],\n",
       " [(0.0,\n",
       "   'with only training words per context this is difficult and in the face of such strong odds against any of the named entity classes the conservative nature of the learning algorithm only braves an entity label correctly for more words than the baseline model'),\n",
       "  (0.0,\n",
       "   'when using dominants the order does not affect the process because of the fact that once a dominant state is reached it cannot change to another dominant state in the future probability mass is moved only from questionable'),\n",
       "  (0.0,\n",
       "   'when tokenization is used each token is inserted in the two morphological tries one that keeps the letters of the tokens in the normal prefix order another that keeps the letter in the reverse suffix order')],\n",
       " [(0.0,\n",
       "   'zt can be written as follows following the derivation of schapire and singer providing that w gt w equ'),\n",
       "  (0.0,\n",
       "   'yarowskycautious does not separate the spelling and contextual features but does have a limit on the number of rules added at each stage'),\n",
       "  (0.0,\n",
       "   'with each iteration more examples are assigned labels by both classifiers while a high level of agreement gt is maintained between them')],\n",
       " [(0.0,\n",
       "   'while we cannot prove there are no such useful features on which one should condition trust we can give some insight into why the features we explored offered no gain'),\n",
       "  (0.0,\n",
       "   'when this metric is less than we expect to incur more errors than we will remove by adding those constituents to the parse'),\n",
       "  (0.0,\n",
       "   'we would like to thank eugene charniak michael collins and adwait ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments')],\n",
       " [(0.0,\n",
       "   'without the chunker the relations finder would have to decide for every word whether it is the head of a constituent that bears a relation to the verb'),\n",
       "  (0.0,\n",
       "   'with the extended job tag set at hand we can tag the sentence after having found prep np and other chunks we collapse preps and nps to pps in a second step'),\n",
       "  (0.0,\n",
       "   'with the churlker the relations finder has to make this decision for fewer words namely only for those which are the last word in a chunk resp the preposition of a pp chunk')]]"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_objetivo_ml_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "bdef9c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:21.149303Z",
     "start_time": "2023-05-01T21:36:21.132121Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_teste_exp2=[]\n",
    "for lista_frase_obj_prob in frases_objetivo_ml_teste:\n",
    "    cada_paper=[]\n",
    "    for frase_obj_prob in lista_frase_obj_prob:\n",
    "        cada_paper.append(frase_obj_prob[1])\n",
    "    frase_objetivo_teste_exp2.append(cada_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "8b7d254d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:37.973521Z",
     "start_time": "2023-05-01T21:36:21.151284Z"
    }
   },
   "outputs": [],
   "source": [
    "p_bi, r_bi, f_bi = calculo_rouge(frase_objetivo_teste_exp2,list(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "cb773056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:36:38.092576Z",
     "start_time": "2023-05-01T21:36:37.976769Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrama com rest\n",
      "precisão: 0.344\n",
      "revocação: 0.401\n",
      "f-score: 0.336\n",
      "precisao acima de 80%: 0.0\n",
      "recall acima de 80%: 1.0\n",
      "igual a 0: 0.0\n",
      "-----\n",
      "bigrama com rest\n",
      "precisão: 0.273\n",
      "revocação: 0.376\n",
      "f-score: 0.282\n",
      "precisao acima de 80%: 0.0\n",
      "recall acima de 80%: 1.0\n",
      "igual a 0: 0.0\n",
      "-----\n",
      "trigrama com rest\n",
      "precisão: 0.273\n",
      "revocação: 0.376\n",
      "f-score: 0.282\n",
      "precisao acima de 80%: 0.0\n",
      "recall acima de 80%: 1.0\n",
      "igual a 0: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('unigrama com rest')\n",
    "print('precisão:',media2(media(max_rouge(p_uni))))\n",
    "print('revocação:',media2(media(max_rouge(r_uni))))\n",
    "print('f-score:',media2(media(max_rouge(f1_uni))))\n",
    "funcao_80_e_0(p_uni,r_uni)\n",
    "\n",
    "print('-----')\n",
    "print('bigrama com rest')\n",
    "print('precisão:',media2(media(max_rouge(p_bi))))\n",
    "print('revocação:',media2(media(max_rouge(r_bi))))\n",
    "print('f-score:',media2(media(max_rouge(f_bi))))\n",
    "funcao_80_e_0(p_bi,r_bi)\n",
    "\n",
    "print('-----')\n",
    "print('trigrama com rest')\n",
    "print('precisão:',media2(media(max_rouge(p_r_tri))))\n",
    "print('revocação:',media2(media(max_rouge(r_r_tri))))\n",
    "print('f-score:',media2(media(max_rouge(f1_r_tri))))\n",
    "funcao_80_e_0(p_r_tri,r_r_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3addf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7eece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25400d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "324.438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
